import pandas as pd
import numpy as np
import joblib
import time
import os
import io
from indicator_calculation import compute_all_indicators, add_time_session_features
import traceback
from collections import deque

BAR_FILE = "./live_data/bar_data.csv"
SIGNAL_FILE = "./trading/signal.txt"
EXIT_FILE = "./trading/exit.txt"
STATUS_FILE = "./trading/status.txt"

TRADE_THRESHOLD = 0.000005
TICK_VALUE = 5
SL_ATR_MULT = 0.1
TP_ATR_MULT = 2.0
TRAIL_START_MULT = 0.3
TRAIL_STOP_MULT = 0.3

BASE_CONTRACTS = 1
MAX_CONTRACTS = 1
PRED_HISTORY = deque(maxlen=1000)
try:
    if os.path.exists("pred_history.txt"):
        with open("pred_history.txt", "r") as f:
            for line in f:
                val = float(line.strip())
                PRED_HISTORY.append(val)
        print(f"‚úÖ Loaded {len(PRED_HISTORY)} predictions from history file.")
except Exception as e:
    print(f"‚ö†Ô∏è Failed to load PRED_HISTORY: {e}")

model = joblib.load("catboost_model_regression_NQ-12&24-single-model-less-features.pkl")

entry_price = None
trail_trigger = None
last_mtime = None
active_trade_side = None

# === MTFA Caches ===
last_15m_update = None
last_1h_update = None
cached_15m_features = None
cached_1h_features = None

# === Initialize dataframes ===
df_window = pd.read_csv(BAR_FILE, names=["datetime", "open", "high", "low", "close", "volume"])
df_window['datetime'] = pd.to_datetime(df_window['datetime'])
df_window.set_index("datetime", inplace=True)
df_window = df_window.tail(1000).copy()

model_features = [

   # --- 5-minute: Entry logic ---
   "ATR_14_5min",
   "RSI_14_5min",
   "RSI_14_Is_Overbought_70_5min",
   "RSI_14_Is_Oversold_30_5min",
   "MACD_12_26_9_Cross_Signal_5min",
   "EMA_20_5min",
   "EMA_10_vs_EMA20_5min",
   "_Slope_10_5min",
   "Candle_Range_5min",
   "Candle_Body_5min",
   "Upper_Wick_5min",
   "Lower_Wick_5min",
   "Body_vs_Range_5min",
   "CDL_DOJI_10_0.1_5min",
   "CDL_HAMMER_5min",
   "CDL_ENGULFING_5min",
   "Log_Return_1_5min",
   "Simple_Return_1_5min",

   # --- 15-minute: Trend filter / Volatility ---
   "MACD_12_26_9_Cross_Signal_15min",
   "EMA_20_15min",
   "ATR_14_15min",
   "BBU_20_2.0_15min",
   "BBL_20_2.0_15min",
   "close_vs_BB_Upper_15min",
   "close_vs_BB_Lower_15min",

   # --- 1-hour: Volume / Macro volatility ---
   "VWAP_D_1h",
   "Volume_SMA_20_1h",
   "ATR_14_1h",

   # --- Lagged Features (from 5m only) ---
   "RSI_14_Lag_1_5min",
   "Candle_Body_Lag_1_5min",
   "Volume_SMA_20_Lag_1_5min",

   # --- Time & Session Flags ---
   "Hour_of_Day",
   "Minute_of_Hour",
   "Day_of_Week",
   "Time_Sin",
   "Time_Cos",
   "Day_Sin",
   "Day_Cos",
   "Is_Asian_Session",
   "Is_London_Session",
   "Is_NY_Session",
   "Is_Overlap",
   "Is_US_Open_Hour",
   "Is_US_Close_Hour"
]

def is_new_bar(current_time, last_time, interval_minutes):
    if last_time is None:
        return True
    return (current_time - last_time).total_seconds() >= interval_minutes * 60

def prepare_live_feature_vector(df_5m, df_15m_raw, df_1h_raw, current_time):
    global last_15m_update, last_1h_update, cached_15m_features, cached_1h_features

    df_5m_f = compute_all_indicators(df_5m.copy(), suffix="_5min", indicators=["ATR", "RSI", "MACD", "MACD", "EMA", "CDL", "RETURN", "SLOPE", "LAG", "Volume_SMA"])
    df_5m_f = add_time_session_features(df_5m_f.copy())

    if is_new_bar(current_time, last_15m_update, 15):
        df_15m_f = compute_all_indicators(df_15m_raw.copy(), suffix="_15min", indicators=["MACD", "EMA", "ATR", "BB", ""])
        cached_15m_features = df_15m_f
        last_15m_update = current_time
    else:
        df_15m_f = cached_15m_features

    if is_new_bar(current_time, last_1h_update, 60):
        df_1h_f = compute_all_indicators(df_1h_raw.copy(), suffix="_1h", indicators=["VWAP", "Volume_SMA", "ATR"])
        cached_1h_features = df_1h_f
        last_1h_update = current_time
    else:
        df_1h_f = cached_1h_features

    df = pd.merge_asof(left=df_5m_f.sort_index(), right=df_15m_f.filter(regex="_15min$").sort_index(),
                       left_index=True, right_index=True, direction='backward')
    df = pd.merge_asof(left=df.sort_index(), right=df_1h_f.filter(regex="_1h$").sort_index(),
                       left_index=True, right_index=True, direction='backward')

    if 'datetime' in df.columns:
        df.set_index('datetime', inplace=True)
    # üß† Forward-fill missing values to allow model prediction
    df.ffill(inplace=True)
    df.head(13)

    # Check last 60 rows for any missing values in model features
    # Identify missing columns (not just NaNs, but truly absent)
    missing_cols_absent = [col for col in model_features if col not in df.columns]

    if missing_cols_absent:
        print(f"‚ùå Columns missing entirely from the DataFrame:")
        for col in missing_cols_absent:
            print(f"   - {col}")
        print("üí° This usually means they weren't generated by your indicator_calculation module.")
        return pd.DataFrame()  # Return empty to prevent crash

    # Proceed to check NaNs only in available model columns
    missing_summary = df[model_features].tail(60).isnull().sum()
    missing_cols = missing_summary[missing_summary > 0]

    if not missing_cols.empty:
        print(f"‚ö†Ô∏è Missing (NaN) values in the last 60 rows:")
        print(missing_cols.sort_values(ascending=False))
        for col in missing_cols.index:
            print(f"\nüîç Missing values in column: {col}")
            print(df[[col]].tail(60).dropna().head())
            print(df[[col]].tail(60).isna().astype(int).T)

    missing_cols = missing_summary[missing_summary > 0]

    if not missing_cols.empty:
        print(f"‚ö†Ô∏è Missing values in the last 60 rows:")
        print(missing_cols.sort_values(ascending=False))
        print("\nüß™ Sample rows with missing data:")
        for col in missing_cols.index:
            print(f"\nüîç Missing values in column: {col}")
            print(df[[col]].tail(60).dropna().head())  # show some recent valid values
            print(df[[col]].tail(60).isna().astype(int).T)  # show NaN positions

    # Still log if any model features are missing post-merge
    last_row = df.iloc[[-1]]
    if last_row.isnull().any().any():
        missing_cols = last_row.columns[last_row.isnull().any()].tolist()
        print(f"‚ö†Ô∏è Merged row still missing data after ffill: {missing_cols}")
        return pd.DataFrame()

    return last_row

def check_trade_status():
    global active_trade_side, entry_price, trail_trigger
    if os.path.exists(STATUS_FILE):
        try:
            with open(STATUS_FILE, "r") as f:
                content = f.read().strip()
            if "flat" in content.lower():
                active_trade_side = None
                entry_price = None
                trail_trigger = None
                return True
            else:
                return False
        except Exception as e:
            print(f"[Status Check] Error reading status file: {e}")

def act_on_model(df_5m_full):
    global active_trade_side, trail_trigger, entry_price, df_window, model_features

    now = df_5m_full.index[-1]
    
    df_15m = df_5m_full.resample("15min", label='right', closed='right').agg({"open": "first", "high": "max", "low": "min", "close": "last", "volume": "sum"}).dropna()
    df_1h = df_5m_full.resample("60min", label='right', closed='right').agg({"open": "first", "high": "max", "low": "min", "close": "last", "volume": "sum"}).dropna()

    df = prepare_live_feature_vector(df_5m_full, df_15m, df_1h, now)
    if df.empty:
        return

    is_flat = check_trade_status()

    missing = set(model_features) - set(df.columns)
    if missing:
        print(f"‚ùå Missing features in live data: {missing}")
        return

    ohlcv_cols = df[['open', 'high', 'low', 'close', 'volume']].copy()
    df = df[model_features]

    blocked_start = now.replace(hour=15, minute=55, second=0, microsecond=0)
    blocked_end = now.replace(hour=18, minute=4, second=0, microsecond=0)
    if blocked_start <= now < blocked_end:
        print(f"‚õî Blocked entry between 15:55 and 18:04 ‚Äî current time: {now.strftime('%H:%M')}")
        return
    
    if is_flat:
        pred = model.predict(df)[0]
        PRED_HISTORY.append(pred)
        try:
            with open("pred_history.txt", "w") as f:
                for p in PRED_HISTORY:
                    f.write(f"{p}\n")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to save PRED_HISTORY: {e}")

        if len(PRED_HISTORY) >= 10:
            preds_array = np.array(PRED_HISTORY)
            mean_pred = preds_array.mean()
            std_pred = preds_array.std() + 1e-9
            zscore = (pred - mean_pred) / std_pred
            zscore = np.clip(zscore, -3.0, 3.0)
            conf = np.clip(abs(zscore), 0.0, 2.0)
            position_size = BASE_CONTRACTS + (MAX_CONTRACTS - BASE_CONTRACTS) * (conf / 2.0)
            position_size = round(position_size, 2)
        else:
            conf = 0
            position_size = BASE_CONTRACTS

        if pred >= TRADE_THRESHOLD:
            side = "long"
        elif pred <= -TRADE_THRESHOLD:
            side = "short"
        else:
            print(f"‚è≠Ô∏è Skipping: Prediction {pred:.6f} not strong enough.")
            return

        high_price = ohlcv_cols['high'].iloc[-1]
        low_price = ohlcv_cols['low'].iloc[-1]
        close_price = ohlcv_cols['close'].iloc[-1]
        
        latest = df.iloc[-1]
        entry_price = close_price
        atr = latest['ATR_14_5min']
        
        expected_move = abs(pred) * entry_price
        min_tp = 0.005 * entry_price
        max_tp = TP_ATR_MULT * atr
        tp_move = np.clip(expected_move, min_tp, max_tp)

        sl_move = SL_ATR_MULT * atr
        # if sl_move > tp_move:
        #     sl_move = tp_move
        sl_price = entry_price - sl_move if side == "long" else entry_price + sl_move
        tp_price = entry_price + tp_move if side == "long" else entry_price - tp_move
        trail_trigger = entry_price + TRAIL_START_MULT * atr if side == "long" else entry_price - TRAIL_START_MULT * atr

        active_trade_side = side
        with open(SIGNAL_FILE, "w") as f:
            f.write(f"action: entry\n")
            f.write(f"side: {side}\n")
            f.write(f"price: {entry_price:.2f}\n")
            f.write(f"take_profit: {tp_price:.2f}\n")
            f.write(f"stop_loss: {sl_price:.2f}\n")
            f.write(f"trail_trigger: {trail_trigger:.2f}\n")
            f.write(f"size: {position_size:.2f}\n")

        print(f"[{now}] üöÄ ENTRY: {side.upper()} @ {entry_price:.2f} Size {position_size} | SL: {sl_price:.2f} | TP: {tp_price:.2f} | TrailTrig: {trail_trigger:.2f}")
    else:
        with open(SIGNAL_FILE, "w") as f:
                f.write("")
        high_price = ohlcv_cols['high'].iloc[-1]
        low_price = ohlcv_cols['low'].iloc[-1]
        close_price = ohlcv_cols['close'].iloc[-1]

        latest = df.iloc[-1]
        atr = latest['ATR_14_5min']
        trail_stop = None

        if active_trade_side == 'long' and high_price >= trail_trigger:
            trail_stop = close_price - TRAIL_STOP_MULT * atr
        elif active_trade_side == 'short' and low_price <= trail_trigger:
            trail_stop = close_price + TRAIL_STOP_MULT * atr

        if trail_stop is not None:
            with open(EXIT_FILE, "w") as f:
                f.write(f"action: update\n")
                f.write(f"side: {active_trade_side}\n")
                f.write(f"trail_stop: {trail_stop:.2f}\n")
            print(f"[{now}] üõë Updated | TrailStop: {trail_stop:.2f}")
        else:
            print(f"[{now}] No update needed | Price: {close_price:.2f}")

def process_new_bar(bar_df):
    global df_window
    bar_df.set_index("datetime", inplace=True)
    df_window = pd.concat([df_window, bar_df], axis=0).tail(1000)
    act_on_model(df_window)

def tail_csv(file_path, callback, sleep_interval=0.02):
    with open(file_path, 'r') as f:
        f.seek(0, os.SEEK_END)
        while True:
            line = f.readline()
            if not line:
                time.sleep(sleep_interval)
                continue
            try:
                row_df = pd.read_csv(io.StringIO(line), header=None)
                row_df.columns = ["datetime", "open", "high", "low", "close", "volume"]
                row_df['datetime'] = pd.to_datetime(row_df['datetime'])
                callback(row_df)
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to process line: {line.strip()} | Error: {e}")
                traceback.print_exc()

print("üîÅ Tailing bar_data.csv for new bars...")
tail_csv(BAR_FILE, process_new_bar)
