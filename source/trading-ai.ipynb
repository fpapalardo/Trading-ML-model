{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from ta.trend import (\n",
    "    MACD,\n",
    ")\n",
    "from ta.momentum import (\n",
    "    RSIIndicator,\n",
    "    StochRSIIndicator\n",
    ")\n",
    "from ta.volatility import (\n",
    "    AverageTrueRange,\n",
    "    BollingerBands\n",
    ")\n",
    "from ta.volume import (\n",
    "    VolumeWeightedAveragePrice,\n",
    "    AccDistIndexIndicator\n",
    ")\n",
    "from itertools import product\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit, KFold\n",
    "from sklearn.linear_model import ElasticNet, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import classification_report, mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.base import clone\n",
    "import optuna\n",
    "from sklearn.inspection import permutation_importance\n",
    "import logging\n",
    "import joblib\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "# === Load Data ===\n",
    "#folder_path = \"/Users/francopapalardo-aleo/Desktop/repos/TradingAI 2/data/\"\n",
    "folder_path = \"./data/\"\n",
    "column_names = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "df_list = []\n",
    "plt.rcParams['font.family'] = 'Segoe UI Emoji'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(('.csv', '.txt')):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path, sep=';', header=None, names=column_names)\n",
    "        df['source_file'] = filename\n",
    "        df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "# Resample to 5-minute candles\n",
    "df = df.drop_duplicates(subset='datetime', keep='first').reset_index(drop=True)\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].astype(float)\n",
    "\n",
    "# Base time features\n",
    "df['hour'] = df['datetime'].dt.hour + df['datetime'].dt.minute / 60\n",
    "df['minute'] = df['datetime'].dt.minute\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek  # 0 = Monday\n",
    "\n",
    "# Custom session flags (adjust if needed)       # Regular Trading Hours\n",
    "df['is_premarket'] = df['hour'].between(7, 9.5)\n",
    "df['is_lunch'] = df['hour'].between(11.5, 13.5)\n",
    "df['is_postmarket'] = df['hour'].between(15.5, 20)\n",
    "df['is_after_hours'] = df['hour'].between(20, 23.5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize features or indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Series' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# VWAP\u001b[39;00m\n\u001b[32m     61\u001b[39m vwap = VolumeWeightedAveragePrice(\n\u001b[32m     62\u001b[39m     high=df[\u001b[33m'\u001b[39m\u001b[33mhigh\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     63\u001b[39m     low=df[\u001b[33m'\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m     window=\u001b[32m14\u001b[39m\n\u001b[32m     67\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mvwap\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mvwap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvwap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mvwap_diff\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m'\u001b[39m] - df[\u001b[33m'\u001b[39m\u001b[33mvwap\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     70\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mabove_vwap\u001b[39m\u001b[33m'\u001b[39m] = (df[\u001b[33m'\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m'\u001b[39m] > df[\u001b[33m'\u001b[39m\u001b[33mvwap\u001b[39m\u001b[33m'\u001b[39m]).astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'Series' object is not callable"
     ]
    }
   ],
   "source": [
    "# === Feature Engineering ===\n",
    "# EMA\n",
    "# df['ema_3'] = ta.ema(df['close'], length=3)\n",
    "# df['ema_8'] = ta.ema(df['close'], length=8)\n",
    "# df['ema_9'] = ta.ema(df['close'], length=9)\n",
    "# df['ema_13'] = ta.ema(df['close'], length=13)\n",
    "# df['ema_21'] = ta.ema(df['close'], length=21)\n",
    "# df['ema_34'] = ta.ema(df['close'], length=34)\n",
    "# df['ema_ratio_8_21'] = df['ema_8'] / df['ema_21']\n",
    "# df['ema_diff_8_21'] = df['ema_8'] - df['ema_21']\n",
    "\n",
    "# RSI\n",
    "df['rsi_6'] = RSIIndicator(df['close'], window=6).rsi()\n",
    "# df['rsi_14'] = ta.rsi(df['close'], length=14)\n",
    "# df['rsi_21'] = ta.rsi(df['close'], length=21)\n",
    "\n",
    "# Bollinger Bands for volatility regime\n",
    "bb = BollingerBands(df['close'], window=20, window_dev=2)\n",
    "df['bb_width'] = (bb.bollinger_hband() - bb.bollinger_lband()) / bb.bollinger_mavg()\n",
    "\n",
    "# Stochastic RSI for overbought/oversold with better signals than regular RSI\n",
    "stoch_rsi = StochRSIIndicator(df['close'], window=14, smooth1=3, smooth2=3)\n",
    "df['stoch_rsi_k'] = stoch_rsi.stochrsi_k()\n",
    "df['stoch_rsi_d'] = stoch_rsi.stochrsi_d()\n",
    "\n",
    "# Swing intensity (measures trend strength)\n",
    "def swing_intensity(high, low, length=10):\n",
    "    swing_high = high.rolling(length).max()\n",
    "    swing_low = low.rolling(length).min()\n",
    "    return (swing_high - swing_low) / swing_low\n",
    "\n",
    "# Intraday seasonality\n",
    "df['time_from_open'] = (df['datetime'].dt.hour * 60 + df['datetime'].dt.minute) - 570  # Minutes from 9:30\n",
    "df['normalized_time'] = df['time_from_open'] / 390  # Normalize by trading day length\n",
    "\n",
    "df['swing_intensity'] = swing_intensity(df['high'], df['low'])\n",
    "\n",
    "# Normalized ATR (might be better than raw ATR)\n",
    "df['natr'] = AverageTrueRange(df['high'], df['low'], df['close'], window=14).average_true_range() / df['close']\n",
    "\n",
    "# ATR\n",
    "df['atr_5'] = AverageTrueRange(df['high'], df['low'], df['close'], window=5).average_true_range()\n",
    "df['atr_14'] = AverageTrueRange(df['high'], df['low'], df['close'], window=14).average_true_range()\n",
    "# df['atr_30'] = ta.atr(df['high'], df['low'], df['close'], length=30)\n",
    "df['atr_pct'] = df['atr_14'] / df['close']\n",
    "\n",
    "# Price relative to recent ranges\n",
    "df['close_to_high'] = (df['high'].rolling(10).max() - df['close']) / df['atr_14']\n",
    "df['close_to_low'] = (df['close'] - df['low'].rolling(10).min()) / df['atr_14']\n",
    "\n",
    "# MACD\n",
    "# macd_slow = ta.macd(df['close'], fast=12, slow=26, signal=9)\n",
    "# df['macd_slow'] = macd_slow['MACDh_12_26_9']\n",
    "# df['macd_slow_diff'] = macd_slow['MACD_12_26_9'] - macd_slow['MACDs_12_26_9']\n",
    "\n",
    "macd_fast = MACD(df['close'], window_fast=6, window_slow=13, window_sign=5).macd()\n",
    "df['macd_fast'] = MACD(df['close'], window_fast=6, window_slow=13, window_sign=5).macd_signal()\n",
    "df['macd_fast_diff'] = MACD(df['close'], window_fast=6, window_slow=13, window_sign=5).macd_diff()\n",
    "\n",
    "# VWAP\n",
    "vwap = VolumeWeightedAveragePrice(\n",
    "    high=df['high'],\n",
    "    low=df['low'],\n",
    "    close=df['close'],\n",
    "    volume=df['volume'],\n",
    "    window=14\n",
    ")\n",
    "df['vwap'] = vwap.vwap\n",
    "df['vwap_diff'] = df['close'] - df['vwap']\n",
    "df['above_vwap'] = (df['close'] > df['vwap']).astype(int)\n",
    "df['below_vwap'] = (df['close'] < df['vwap']).astype(int)\n",
    "\n",
    "# # Candle body and total range\n",
    "# df['candle_body'] = abs(df['close'] - df['open'])\n",
    "df['candle_range'] = df['high'] - df['low'] + 1e-9  # avoid division by zero\n",
    "\n",
    "# # Candle body % of range\n",
    "# df['body_pct'] = df['candle_body'] / df['candle_range']\n",
    "\n",
    "# # Wick sizes (relative to range)\n",
    "# df['upper_wick'] = (df['high'] - df[['close', 'open']].max(axis=1)) / df['candle_range']\n",
    "# df['lower_wick'] = (df[['close', 'open']].min(axis=1) - df['low']) / df['candle_range']\n",
    "\n",
    "# Previous\n",
    "# df['prev_close'] = df['close'].shift(1)\n",
    "# df['prev_rsi_14'] = df['rsi_14'].shift(1)\n",
    "# df['prev_macd_fast'] = df['macd_fast'].shift(1)\n",
    "# df['prev_ema_diff'] = df['ema_diff_8_21'].shift(1)\n",
    "\n",
    "# First calculate return_1\n",
    "df['return_1'] = df['close'].pct_change(1)\n",
    "\n",
    "# Then use it for session_vol\n",
    "df['session_vol'] = df.groupby(df['datetime'].dt.date)['return_1'].transform(\n",
    "    lambda x: x.expanding().std()\n",
    ")\n",
    "\n",
    "# Rest of your feature calculations\n",
    "df['time_from_open'] = (df['datetime'].dt.hour * 60 + df['datetime'].dt.minute) - 570\n",
    "df['normalized_time'] = df['time_from_open'] / 390\n",
    "\n",
    "# Volume-weighted momentum\n",
    "df['volume_weighted_return'] = df['return_1'] * (df['volume'] / df['volume'].rolling(20).mean())\n",
    "# df['above_vwap'] = (df['close'] > df['vwap']).astype(int)\n",
    "# df['above_ema_21'] = (df['close'] > df['ema_21']).astype(int)\n",
    "# df['below_vwap'] = (df['close'] < df['vwap']).astype(int)\n",
    "# df['below_ema_21'] = (df['close'] < df['ema_21']).astype(int)\n",
    "# df['velocity'] = df['return_1'] - df['return_1'].shift(1)\n",
    "\n",
    "# df['vol_rolling_mean'] = df['volume'].rolling(20).mean()\n",
    "df['vol_spike'] = df['volume'] / (df['volume'].rolling(20).mean() + 1e-9)\n",
    "\n",
    "df['candle_body'] = (df['close'] - df['open']).abs()\n",
    "df['candle_range'] = df['high'] - df['low'] + 1e-9  # To avoid division by zero\n",
    "\n",
    "df['body_pct'] = df['candle_body'] / df['candle_range']\n",
    "df['upper_wick'] = (df['high'] - df[['close', 'open']].max(axis=1)) / df['candle_range']\n",
    "df['lower_wick'] = (df[['close', 'open']].min(axis=1) - df['low']) / df['candle_range']\n",
    "\n",
    "\n",
    "\n",
    "# Breaks\n",
    "# df['break_high_20'] = (df['high'] > df['high'].rolling(20).max().shift(1)).astype(int)\n",
    "# df['break_low_20'] = (df['low'] < df['low'].rolling(20).min().shift(1)).astype(int)\n",
    "\n",
    "\n",
    "highs = df['high']\n",
    "lows = df['low']\n",
    "\n",
    "def choppiness_index(high, low, close, length=14):\n",
    "    tr = AverageTrueRange(high=high, low=low, close=close, window=length).average_true_range()\n",
    "    atr_sum = tr.rolling(length).sum()\n",
    "    high_max = high.rolling(length).max()\n",
    "    low_min = low.rolling(length).min()\n",
    "    return 100 * np.log10(atr_sum / (high_max - low_min)) / np.log10(length)\n",
    "\n",
    "# def detect_pivot_highs_lows_3(df, lookback=3, lookforward=3):\n",
    "#     df['pivot_high_3'] = highs[(highs.shift(lookback) < highs) & (highs.shift(-lookforward) < highs)]\n",
    "#     df['pivot_low_3'] = lows[(lows.shift(lookback) > lows) & (lows.shift(-lookforward) > lows)]\n",
    "    \n",
    "#     df['is_pivot_high_3'] = df['pivot_high_3'].notna().astype(int)\n",
    "#     df['is_pivot_low_3'] = df['pivot_low_3'].notna().astype(int)\n",
    "#     return df\n",
    "\n",
    "def detect_pivot_highs_lows_5(df, lookback=5, lookforward=5):\n",
    "    df['pivot_high_5'] = highs[(highs.shift(lookback) < highs) & (highs.shift(-lookforward) < highs)]\n",
    "    df['pivot_low_5'] = lows[(lows.shift(lookback) > lows) & (lows.shift(-lookforward) > lows)]\n",
    "    \n",
    "    df['is_pivot_high_5'] = df['pivot_high_5'].notna().astype(int)\n",
    "    df['is_pivot_low_5'] = df['pivot_low_5'].notna().astype(int)\n",
    "    return df\n",
    "\n",
    "def detect_pivot_highs_lows_10(df, lookback=10, lookforward=10):\n",
    "    df['pivot_high_10'] = highs[(highs.shift(lookback) < highs) & (highs.shift(-lookforward) < highs)]\n",
    "    df['pivot_low_10'] = lows[(lows.shift(lookback) > lows) & (lows.shift(-lookforward) > lows)]\n",
    "    \n",
    "    df['is_pivot_high_10'] = df['pivot_high_10'].notna().astype(int)\n",
    "    df['is_pivot_low_10'] = df['pivot_low_10'].notna().astype(int)\n",
    "    return df\n",
    "\n",
    "def add_session_flags(df):\n",
    "    # Ensure datetime is timezone-aware (New York time)\n",
    "    ny_tz = pytz.timezone('America/New_York')\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df['datetime'] = df['datetime'].dt.tz_localize('UTC').dt.tz_convert(ny_tz)\n",
    "\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['minute'] = df['datetime'].dt.minute\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek  # 0 = Monday\n",
    "\n",
    "    # Define sessions in NY time\n",
    "    def classify_session(row):\n",
    "        hour = row['hour']\n",
    "        minute = row['minute']\n",
    "        time_val = hour * 60 + minute\n",
    "\n",
    "        if 19*60 <= time_val < 3*60 + 30 + 1440:  # 7:00 PM to 3:30 AM (Asia)\n",
    "            return 'asia'\n",
    "        elif 3*60 + 30 <= time_val < 8*60:        # 3:30 AM to 8:00 AM (London pre-open)\n",
    "            return 'london_pre'\n",
    "        elif 8*60 <= time_val < 12*60:            # 8:00 AM to 12:00 PM (London/NY overlap)\n",
    "            return 'london_ny'\n",
    "        elif 12*60 <= time_val < 16*60:           # 12:00 PM to 4:00 PM (NY)\n",
    "            return 'ny'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "    df['session'] = df.apply(classify_session, axis=1)\n",
    "\n",
    "    df['is_asian_session'] = (df['session'] == 'asia').astype(int)\n",
    "    df['is_london_session'] = ((df['session'] == 'london_pre') | (df['session'] == 'london_ny')).astype(int)\n",
    "    df['is_ny_session'] = (df['session'] == 'ny').astype(int)\n",
    "    df['is_session_overlap'] = (df['session'] == 'london_ny').astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# def calc_nearest_sr_distance_fast(df):\n",
    "#     highs_idx = df.index[df['is_pivot_high'] == 1].tolist()\n",
    "#     lows_idx = df.index[df['is_pivot_low'] == 1].tolist()\n",
    "\n",
    "#     res_dist = np.full(len(df), np.nan)\n",
    "#     sup_dist = np.full(len(df), np.nan)\n",
    "\n",
    "#     for i in range(len(df)):\n",
    "#         current_close = df.at[i, 'close']\n",
    "\n",
    "#         # Resistance: Find all prior pivot highs\n",
    "#         prior_highs = [abs(current_close - df.at[idx, 'high']) for idx in highs_idx if idx < i]\n",
    "#         res_dist[i] = min(prior_highs) if prior_highs else np.nan\n",
    "\n",
    "#         # Support: Find all prior pivot lows\n",
    "#         prior_lows = [abs(current_close - df.at[idx, 'low']) for idx in lows_idx if idx < i]\n",
    "#         sup_dist[i] = min(prior_lows) if prior_lows else np.nan\n",
    "\n",
    "#     df['dist_to_resistance'] = res_dist\n",
    "#     df['dist_to_support'] = sup_dist\n",
    "\n",
    "#     df['dist_to_res_pct'] = df['dist_to_resistance'] / df['close']\n",
    "#     df['dist_to_sup_pct'] = df['dist_to_support'] / df['close']\n",
    "#     return df\n",
    "\n",
    "# def compute_fvg(df):\n",
    "#     df = df.copy()\n",
    "#     df['fvg_up'] = np.where((df['low'].shift(1) > df['high'].shift(2)), 1, 0)\n",
    "#     df['fvg_down'] = np.where((df['high'].shift(1) < df['low'].shift(2)), 1, 0)\n",
    "#     return df\n",
    "\n",
    "# def compute_liquidity_sweeps(df, swing_window=10):\n",
    "#     df = df.copy()\n",
    "#     df['swing_high'] = df['high'].rolling(window=swing_window, center=False).max().shift(1)\n",
    "#     df['swing_low'] = df['low'].rolling(window=swing_window, center=False).min().shift(1)\n",
    "\n",
    "#     df['liquidity_sweep_high'] = ((df['high'] > df['swing_high']) & (df['close'] < df['swing_high'])).astype(int)\n",
    "#     df['liquidity_sweep_low'] = ((df['low'] < df['swing_low']) & (df['close'] > df['swing_low'])).astype(int)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# df = compute_fvg(df)\n",
    "# df = compute_liquidity_sweeps(df)\n",
    "# df = detect_pivot_highs_lows_3(df)\n",
    "df = detect_pivot_highs_lows_5(df)\n",
    "df = detect_pivot_highs_lows_10(df)\n",
    "df = add_session_flags(df)\n",
    "\n",
    "df['is_pivot_high'] = df[['is_pivot_high_5', 'is_pivot_high_10']].max(axis=1)\n",
    "df['is_pivot_low']  = df[['is_pivot_low_5', 'is_pivot_low_10']].max(axis=1)\n",
    "\n",
    "# df = calc_nearest_sr_distance_fast(df)\n",
    "\n",
    "# === Add Feature ===\n",
    "df['chop_index'] = choppiness_index(df['high'], df['low'], df['close'])\n",
    "\n",
    "# === Strategy Setup ===\n",
    "TICK_VALUE = 5\n",
    "SL_ATR_MULT = 1.0\n",
    "TP_ATR_MULT = 3.0\n",
    "TRAIL_START_MULT = 2.5\n",
    "TRAIL_STOP_MULT = 1.0\n",
    "MAX_CONTRACTS = 1\n",
    "\n",
    "param_grid_strategy = {\n",
    "    'SL_ATR_MULT': [1.0],\n",
    "    'TP_ATR_MULT': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
    "    'TRAIL_START_MULT': [0.5, 1.0, 1.5],\n",
    "    'TRAIL_STOP_MULT': [0.5, 1.0, 1.5],\n",
    "    'TICK_VALUE': [5],  # optional, or expand for futures like NQ/ES\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid_strategy.items())\n",
    "combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "# features = [\n",
    "#     'rsi_6', 'rsi_14', 'rsi_21',\n",
    "#     'ema_3', 'ema_8', 'ema_13', 'ema_9', 'ema_21','ema_34',\n",
    "#     'ema_ratio_8_21', 'ema_diff_8_21',\n",
    "#     'macd_slow', 'macd_slow_diff',\n",
    "#     'macd_fast', 'macd_fast_diff',\n",
    "#     'atr_5', 'atr_30', 'atr_14', 'atr_pct',\n",
    "#     'vwap', 'vwap_diff',\n",
    "#     'candle_body', 'candle_range',\n",
    "#     'volume', 'chop_index',\n",
    "#     'hour', 'minute', 'day_of_week',\n",
    "#     'is_premarket', 'is_lunch',\n",
    "#     'body_pct',  \n",
    "#     'upper_wick', 'lower_wick',  # just added\n",
    "#     'volume_delta_ema',\n",
    "#     'return_1', 'return_3',\n",
    "#     'prev_close', 'prev_rsi_14', 'prev_macd_fast', 'prev_ema_diff',\n",
    "#     'above_vwap', 'above_ema_21',\n",
    "#     'velocity', 'vol_spike',\n",
    "#     'break_high_20', 'break_low_20',\n",
    "#     'is_pivot_high_10', 'is_pivot_low_10',\n",
    "#     'is_pivot_high_3', 'is_pivot_low_3',\n",
    "#     'is_pivot_high_5', 'is_pivot_low_5',\n",
    "#     # 'dist_to_resistance', 'dist_to_support',\n",
    "#     'fvg_up', 'fvg_down',\n",
    "#     'liquidity_sweep_high', 'liquidity_sweep_low',\n",
    "#     'below_vwap', 'below_ema_21',\n",
    "# ]\n",
    "\n",
    "features = [\n",
    "    # Existing features\n",
    "    'is_pivot_low', 'is_pivot_high',\n",
    "    'candle_range', 'rsi_6', 'atr_5',\n",
    "    'macd_fast_diff', 'atr_pct',\n",
    "    'return_1', 'macd_fast', 'volume',\n",
    "    'hour', 'chop_index', 'day_of_week',\n",
    "    \n",
    "    # New features\n",
    "    'bb_width', 'natr',\n",
    "    'stoch_rsi_k', 'stoch_rsi_d',\n",
    "    'volume_weighted_return',\n",
    "    'swing_intensity',\n",
    "    'close_to_high', 'close_to_low',\n",
    "    'session_vol', 'normalized_time',\n",
    "\n",
    "    # Just added\n",
    "    'above_vwap', 'below_vwap',\n",
    "    'vwap', 'vwap_diff',\n",
    "    'vol_spike', 'body_pct',\n",
    "    'upper_wick', 'lower_wick',\n",
    "    'is_asian_session', 'is_london_session', 'is_ny_session',\n",
    "    'is_session_overlap',\n",
    "]\n",
    "\n",
    "avoid_funcs = {\n",
    "}\n",
    "\n",
    "df[features] = df[features].fillna(-999)\n",
    "\n",
    "def is_same_session(start_time, end_time):\n",
    "    session_start = start_time.replace(hour=18, minute=0, second=0)\n",
    "    if start_time.hour < 18:\n",
    "        session_start -= timedelta(days=1)\n",
    "    session_end = session_start + timedelta(hours=23)\n",
    "    return session_start <= start_time <= session_end and session_start <= end_time <= session_end\n",
    "\n",
    "combo_trades = defaultdict(set)\n",
    "\n",
    "def combo_overlap(c1, c2):\n",
    "    a, b = combo_trades[frozenset(c1)], combo_trades[frozenset(c2)]\n",
    "    if not a or not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / min(len(a), len(b))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Combo function for serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression_combo(\n",
    "    X_test, preds, labeled, df,\n",
    "    avoid_funcs,\n",
    "    SL_ATR_MULT, TP_ATR_MULT, TRAIL_START_MULT, TRAIL_STOP_MULT, TICK_VALUE,\n",
    "    is_same_session,\n",
    "    long_thresh=0.003,\n",
    "    short_thresh=-0.003\n",
    "):\n",
    "    temp_trades_data = []\n",
    "    skipped_trades = 0\n",
    "    avoid_hits = defaultdict(int)\n",
    "    long_trades = 0\n",
    "    short_trades = 0\n",
    "\n",
    "    for i, idx in enumerate(X_test.index):\n",
    "        row = labeled.loc[idx]\n",
    "        pred_return = preds[i]\n",
    "\n",
    "        # Decide trade direction\n",
    "        if pred_return >= long_thresh:\n",
    "            side = 'long'\n",
    "            long_trades += 1\n",
    "        elif pred_return <= short_thresh:\n",
    "            side = 'short'\n",
    "            short_trades += 1\n",
    "        else:\n",
    "            continue  # skip neutral signals\n",
    "\n",
    "        # Trade filters\n",
    "        skip_trade = False\n",
    "        for name, f in avoid_funcs.items():\n",
    "            try:\n",
    "                if f(row):\n",
    "                    avoid_hits[name] += 1\n",
    "                    skip_trade = True\n",
    "            except:\n",
    "                continue\n",
    "        if skip_trade or idx >= len(df) - 6:\n",
    "            skipped_trades += 1\n",
    "            continue\n",
    "\n",
    "        # --- Trade Simulation ---\n",
    "        entry_price = row['close']\n",
    "        entry_time = row['datetime']\n",
    "        atr = row['atr_14']\n",
    "\n",
    "        # Stop Loss (fixed volatility-based)\n",
    "        sl_price = entry_price - SL_ATR_MULT * atr if side == 'long' else entry_price + SL_ATR_MULT * atr\n",
    "\n",
    "        # Take Profit (dynamic, from model prediction, clipped)\n",
    "        expected_move = abs(pred_return) * entry_price\n",
    "        min_tp = 0.001 * entry_price  # minimum 0.1% move\n",
    "        max_tp = TP_ATR_MULT * atr\n",
    "        tp_move = np.clip(expected_move, min_tp, max_tp)\n",
    "        tp_price = entry_price + tp_move if side == 'long' else entry_price - tp_move\n",
    "\n",
    "        # Trailing logic\n",
    "        trail_trigger = entry_price + TRAIL_START_MULT * atr if side == 'long' else entry_price - TRAIL_START_MULT * atr\n",
    "        trail_stop = None\n",
    "\n",
    "        max_price, min_price = entry_price, entry_price\n",
    "        exit_price, exit_time = None, None\n",
    "\n",
    "        fwd_idx = idx + 1\n",
    "        while fwd_idx < len(df):\n",
    "            fwd_row = df.loc[fwd_idx]\n",
    "            max_price = max(max_price, fwd_row['high'])\n",
    "            min_price = min(min_price, fwd_row['low'])\n",
    "\n",
    "            if (side == 'long' and fwd_row['low'] <= sl_price) or (side == 'short' and fwd_row['high'] >= sl_price):\n",
    "                exit_price = sl_price\n",
    "                exit_time = fwd_row['datetime']\n",
    "                break\n",
    "\n",
    "            if (side == 'long' and fwd_row['high'] >= tp_price) or (side == 'short' and fwd_row['low'] <= tp_price):\n",
    "                exit_price = tp_price\n",
    "                exit_time = fwd_row['datetime']\n",
    "                break\n",
    "\n",
    "            if side == 'long' and fwd_row['high'] >= trail_trigger:\n",
    "                trail_stop = fwd_row['close'] - TRAIL_STOP_MULT * atr\n",
    "            if side == 'short' and fwd_row['low'] <= trail_trigger:\n",
    "                trail_stop = fwd_row['close'] + TRAIL_STOP_MULT * atr\n",
    "\n",
    "            if trail_stop:\n",
    "                if (side == 'long' and fwd_row['low'] <= trail_stop) or (side == 'short' and fwd_row['high'] >= trail_stop):\n",
    "                    exit_price = trail_stop\n",
    "                    exit_time = fwd_row['datetime']\n",
    "                    break\n",
    "\n",
    "            fwd_idx += 1\n",
    "\n",
    "        if exit_price is None:\n",
    "            exit_price = df.loc[len(df) - 1, 'close']\n",
    "            exit_time = df.loc[len(df) - 1, 'datetime']\n",
    "\n",
    "        if not is_same_session(entry_time, exit_time):\n",
    "            continue\n",
    "\n",
    "        GROSS_PNL = (exit_price - entry_price) * TICK_VALUE if side == 'long' else (entry_price - exit_price) * TICK_VALUE\n",
    "        COMMISSION = 3.98\n",
    "        pnl = GROSS_PNL - COMMISSION\n",
    "\n",
    "        mfe = max_price - entry_price if side == 'long' else entry_price - min_price\n",
    "        mae = entry_price - min_price if side == 'long' else max_price - entry_price\n",
    "\n",
    "        temp_trades_data.append({\n",
    "            'datetime': exit_time,\n",
    "            'pnl': pnl,\n",
    "            'mfe': mfe,\n",
    "            'mae': mae,\n",
    "            'gross_pnl': GROSS_PNL\n",
    "        })\n",
    "\n",
    "    # === Metrics ===\n",
    "    results = pd.DataFrame(temp_trades_data)\n",
    "    pnl_total = results['pnl'].sum() if not results.empty else 0\n",
    "    trades = len(results)\n",
    "    win_rate = (results['pnl'] > 0).mean() if not results.empty else 0\n",
    "    expectancy = results['pnl'].mean() if not results.empty else 0\n",
    "    profit_factor = results[results['pnl'] > 0]['pnl'].sum() / abs(results[results['pnl'] < 0]['pnl'].sum()) if not results.empty and (results['pnl'] < 0).any() else np.nan\n",
    "    sharpe = results['pnl'].mean() / (results['pnl'].std() + 1e-9) * np.sqrt(trades) if trades > 1 else 0\n",
    "\n",
    "    return {\n",
    "        'pnl': pnl_total,\n",
    "        'trades': trades,\n",
    "        'win_rate': win_rate,\n",
    "        'expectancy': expectancy,\n",
    "        'profit_factor': profit_factor,\n",
    "        'sharpe': sharpe,\n",
    "        'long_trades': long_trades,\n",
    "        'short_trades': short_trades,\n",
    "        'avoid_hits': dict(avoid_hits),\n",
    "        'results': results\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_future_return_labels(df: pd.DataFrame, lookahead: int, is_same_session_fn) -> pd.DataFrame:\n",
    "   # Create a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Forward-looking feature detection\n",
    "    forward_looking = []\n",
    "    for col in df.columns:\n",
    "        # Check for keywords that suggest forward-looking computation\n",
    "        if any(x in col.lower() for x in ['future', 'next', 'fwd', 'forward']):\n",
    "            forward_looking.append(col)\n",
    "            \n",
    "    if forward_looking:\n",
    "        logging.warning(f\"Potential forward-looking features detected: {forward_looking}\")\n",
    "    \n",
    "    # 2. Ensure proper temporal alignment\n",
    "    def align_features(row_idx):\n",
    "        # Only use data available at prediction time\n",
    "        current_time = df.loc[row_idx, 'datetime']\n",
    "        mask = df['datetime'] < current_time\n",
    "        \n",
    "        # Update rolling calculations to only use past data\n",
    "        for col in df.columns:\n",
    "            if 'rolling' in col or 'ewm' in col:\n",
    "                df.loc[row_idx, col] = df[mask][col].iloc[-1]\n",
    "                \n",
    "    # Apply alignment\n",
    "    for idx in df.index:\n",
    "        align_features(idx)\n",
    "\n",
    "    # 3. Label computation with session boundary check\n",
    "    future_returns = []\n",
    "    trade_dirs = [] \n",
    "\n",
    "    for idx in range(len(df) - lookahead):\n",
    "        start_time = df.loc[idx, 'datetime']\n",
    "        end_time = df.loc[idx + lookahead, 'datetime']\n",
    "\n",
    "        if not is_same_session_fn(start_time, end_time):\n",
    "            future_returns.append(np.nan)\n",
    "            trade_dirs.append(None)\n",
    "            continue\n",
    "\n",
    "        entry_price = df.loc[idx, 'close']\n",
    "        future_price = df.loc[idx + lookahead, 'close']\n",
    "        future_return = (future_price / entry_price) - 1\n",
    "\n",
    "        future_returns.append(future_return)\n",
    "        trade_dirs.append('long' if future_return > 0 else 'short')\n",
    "\n",
    "    # Align output with original df\n",
    "    df_labeled = df.iloc[:len(future_returns)].copy()\n",
    "    df_labeled['future_return'] = future_returns\n",
    "    df_labeled['trade_dir'] = trade_dirs\n",
    "\n",
    "    # Drop NaNs\n",
    "    df_labeled = df_labeled.dropna(subset=['future_return'])\n",
    "\n",
    "    return df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File labeled_data_5.parquet already exists. Skipping...\n",
      "File labeled_data_15.parquet already exists. Skipping...\n",
      "File labeled_data_20.parquet already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "lookahead_values = [5, 15, 20]\n",
    "\n",
    "def label_and_save(lookahead):\n",
    "    df_labeled = compute_future_return_labels(df, lookahead=lookahead, is_same_session_fn=is_same_session)\n",
    "    df_labeled.to_parquet(f\"labeled_data_{lookahead}.parquet\")\n",
    "\n",
    "for lookahead in lookahead_values:\n",
    "    if os.path.exists(f\"labeled_data_{lookahead}.parquet\"):\n",
    "        print(f\"File labeled_data_{lookahead}.parquet already exists. Skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Processing lookahead {lookahead}...\")\n",
    "        label_and_save(lookahead)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overfit(model, X_tr, X_te):\n",
    "    train_preds = model.predict(X_tr)\n",
    "    test_preds = model.predict(X_te)\n",
    "    train_mse = mean_squared_error(y_train, train_preds)\n",
    "    test_mse = mean_squared_error(y_test, test_preds)\n",
    "    ratio = test_mse / train_mse if train_mse != 0 else float('inf')\n",
    "\n",
    "    print(f\"\\n📉 Overfitting check:\")\n",
    "    print(f\"Train MSE: {train_mse:.8f}\")\n",
    "    print(f\"Test MSE: {test_mse:.8f}\")\n",
    "    print(f\"Overfit ratio (Test / Train): {ratio:.2f}\")\n",
    "    if ratio > 1.5:\n",
    "        print(\"⚠️ Potential overfitting detected.\")\n",
    "    elif ratio < 0.7:\n",
    "        print(\"⚠️ Possibly underfitting (too simple).\")\n",
    "    else:\n",
    "        print(\"✅ Generalization looks reasonable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_oof_predictions(models, X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    Generate out-of-fold predictions from a list of trained base models.\n",
    "    Returns a matrix of shape (len(X), len(models)) with OOF predictions.\n",
    "    \"\"\"\n",
    "    oof_preds = np.zeros((len(X), len(models)))\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        fold_preds = np.zeros(len(X))\n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            clone_model = clone(model)  # Avoid reusing fitted model\n",
    "            clone_model.fit(X[train_idx], y[train_idx])\n",
    "            fold_preds[val_idx] = clone_model.predict(X[val_idx])\n",
    "        oof_preds[:, i] = fold_preds\n",
    "    return oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lookahead(LOOKAHEAD):\n",
    "    print(f\"\\n🔍 Running LOOKAHEAD={LOOKAHEAD}\")\n",
    "    logging.info(f\"Loading labeled data for LOOKAHEAD={LOOKAHEAD}\")\n",
    "\n",
    "    # === Load and split data ===\n",
    "    labeled = pd.read_parquet(f\"labeled_data_{LOOKAHEAD}.parquet\")\n",
    "    labeled = labeled.replace([np.inf, -np.inf], np.nan)\n",
    "    labeled = labeled.dropna(subset=features + ['future_return'])\n",
    "\n",
    "    cutoff_date = pd.Timestamp(\"2025-05-01\")\n",
    "    train = labeled[labeled['datetime'] < cutoff_date]\n",
    "    test = labeled[labeled['datetime'] >= cutoff_date]\n",
    "\n",
    "    X_train_full, y_train = train[features], train['future_return']\n",
    "    X_test_full, y_test = test[features], test['future_return']\n",
    "\n",
    "    print(f\"Train range: {train['datetime'].min()} to {train['datetime'].max()} | Rows: {len(train)}\")\n",
    "    print(f\"Test range: {test['datetime'].min()} to {test['datetime'].max()} | Rows: {len(test)}\")\n",
    "\n",
    "    # === Step 1: Initial RF Training ===\n",
    "    def objective(trial):\n",
    "        try:\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 20, 40),\n",
    "                'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 20),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 40),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 20, 300),  # controls model complexity\n",
    "            }\n",
    "            model = RandomForestRegressor(**params, random_state=42, n_jobs=-5)\n",
    "            tscv = TimeSeriesSplit(n_splits=3)\n",
    "            return cross_val_score(model, X_train_full, y_train, cv=tscv, scoring='neg_mean_squared_error').mean()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Trial failed: {e}\")\n",
    "            return float('-inf')\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        study_name='rf_opt',\n",
    "        storage=f'sqlite:///rf_opt_study{LOOKAHEAD}.db',\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(objective, n_trials=70)\n",
    "\n",
    "    rf_best_params = study.best_params\n",
    "    print(f\"✅ Best hyperparameters: {rf_best_params}\")\n",
    "\n",
    "    # === Step 2: Fit on Full Feature Set to Extract Importance ===\n",
    "    rf_full = RandomForestRegressor(**rf_best_params, random_state=42, n_jobs=-5)\n",
    "    rf_full.fit(X_train_full, y_train)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train_full.columns,\n",
    "        'importance': rf_full.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    print(\"\\n📊 Top 25 Feature Importances (from full set):\")\n",
    "    print(importance_df.head(25))\n",
    "\n",
    "    # === Step 3: Retrain on Top-N Features ===\n",
    "    top_features = importance_df.head(25)['feature'].tolist()\n",
    "    X_train, X_test = X_train_full[top_features], X_test_full[top_features]\n",
    "\n",
    "    rf_best = RandomForestRegressor(**rf_best_params, random_state=42, n_jobs=-5)\n",
    "    rf_best.fit(X_train, y_train)\n",
    "\n",
    "    check_overfit(rf_best, X_train, X_test)\n",
    "\n",
    "    # === Step 5: Permutation Importance ===\n",
    "    perm_df = permutation_importance(\n",
    "        rf_best, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-5\n",
    "    )\n",
    "    perm_df = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'importance': perm_df.importances_mean\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    print(\"\\n📊 Top 25 Permutation Important Features:\")\n",
    "    print(perm_df.head(25))\n",
    "\n",
    "     # === Step 6: Combine RF + L1-selected features ===\n",
    "    print(\"\\n🧠 Combining RF + L1 features...\")\n",
    "\n",
    "    top_rf_features = importance_df.head(20)['feature'].tolist()\n",
    "\n",
    "    if 'selected_features_l1' not in globals():\n",
    "        print(\"⚠️ 'selected_features_l1' not defined. Using only RF top features.\")\n",
    "        combined_features = top_rf_features\n",
    "    # else:\n",
    "    #     combined_features = list(set(top_rf_features + selected_features_l1.tolist()))\n",
    "\n",
    "    print(f\"\\n🔧 Combined selected features (RF + L1):\")\n",
    "    print(combined_features)\n",
    "\n",
    "    X_train_combined = X_train_full[combined_features]\n",
    "    X_test_combined = X_test_full[combined_features]\n",
    "\n",
    "    # === Step 7: Train individual models on combined features ===\n",
    "    print(\"\\n⚙️ Training individual models...\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "    X_test_scaled = scaler.transform(X_test_combined)\n",
    "\n",
    "    def tune_xgb(X_train, y_train):\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'gamma': trial.suggest_float('gamma', 0, 5.0),  # regularization – helps pruning\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0)\n",
    "            }\n",
    "            model = XGBRegressor(**params, eval_metric='rmse', random_state=42)\n",
    "            return cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name='xgb_opt',\n",
    "            storage=f'sqlite:///xgb_opt_study{LOOKAHEAD}.db',\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=400)\n",
    "        return study.best_params\n",
    "    \n",
    "    def tune_elasticnet(X_train, y_train):\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'alpha': trial.suggest_float('alpha', 1e-4, 1.0, log=True),\n",
    "                'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "            }\n",
    "            model = ElasticNet(**params, max_iter=1000)\n",
    "            return cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name='elasticnet_opt',\n",
    "            storage=f'sqlite:///elasticnet_opt_study{LOOKAHEAD}.db',\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=400)\n",
    "        return study.best_params\n",
    "\n",
    "    xgb_params = tune_xgb(X_train_scaled, y_train)\n",
    "    enet_params = tune_elasticnet(X_train_scaled, y_train)\n",
    "\n",
    "    xgb = XGBRegressor(**xgb_params, eval_metric='rmse', random_state=42)\n",
    "    elasticnet = ElasticNet(**enet_params, max_iter=1000)\n",
    "    rf_best_combined = RandomForestRegressor(**rf_best_params, random_state=42, n_jobs=-5)\n",
    "\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "    elasticnet.fit(X_train_scaled, y_train)\n",
    "    rf_best_combined.fit(X_train_combined, y_train)\n",
    "\n",
    "    # Meta Model\n",
    "    X_train_rf = pd.DataFrame(X_train_scaled, columns=X_train_combined.columns, index=X_train_combined.index)\n",
    "\n",
    "    def tune_meta_xgb(X_train, y_train, base_models_preds):\n",
    "        \"\"\"\n",
    "        Tune XGBRegressor as the meta-learner using predictions from base models as input.\n",
    "        \"\"\"\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "            }\n",
    "            model = XGBRegressor(**params, random_state=42)\n",
    "            return cross_val_score(model, base_models_preds, y_train, cv=3, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name='meta_xgb_stack',\n",
    "            storage=f'sqlite:///meta_xgb_stack_{LOOKAHEAD}.db',\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=400)\n",
    "        return study.best_params\n",
    "    \n",
    "    base_models = [\n",
    "        rf_best_combined,\n",
    "        xgb,\n",
    "        elasticnet\n",
    "    ]\n",
    "\n",
    "    X_meta = X_train_scaled  # All your models use scaled inputs\n",
    "    base_models_preds_train = generate_oof_predictions(base_models, X_meta, y_train)\n",
    "\n",
    "    simple_avg = base_models_preds_train.mean(axis=1)\n",
    "    print(\"Simple OOF ensemble MAE:\", mean_absolute_error(y_train, simple_avg))\n",
    "\n",
    "    np.save(\"meta_features.npy\", base_models_preds_train)\n",
    "\n",
    "    meta_params = tune_meta_xgb(X_train_scaled, y_train, base_models_preds_train)\n",
    "    print(\"🔍 Best meta-model params:\", meta_params)\n",
    "\n",
    "    meta_model = XGBRegressor(**meta_params, random_state=42)\n",
    "\n",
    "    # === Step 8: Ensemble  Regressor ===\n",
    "    stack = StackingRegressor(\n",
    "        estimators=[('rf', rf_best_combined), ('xgb', xgb), ('enet', elasticnet)],\n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-5\n",
    "    )\n",
    "    stack.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # === Step 9: Evaluate all models ===\n",
    "    def evaluate_model(name, model, Xtr, Xte, scaled=False):\n",
    "        train_preds = model.predict(Xtr)\n",
    "        test_preds = model.predict(Xte)\n",
    "        train_mse = mean_squared_error(y_train, train_preds)\n",
    "        test_mse = mean_squared_error(y_test, test_preds)\n",
    "        overfit_ratio = test_mse / train_mse if train_mse != 0 else float('inf')\n",
    "\n",
    "        print(f\"\\n📊 {name} Performance:\")\n",
    "        print(f\"Train MSE: {train_mse:.8f}\")\n",
    "        print(f\"Test MSE: {test_mse:.8f}\")\n",
    "        print(f\"Overfit ratio (Test / Train): {overfit_ratio:.2f}\")\n",
    "        if overfit_ratio > 1.5:\n",
    "            print(\"⚠️ Potential overfitting detected.\")\n",
    "        elif overfit_ratio < 0.7:\n",
    "            print(\"⚠️ Possibly underfitting.\")\n",
    "        else:\n",
    "            print(\"✅ Generalization looks reasonable.\")\n",
    "        return test_preds\n",
    "\n",
    "    X_test_rf = pd.DataFrame(X_test_scaled, columns=X_train_combined.columns, index=X_test_combined.index)\n",
    "    X_train_rf = pd.DataFrame(X_train_scaled, columns=X_train_combined.columns, index=X_train_combined.index)\n",
    "    preds_rf = evaluate_model(\"RandomForest\", rf_best_combined, X_train_rf, X_test_rf)\n",
    "    # preds_rf = evaluate_model(\"RandomForest\", rf_best_combined, X_train_scaled, X_test_scaled)\n",
    "\n",
    "    preds_xgb = evaluate_model(\"XGBoost\", xgb, X_train_scaled, X_test_scaled)\n",
    "    preds_elasticnet = evaluate_model(\"ElasticNet\", elasticnet, X_train_scaled, X_test_scaled)\n",
    "    preds_stack   = evaluate_model(\"Stacking Ensemble\", stack, X_train_scaled, X_test_scaled)\n",
    "\n",
    "    # === Step 9.5: Isotopic Regression ===\n",
    "    stack_preds = stack.predict(X_test_scaled)\n",
    "    iso = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso.fit(stack_preds, y_test)\n",
    "\n",
    "    calibrated_preds = iso.predict(stack_preds)\n",
    "    # === Step 10: Choose the final model to backtest ===\n",
    "    preds = calibrated_preds  \n",
    "    X_test = X_test_scaled  \n",
    "\n",
    "    # === Step 11: Backtest Strategy ===\n",
    "    #thresholds = [0.00005, 0.0001, 0.0002, 0.0005, 0.001]\n",
    "    thresholds = [0.0005, 0.001]\n",
    "\n",
    "    X_test_df = pd.DataFrame(X_test_scaled, columns=combined_features, index=X_test_combined.index)\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    y_pred = stack.predict(X_test_df)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "    for params in combinations:\n",
    "        for thresh in thresholds:\n",
    "            results = evaluate_regression_combo(\n",
    "                X_test=X_test_df,\n",
    "                preds=preds,\n",
    "                labeled=labeled,\n",
    "                df=df,\n",
    "                avoid_funcs=avoid_funcs,\n",
    "                SL_ATR_MULT=SL_ATR_MULT,\n",
    "                TP_ATR_MULT=TP_ATR_MULT,\n",
    "                TRAIL_START_MULT=TRAIL_START_MULT,\n",
    "                TRAIL_STOP_MULT=TRAIL_STOP_MULT,\n",
    "                TICK_VALUE=TICK_VALUE,\n",
    "                is_same_session=is_same_session,\n",
    "                long_thresh=thresh,\n",
    "                short_thresh=-thresh\n",
    "            )\n",
    "\n",
    "            results['params'] = params\n",
    "            all_results.append(results)\n",
    "\n",
    "            print(\n",
    "                f\"\\n✅ LOOKAHEAD={LOOKAHEAD} | Threshold={thresh}\"\n",
    "                f\"\\nPnL: ${results['pnl']:.2f}\"\n",
    "                f\"\\nTrades: {results['trades']}\"\n",
    "                f\"\\nWin Rate: {results['win_rate']:.2%}\"\n",
    "                f\"\\nExpectancy: {results['expectancy']:.2f}\"\n",
    "                f\"\\nProfit Factor: {results['profit_factor']:.2f}\"\n",
    "                f\"\\nSharpe Ratio: {results['sharpe']:.2f}\"\n",
    "                f\"\\nLong Trades: {results['long_trades']} | Short Trades: {results['short_trades']}\"\n",
    "            )\n",
    "\n",
    "            print(\"Avoid Hits:\")\n",
    "            for name, count in results['avoid_hits'].items():\n",
    "                print(f\" - {name}: {count}\")\n",
    "\n",
    "            print(\"\\n🔢 Top 5 PnL trades:\")\n",
    "            print(results['results'].sort_values(by='pnl', ascending=False).head(5))\n",
    "\n",
    "            print(\"\\n🔻 Bottom 5 PnL trades:\")\n",
    "            print(results['results'].sort_values(by='pnl', ascending=True).head(5))\n",
    "\n",
    "\n",
    "    summary_df = pd.DataFrame([{\n",
    "        'pnl': r['pnl'],\n",
    "        'sharpe': r['sharpe'],\n",
    "        'expectancy': r['expectancy'],\n",
    "        'profit_factor': r['profit_factor'],\n",
    "        'win_rate': r['win_rate'],\n",
    "        'trades': r['trades'],\n",
    "        **r['params']\n",
    "    } for r in all_results])\n",
    "    top = summary_df.sort_values(by='sharpe', ascending=False).head(5)\n",
    "    print(top)\n",
    "\n",
    "    metadata = {\n",
    "        \"lookahead\": LOOKAHEAD,\n",
    "        \"train_range\": [str(train[\"datetime\"].min()), str(train[\"datetime\"].max())],\n",
    "        \"test_range\": [str(test[\"datetime\"].min()), str(test[\"datetime\"].max())],\n",
    "        \"features_used\": combined_features,\n",
    "        \"rf_params\": rf_best_params,\n",
    "        \"xgb_params\": xgb_params,\n",
    "        \"enet_params\": enet_params\n",
    "    }\n",
    "    with open(f\"model_metadata_{LOOKAHEAD}.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "        \n",
    "    joblib.dump(meta_params, f\"meta_model_params_LOOKAHEAD_{LOOKAHEAD}_05-14.pkl\")\n",
    "    joblib.dump(stack, f\"stack_model_LOOKAHEAD_{LOOKAHEAD}_05-14.pkl\")\n",
    "    joblib.dump(scaler, f\"scaler_LOOKAHEAD_{LOOKAHEAD}_05-14.pkl\")\n",
    "    joblib.dump(study.trials_dataframe(), f\"xgb_trials_df_{LOOKAHEAD}_05-14.pkl\")\n",
    "\n",
    "    return {\n",
    "        'lookahead': LOOKAHEAD,\n",
    "        'pnl': results['pnl'],\n",
    "        'win_rate': results['win_rate'],\n",
    "        'expectancy': results['expectancy'],\n",
    "        'profit_factor': results['profit_factor'],\n",
    "        'sharpe': results['sharpe'],\n",
    "        'trades': results['trades'],\n",
    "        'best_params': rf_best_params,\n",
    "        'preds_rf': preds_rf,\n",
    "        'preds_xgb': preds_xgb,\n",
    "        'preds_elasticnet': preds_elasticnet,\n",
    "        'preds_stack': preds_stack\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookahead_values = [5, 15]\n",
    "\n",
    "lookahead_results = Parallel(n_jobs=1)(\n",
    "    delayed(run_lookahead)(val) for val in lookahead_values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in lookahead_results:\n",
    "    stack_preds = result['stack'].predict(X_test_scaled)\n",
    "    rf_preds = result['models']['rf'].predict(X_test_scaled)\n",
    "    xgb_preds = result['models']['xgb'].predict(X_test_scaled)\n",
    "    enet_preds = result['models']['elasticnet'].predict(X_test_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(rf_preds[:100], label='RF')\n",
    "    plt.plot(xgb_preds[:100], label='XGB')\n",
    "    plt.plot(enet_preds[:100], label='ElasticNet')\n",
    "    plt.plot(stack_preds[:100], label='Stack', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in lookahead_results:\n",
    "    df = result['results_df'].copy()\n",
    "    df = df.sort_values(by='datetime')  # Ensure correct order\n",
    "    df['cumulative_pnl'] = df['pnl'].cumsum()\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(df['datetime'], df['cumulative_pnl'], label='Cumulative PnL', color='green')\n",
    "    plt.title(f\"Cumulative PnL (LOOKAHEAD={result['lookahead']})\")\n",
    "    plt.xlabel(\"Datetime\")\n",
    "    plt.ylabel(\"PnL\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef([lookahead_results['preds_rf'], lookahead_results['preds_xgb'], lookahead_results['preds_elasticnet']])\n",
    "preds_matrix = np.vstack([lookahead_results['preds_rf'], lookahead_results['preds_xgb'], lookahead_results['preds_elasticnet']])\n",
    "corr_matrix = np.corrcoef(preds_matrix)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(corr_matrix, annot=True, xticklabels=['RF', 'XGB', 'ENet'], yticklabels=['RF', 'XGB', 'ENet'], cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Between Base Model Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[570]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# y_pred = best_lookahead.predict(X_test)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m best_lookahead = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlookahead_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpnl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresults\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m y_pred = best_lookahead[\u001b[33m'\u001b[39m\u001b[33mstack\u001b[39m\u001b[33m'\u001b[39m].predict(X_test_scaled)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Confusion Matrix\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "# y_pred = best_lookahead.predict(X_test)\n",
    "best_lookahead = max(lookahead_results, key=lambda x: max(r['pnl'] for r in x['results']))\n",
    "y_pred = best_lookahead['stack'].predict(X_test_scaled)\n",
    "\n",
    "# Confusion Matrix\n",
    "labels = sorted(class_mapping)  # Make sure the order matches\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "# Display Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, labels=labels, digits=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
