{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import platform\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "from scipy.stats.mstats import winsorize\n",
    "import numba\n",
    "\n",
    "# TA Indicators\n",
    "from ta.volatility import BollingerBands, AverageTrueRange\n",
    "from ta.trend import SMAIndicator, EMAIndicator, MACD, ADXIndicator, PSARIndicator, CCIIndicator\n",
    "from ta.momentum import RSIIndicator, StochasticOscillator, PercentagePriceOscillator, ROCIndicator\n",
    "#\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import Huber, MeanSquaredError\n",
    "import tensorflow as tf\n",
    "#\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.base import clone, BaseEstimator, RegressorMixin, ClassifierMixin\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, root_mean_squared_error, mean_squared_error, mean_absolute_error, r2_score, accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n",
    "\n",
    "# Models and Training\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import shap\n",
    "#\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*There are no meaningful features.*\", category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./../data/\"\n",
    "column_names = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "df_list = []\n",
    "\n",
    "# Set emoji-compatible font based on OS (Optional, fine as is)\n",
    "system = platform.system()\n",
    "if system == 'Windows':\n",
    "    plt.rcParams['font.family'] = 'Segoe UI Emoji'\n",
    "elif system == 'Linux':\n",
    "    plt.rcParams['font.family'] = 'Noto Color Emoji'\n",
    "\n",
    "# Read all files\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(('.csv', '.txt')):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Added error_bad_lines=False and warn_bad_lines=True for robustness\n",
    "        df_temp = pd.read_csv(file_path, sep=';', header=None, names=column_names, \n",
    "                              on_bad_lines='warn') # Consider 'skip' or 'warn'\n",
    "        df_list.append(df_temp)\n",
    "\n",
    "# Combine and clean\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True).dt.tz_convert('America/New_York')\n",
    "df = df.drop_duplicates(subset='datetime', keep='first').reset_index(drop=True)\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].astype(float)\n",
    "df = df.set_index('datetime') # Set index ONCE before resampling\n",
    "\n",
    "# === ðŸ“Š Resample to 5-minute and 15-minute candles ===\n",
    "df_5min = df.resample('5min').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last',\n",
    "    'volume': 'sum'\n",
    "}).dropna()\n",
    "\n",
    "df_15min = df.resample('15min').agg({ # Resample from original for accuracy\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last',\n",
    "    'volume': 'sum'\n",
    "}).dropna()\n",
    "\n",
    "# Now df_5min and df_15min have datetime as their index. Ready for features!\n",
    "print(\"5-minute data shape:\", df_5min.shape)\n",
    "print(\"15-minute data shape:\", df_15min.shape)\n",
    "print(\"\\n5-minute data head:\\n\", df_5min.head())\n",
    "print(\"\\n15-minute data head:\\n\", df_15min.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions for Custom Features (Keep these as they are) ---\n",
    "def add_price_vs_ma(df, price_col='close', ma_col_name='EMA_20', new_col_name_suffix='_vs_EMA20'):\n",
    "    if ma_col_name in df.columns and price_col in df.columns:\n",
    "        df[price_col + new_col_name_suffix] = df[price_col] / df[ma_col_name]\n",
    "    return df\n",
    "\n",
    "def add_ma_vs_ma(df, ma1_col_name='EMA_10', ma2_col_name='EMA_20', new_col_name_suffix='_vs_EMA20'):\n",
    "    if ma1_col_name in df.columns and ma2_col_name in df.columns:\n",
    "        df[ma1_col_name + new_col_name_suffix] = df[ma1_col_name] / df[ma2_col_name]\n",
    "    return df\n",
    "\n",
    "def add_ma_slope(df, ma_col_name='EMA_10', new_col_name_suffix='_Slope_10', periods=1):\n",
    "    if ma_col_name in df.columns:\n",
    "        df[new_col_name_suffix] = df[ma_col_name].diff(periods) / periods\n",
    "    return df\n",
    "\n",
    "def add_rsi_signals(df, rsi_col_name='RSI_14', ob_level=70, os_level=30):\n",
    "    if rsi_col_name in df.columns:\n",
    "        df[rsi_col_name + f'_Is_Overbought_{ob_level}'] = (df[rsi_col_name] > ob_level).astype(int)\n",
    "        df[rsi_col_name + f'_Is_Oversold_{os_level}'] = (df[rsi_col_name] < os_level).astype(int)\n",
    "    return df\n",
    "\n",
    "def add_stoch_signals(df, stoch_k_col_name='Stoch_K_14_3', ob_level=80, os_level=20):\n",
    "    if stoch_k_col_name in df.columns:\n",
    "        df[stoch_k_col_name + f'_Is_Overbought_{ob_level}'] = (df[stoch_k_col_name] > ob_level).astype(int)\n",
    "        df[stoch_k_col_name + f'_Is_Oversold_{os_level}'] = (df[stoch_k_col_name] < os_level).astype(int)\n",
    "    return df\n",
    "\n",
    "def add_macd_cross_signal(df, macd_col_name='MACD_12_26_9', signal_col_name='MACD_Signal_12_26_9'):\n",
    "    if macd_col_name in df.columns and signal_col_name in df.columns:\n",
    "        crossed_above = (df[macd_col_name] > df[signal_col_name]) & (df[macd_col_name].shift(1) < df[signal_col_name].shift(1))\n",
    "        crossed_below = (df[macd_col_name] < df[signal_col_name]) & (df[macd_col_name].shift(1) > df[signal_col_name].shift(1))\n",
    "        df[macd_col_name + '_Cross_Signal'] = np.where(crossed_above, 1, np.where(crossed_below, -1, 0))\n",
    "    return df\n",
    "\n",
    "def add_price_vs_bb(df, price_col='close', bb_upper_col='BB_Upper_20_2', bb_lower_col='BB_Lower_20_2'):\n",
    "    if price_col in df.columns and bb_upper_col in df.columns and bb_lower_col in df.columns:\n",
    "        df[price_col + '_vs_BB_Upper'] = (df[price_col] > df[bb_upper_col]).astype(int)\n",
    "        df[price_col + '_vs_BB_Lower'] = (df[price_col] < df[bb_lower_col]).astype(int)\n",
    "    return df\n",
    "\n",
    "def add_psar_flip_signal(df, psar_col_name='PSAR'):\n",
    "    if psar_col_name in df.columns and 'close' in df.columns:\n",
    "        uptrend_now = df[psar_col_name] < df['close']\n",
    "        uptrend_prev = df[psar_col_name].shift(1) < df['close'].shift(1)\n",
    "        flipped_to_up = uptrend_now & ~uptrend_prev\n",
    "        flipped_to_down = ~uptrend_now & uptrend_prev\n",
    "        df[psar_col_name + '_Flip_Signal'] = np.where(flipped_to_up, 1, np.where(flipped_to_down, -1, 0))\n",
    "    return df\n",
    "\n",
    "def add_daily_vwap(df, high_col='high', low_col='low', close_col='close', volume_col='volume', new_col_name='VWAP'):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"Error: DataFrame index must be DatetimeIndex for daily VWAP.\")\n",
    "        return df\n",
    "    tpv = ((df[high_col] + df[low_col] + df[close_col]) / 3) * df[volume_col]\n",
    "    daily_grouped_tpv = tpv.groupby(df.index.date)\n",
    "    cumulative_tpv = daily_grouped_tpv.cumsum()\n",
    "    daily_grouped_volume = df[volume_col].groupby(df.index.date)\n",
    "    cumulative_volume = daily_grouped_volume.cumsum()\n",
    "    df[new_col_name] = cumulative_tpv / cumulative_volume\n",
    "    df[new_col_name].replace([np.inf, -np.inf], np.nan, inplace=True) # Handle potential inf from division by zero volume at start of day\n",
    "    return df\n",
    "\n",
    "def add_candle_features(df):\n",
    "    df['Candle_Range'] = df['high'] - df['low']\n",
    "    df['Candle_Body'] = (df['close'] - df['open']).abs()\n",
    "    df['Upper_Wick'] = df['high'] - np.maximum(df['open'], df['close'])\n",
    "    df['Lower_Wick'] = np.minimum(df['open'], df['close']) - df['low']\n",
    "    df['Body_vs_Range'] = (df['Candle_Body'] / df['Candle_Range']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    return df\n",
    "\n",
    "def add_candlestick_patterns(df):\n",
    "    if 'Candle_Body' not in df.columns or 'Candle_Range' not in df.columns: # Ensure dependencies exist\n",
    "        df = add_candle_features(df)\n",
    "    df['Is_Doji'] = ((df['Candle_Body'] / df['Candle_Range']).replace([np.inf, -np.inf], np.nan) < 0.1).astype(int)\n",
    "    df['Is_Hammer'] = (\n",
    "        (df['Upper_Wick'] < df['Candle_Body']) & \\\n",
    "        (df['Lower_Wick'] > 2 * df['Candle_Body']) & \\\n",
    "        ((df['close'] > (df['high'] - df['Candle_Range'].replace([np.inf, -np.inf], np.nan) * 0.3)) | \\\n",
    "         (df['open'] > (df['high'] - df['Candle_Range'].replace([np.inf, -np.inf], np.nan) * 0.3)))\n",
    "    ).astype(int)\n",
    "    df['Is_Engulfing_Bullish'] = (\n",
    "        (df['close'] > df['open']) & (df['close'].shift(1) < df['open'].shift(1)) & \\\n",
    "        (df['close'] > df['open'].shift(1)) & (df['open'] < df['close'].shift(1))\n",
    "    ).astype(int)\n",
    "    df['Is_Engulfing_Bearish'] = (\n",
    "        (df['close'] < df['open']) & (df['close'].shift(1) > df['open'].shift(1)) & \\\n",
    "        (df['close'] < df['open'].shift(1)) & (df['open'] > df['close'].shift(1))\n",
    "    ).astype(int)\n",
    "    return df\n",
    "\n",
    "def add_return_features(df, price_col='close'):\n",
    "    df[f'Log_Return_1'] = np.log(df[price_col].replace(0, np.nan) / df[price_col].shift(1).replace(0, np.nan))\n",
    "    df[f'Log_Return_3'] = np.log(df[price_col].replace(0, np.nan) / df[price_col].shift(3).replace(0, np.nan))\n",
    "    df[f'Log_Return_6'] = np.log(df[price_col].replace(0, np.nan) / df[price_col].shift(6).replace(0, np.nan))\n",
    "    df[f'Simple_Return_1'] = df[price_col].pct_change(1)\n",
    "    for col in [f'Log_Return_1', f'Log_Return_3', f'Log_Return_6', f'Simple_Return_1']:\n",
    "        df[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_rolling_stats(df, price_col='close', window1=14, window2=30):\n",
    "    returns = df[price_col].pct_change(1).replace([np.inf, -np.inf], np.nan)\n",
    "    df[f'Rolling_Std_Dev_{window1}'] = returns.rolling(window=window1).std()\n",
    "    df[f'Rolling_Skew_{window2}'] = returns.rolling(window=window2).skew()\n",
    "    df[f'Rolling_Kurtosis_{window2}'] = returns.rolling(window=window2).kurt()\n",
    "    return df\n",
    "\n",
    "def add_lagged_features(df, cols_to_lag, lags=[1, 3, 6]):\n",
    "    for col in cols_to_lag:\n",
    "        if col in df.columns:\n",
    "            for lag in lags:\n",
    "                df[f'{col}_Lag_{lag}'] = df[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "# --- Main Feature Generation Function ---\n",
    "def add_all_features(df, suffix=''):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(f\"Warning: DataFrame for suffix '{suffix}' does not have a DatetimeIndex. Some features might not work as expected (e.g., VWAP).\")\n",
    "    df = df.copy()\n",
    "    base_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    if not all(col in df.columns for col in base_cols):\n",
    "         raise ValueError(f\"DataFrame must contain {base_cols} for suffix {suffix}\")\n",
    "\n",
    "    df['Volume_SMA_20'] = df['volume'].rolling(window=20).mean()\n",
    "    df = add_daily_vwap(df, new_col_name='VWAP')\n",
    "    df = add_price_vs_ma(df, ma_col_name='VWAP', new_col_name_suffix='_vs_VWAP')\n",
    "\n",
    "    bb_indicator = BollingerBands(close=df['close'], window=20, window_dev=2)\n",
    "    df['BB_Upper_20_2'] = bb_indicator.bollinger_hband()\n",
    "    df['BB_Lower_20_2'] = bb_indicator.bollinger_lband()\n",
    "    df['BB_Mid_20'] = bb_indicator.bollinger_mavg()\n",
    "    df['BB_Width_20_2'] = bb_indicator.bollinger_wband()\n",
    "    df['Percent_B_20_2'] = bb_indicator.bollinger_pband()\n",
    "    df = add_price_vs_bb(df, bb_upper_col='BB_Upper_20_2', bb_lower_col='BB_Lower_20_2')\n",
    "    df['ATR_14'] = AverageTrueRange(df['high'], df['low'], df['close'], window=14).average_true_range()\n",
    "\n",
    "    df['SMA_10'] = SMAIndicator(df['close'], window=10).sma_indicator()\n",
    "    df['SMA_20'] = SMAIndicator(df['close'], window=20).sma_indicator()\n",
    "    df['SMA_50'] = SMAIndicator(df['close'], window=50).sma_indicator()\n",
    "    df['EMA_10'] = EMAIndicator(df['close'], window=10).ema_indicator()\n",
    "    df['EMA_20'] = EMAIndicator(df['close'], window=20).ema_indicator()\n",
    "    df['EMA_50'] = EMAIndicator(df['close'], window=50).ema_indicator()\n",
    "    \n",
    "    df = add_price_vs_ma(df, ma_col_name='EMA_20', new_col_name_suffix='_vs_EMA20')\n",
    "    df = add_ma_vs_ma(df, ma1_col_name='EMA_10', ma2_col_name='EMA_20', new_col_name_suffix='_vs_EMA20')\n",
    "    df = add_ma_slope(df, ma_col_name='EMA_10', new_col_name_suffix='_Slope_10')\n",
    "\n",
    "    macd = MACD(close=df['close'], window_slow=26, window_fast=12, window_sign=9)\n",
    "    df['MACD_12_26_9'] = macd.macd()\n",
    "    df['MACD_Signal_12_26_9'] = macd.macd_signal()\n",
    "    df['MACD_Hist_12_26_9'] = macd.macd_diff()\n",
    "    df = add_macd_cross_signal(df)\n",
    "\n",
    "    adx_indicator = ADXIndicator(df['high'], df['low'], df['close'], window=14)\n",
    "    df['ADX_14'] = adx_indicator.adx()\n",
    "    df['Plus_DI_14'] = adx_indicator.adx_pos()\n",
    "    df['Minus_DI_14'] = adx_indicator.adx_neg()\n",
    "    \n",
    "    psar_indicator = PSARIndicator(df['high'], df['low'], df['close'])\n",
    "    df['PSAR'] = psar_indicator.psar()\n",
    "    df = add_psar_flip_signal(df)\n",
    "\n",
    "    df['CCI_20'] = CCIIndicator(df['high'], df['low'], df['close'], window=20).cci()\n",
    "\n",
    "    df['RSI_14'] = RSIIndicator(df['close'], window=14).rsi()\n",
    "    df = add_rsi_signals(df, rsi_col_name='RSI_14')\n",
    "    \n",
    "    stoch = StochasticOscillator(df['high'], df['low'], df['close'], window=14, smooth_window=3)\n",
    "    df['Stoch_K_14_3'] = stoch.stoch()\n",
    "    df['Stoch_D_3'] = stoch.stoch_signal()\n",
    "    df = add_stoch_signals(df, stoch_k_col_name='Stoch_K_14_3')\n",
    "\n",
    "    # --- MODIFIED SECTION FOR PPO AND ROC ---\n",
    "    df['PPO_12_26_9'] = PercentagePriceOscillator(df['close'], window_slow=26, window_fast=12, window_sign=9).ppo()\n",
    "    df['ROC_10'] = ROCIndicator(df['close'], window=10).roc()\n",
    "\n",
    "    # Explicitly handle potential non-numeric types and inf values for PPO and ROC\n",
    "    for col_name in ['PPO_12_26_9', 'ROC_10']:\n",
    "        if col_name in df.columns:\n",
    "            df[col_name] = df[col_name].replace([np.inf, -np.inf], np.nan) # Replace inf with NaN\n",
    "            df[col_name] = pd.to_numeric(df[col_name], errors='coerce')   # Convert to numeric, coerce errors to NaN\n",
    "    # --- END OF MODIFIED SECTION ---\n",
    "\n",
    "    df = add_candle_features(df)\n",
    "    df = add_candlestick_patterns(df)\n",
    "    df = add_return_features(df)\n",
    "    df = add_rolling_stats(df)\n",
    "    \n",
    "    cols_to_lag = ['close', 'RSI_14', 'Candle_Body', 'Volume_SMA_20'] # Ensure these exist before lagging\n",
    "    # Check if cols_to_lag are actually present before trying to lag them\n",
    "    valid_cols_to_lag = [col for col in cols_to_lag if col in df.columns]\n",
    "    df = add_lagged_features(df, valid_cols_to_lag, lags=[1,2,3])\n",
    "\n",
    "    current_cols = list(df.columns)\n",
    "    new_feature_cols = [col for col in current_cols if col not in base_cols] # Features are non-base cols\n",
    "    \n",
    "    rename_dict = {col: col + suffix for col in new_feature_cols}\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Time & Session Features (Keep as is) ---\n",
    "def add_time_session_features(df):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"Error: DataFrame index must be DatetimeIndex for time/session features.\")\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df['Hour_of_Day'] = df.index.hour\n",
    "    df['Minute_of_Hour'] = df.index.minute\n",
    "    df['Day_of_Week'] = df.index.dayofweek\n",
    "    time_fraction_of_day = df['Hour_of_Day'] + df['Minute_of_Hour'] / 60.0\n",
    "    df['Time_Sin'] = np.sin(2 * np.pi * time_fraction_of_day / 24.0)\n",
    "    df['Time_Cos'] = np.cos(2 * np.pi * time_fraction_of_day / 24.0)\n",
    "    df['Day_Sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7.0)\n",
    "    df['Day_Cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7.0)\n",
    "    df['Is_Asian_Session'] = ((df['Hour_of_Day'] >= 20) | (df['Hour_of_Day'] < 5)).astype(int)\n",
    "    df['Is_London_Session'] = ((df['Hour_of_Day'] >= 3) & (df['Hour_of_Day'] < 12)).astype(int)\n",
    "    df['Is_NY_Session'] = ((df['Hour_of_Day'] >= 8) & (df['Hour_of_Day'] < 17)).astype(int)\n",
    "    df['Is_Overlap'] = ((df['Hour_of_Day'] >= 8) & (df['Hour_of_Day'] < 12)).astype(int)\n",
    "    df['Is_US_Open_Hour'] = ((df['Hour_of_Day'] == 9) & (df['Minute_of_Hour'] >= 30) | (df['Hour_of_Day'] == 10) & (df['Minute_of_Hour'] < 30)).astype(int)\n",
    "    df['Is_US_Close_Hour'] = ((df['Hour_of_Day'] == 15) | (df['Hour_of_Day'] == 16) & (df['Minute_of_Hour'] == 0)).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def avoid_news(row):\n",
    "#     ts = row[\"datetime\"]\n",
    "#     return any(start <= ts <= end for (start, end) in news_windows)\n",
    "\n",
    "# def avoid_hour_18_19(row):\n",
    "#     \"\"\"\n",
    "#     Avoid trading in the first hour of the session (18:00 to 19:00 inclusive).\n",
    "#     \"\"\"\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(row['datetime']):\n",
    "#         return False\n",
    "#     hour = row['datetime'].hour\n",
    "#     return hour == 18\n",
    "\n",
    "avoid_funcs = {\n",
    "    #'avoid_hour_18_19': avoid_hour_18_19\n",
    "    #'news_window': avoid_news,\n",
    "}\n",
    "\n",
    "param_grid_strategy = {\n",
    "    'SL_ATR_MULT': [1.0, 1.5, 0.5],\n",
    "    'TP_ATR_MULT': [2.0, 3.0, 4.0],\n",
    "    'TRAIL_START_MULT': [0.5, 1.0],\n",
    "    'TRAIL_STOP_MULT': [0.5, 1.0],\n",
    "    'TICK_VALUE': [5], \n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid_strategy.items())\n",
    "combinations = [dict(zip(keys, v)) for v in product(*values)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell 9: Feature Engineering Execution\n",
    "\n",
    "print(\"--- Starting Feature Engineering Execution ---\")\n",
    "\n",
    "# Ensure df_5min and df_15min are your REAL data from Cell 2\n",
    "if 'df_5min' not in locals() or not isinstance(df_5min, pd.DataFrame) or df_5min.empty:\n",
    "    raise ValueError(\"REAL df_5min (from Cell 2) is not loaded, not a DataFrame, or is empty before feature engineering!\")\n",
    "if 'df_15min' not in locals() or not isinstance(df_15min, pd.DataFrame) or df_15min.empty:\n",
    "    raise ValueError(\"REAL df_15min (from Cell 2) is not loaded, not a DataFrame, or is empty before feature engineering!\")\n",
    "\n",
    "# Ensure required functions are defined\n",
    "if 'add_all_features' not in locals() or 'add_time_session_features' not in locals():\n",
    "    raise NameError(\"Functions 'add_all_features' or 'add_time_session_features' are not defined. Ensure Cell 5 was run.\")\n",
    "\n",
    "print(f\"Input df_5min shape for features: {df_5min.shape}\")\n",
    "# Ensure df_5min has base OHLCV columns before passing to add_all_features\n",
    "expected_cols_5min = ['open', 'high', 'low', 'close', 'volume']\n",
    "if not all(col in df_5min.columns for col in expected_cols_5min):\n",
    "    raise ValueError(f\"df_5min is missing one of the required columns: {expected_cols_5min}. Found: {df_5min.columns.tolist()}\")\n",
    "df_5min_features = add_all_features(df_5min.copy(), suffix='_5m')\n",
    "print(f\"Output df_5min_features shape: {df_5min_features.shape}\")\n",
    "# print(f\"Columns in df_5min_features: {df_5min_features.columns.tolist()}\")\n",
    "\n",
    "\n",
    "print(f\"\\nInput df_15min shape for features: {df_15min.shape}\")\n",
    "# Ensure df_15min has base OHLCV columns\n",
    "expected_cols_15min = ['open', 'high', 'low', 'close', 'volume']\n",
    "if not all(col in df_15min.columns for col in expected_cols_15min):\n",
    "    raise ValueError(f\"df_15min is missing one of the required columns: {expected_cols_15min}. Found: {df_15min.columns.tolist()}\")\n",
    "df_15min_features = add_all_features(df_15min.copy(), suffix='_15m')\n",
    "print(f\"Output df_15min_features shape: {df_15min_features.shape}\")\n",
    "# print(f\"Columns in df_15min_features: {df_15min_features.columns.tolist()}\")\n",
    "\n",
    "\n",
    "print(\"\\nAdding time and session features to 5-minute data...\")\n",
    "# The add_time_session_features function will add columns like 'Hour_of_Day', etc.\n",
    "# These do not get a '_5m' suffix from add_all_features by design if they are added separately.\n",
    "# df_5min_features already contains features with '_5m' suffix and base 'open', 'high', 'low', 'close', 'volume'.\n",
    "df_5min_final_features = add_time_session_features(df_5min_features.copy())\n",
    "print(f\"Output df_5min_final_features shape: {df_5min_final_features.shape}\")\n",
    "# print(f\"Columns in df_5min_final_features: {df_5min_final_features.columns.tolist()}\")\n",
    "\n",
    "\n",
    "print(\"\\nMerging 5-minute and 15-minute features...\")\n",
    "# Select only suffixed feature columns from df_15min_features for merging\n",
    "# This avoids duplicating 'open', 'high', 'low', 'close', 'volume' if they were not suffixed in add_all_features\n",
    "# (Our current add_all_features *does* suffix all newly created features,\n",
    "# and keeps 'open', 'high', etc. as is on the df it processes.)\n",
    "# The columns from df_15min_features will be like 'ATR_14_15m', 'SMA_10_15m', etc.\n",
    "cols_to_merge_15m = [col for col in df_15min_features.columns if col.endswith('_15m')]\n",
    "\n",
    "if not cols_to_merge_15m:\n",
    "    print(\"Warning: No columns ending with '_15m' found in df_15min_features to merge.\")\n",
    "    # If this happens, df_merged will essentially be df_5min_final_features\n",
    "    # This might indicate an issue with the suffixing in add_all_features for the 15min run.\n",
    "\n",
    "# Ensure inputs to merge are valid and have sorted DatetimeIndex\n",
    "if not isinstance(df_5min_final_features.index, pd.DatetimeIndex) or \\\n",
    "   not isinstance(df_15min_features.index, pd.DatetimeIndex):\n",
    "    raise TypeError(\"Indexes of DataFrames to merge must be DatetimeIndex.\")\n",
    "\n",
    "if df_5min_final_features.empty:\n",
    "    raise ValueError(\"df_5min_final_features DataFrame for merging is empty!\")\n",
    "# Only try to merge if there are 15m columns to merge\n",
    "if cols_to_merge_15m and df_15min_features[cols_to_merge_15m].empty :\n",
    "     print(\"Warning: The selection of 15m feature columns resulted in an empty DataFrame. Merging will likely result in only 5m features.\")\n",
    "\n",
    "\n",
    "df_merged = pd.merge_asof(\n",
    "    left=df_5min_final_features.sort_index(),\n",
    "    right=df_15min_features[cols_to_merge_15m].sort_index() if cols_to_merge_15m else pd.DataFrame(index=df_5min_final_features.index), # Ensure right is DataFrame\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    direction='backward' # Use the 15-min data from the start of the 15-min bar\n",
    ")\n",
    "\n",
    "print(\"--- Feature Engineering Execution COMPLETE ---\")\n",
    "print(f\"Final df_merged shape: {df_merged.shape}\")\n",
    "# print(\"Final df_merged columns:\", df_merged.columns.tolist()) # Uncomment to verify all columns\n",
    "# print(\"Final df_merged head:\\n\", df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba # Make sure numba is imported in your first cell (Cell 2)\n",
    "\n",
    "# --- Target Labeling Function Definitions ---\n",
    "\n",
    "# --- Add Target Labels to REAL df_merged ---\n",
    "# This section will ONLY run if your REAL df_merged (from Cell 9) exists and is not empty.\n",
    "print(\"\\n--- Adding Target Labels to REAL df_merged ---\")\n",
    "if 'df_merged' in locals() and isinstance(df_merged, pd.DataFrame) and not df_merged.empty:\n",
    "    print(f\"Using REAL df_merged (shape: {df_merged.shape}) for target labeling.\")\n",
    "    for req_col_real in ['open', 'close', 'high', 'low', 'ATR_14_5m']: \n",
    "        if req_col_real not in df_merged.columns:\n",
    "            raise KeyError(f\"Essential column '{req_col_real}' for labeling is MISSING from REAL df_merged. Available: {df_merged.columns.tolist()}\")\n",
    "\n",
    "    # 1. Regression Target\n",
    "    lookahead_val = 6 \n",
    "    regression_col_name = f'reg_target_lookahead{lookahead_val}'\n",
    "    print(f\"\\nAdding Regression Target to REAL df_merged: {regression_col_name} ...\")\n",
    "    df_merged[regression_col_name] = compute_regression_labels(\n",
    "        df_merged.copy(), \n",
    "        price_col_entry='open', price_col_exit='close', lookahead=lookahead_val,\n",
    "        vol_col='ATR_14_5m', min_vol_threshold=0.1, cap_outliers=True,\n",
    "        lower_cap_percentile=1.0, upper_cap_percentile=99.0, same_day_trade=True\n",
    "    )\n",
    "    print(f\"Finished adding {regression_col_name}. Description:\")\n",
    "    print(df_merged[regression_col_name].describe(percentiles=[]))\n",
    "\n",
    "    # 2. Classification Target (using the descriptive name)\n",
    "    pt_val = 2.0\n",
    "    sl_val = 1.5\n",
    "    vb_val = 12 \n",
    "    classification_col_name = f'clf_target_numba_pt{pt_val}sl{sl_val}vb{vb_val}'\n",
    "    print(f\"\\nAdding Classification Target to REAL df_merged: {classification_col_name} ...\")\n",
    "    df_merged[classification_col_name] = compute_classification_labels_triple_barrier_numba(\n",
    "        df_prices=df_merged.copy(), \n",
    "        entry_price_col='close', high_col='high', low_col='low', atr_col='ATR_14_5m',\n",
    "        pt_atr_mult=pt_val, sl_atr_mult=sl_val, vertical_barrier_periods=vb_val,\n",
    "        min_target_return_pct=0.0005\n",
    "    )\n",
    "    print(f\"Finished adding {classification_col_name}. Value Counts:\")\n",
    "    print(df_merged[classification_col_name].value_counts(dropna=False))\n",
    "\n",
    "    print(\"\\nColumns in REAL df_merged AFTER adding all targets:\", df_merged.columns.tolist())\n",
    "    print(f\"REAL df_merged shape after adding targets: {df_merged.shape}\")\n",
    "else:\n",
    "    print(\"CRITICAL ERROR: REAL 'df_merged' was NOT labeled because it was not found or was empty when this cell was run. Check the output of Cell 9 (Feature Engineering Execution).\")\n",
    "# --- END OF Adding Target Labels to REAL df_merged ---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indicator list for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "    # I. Technical Indicators (5m & 15m)\n",
    "    'SMA_10_5m', 'SMA_20_5m', 'SMA_50_5m', 'EMA_10_5m', 'EMA_20_5m', 'EMA_50_5m',\n",
    "    'Price_vs_EMA20_5m', 'EMA10_vs_EMA20_5m', 'EMA_Slope_10_5m',\n",
    "    'RSI_14_5m', 'RSI_Is_Overbought_70_5m', 'RSI_Is_Oversold_30_5m',\n",
    "    'Stoch_K_14_3_5m', 'Stoch_D_3_5m', 'Stoch_Is_Overbought_80_5m', 'Stoch_Is_Oversold_20_5m',\n",
    "    'CCI_20_5m', 'MACD_12_26_9_5m', 'MACD_Signal_12_26_9_5m', 'MACD_Hist_12_26_9_5m',\n",
    "    'MACD_Cross_Signal_5m', 'PPO_12_26_9_5m', 'ROC_10_5m',\n",
    "    'ATR_14_5m', 'BB_Upper_20_2_5m', 'BB_Lower_20_2_5m', 'BB_Mid_20_5m',\n",
    "    'BB_Width_20_2_5m', 'Price_vs_BB_Upper_5m', 'Price_vs_BB_Lower_5m', 'Percent_B_20_2_5m',\n",
    "    'ADX_14_5m', 'Plus_DI_14_5m', 'Minus_DI_14_5m', 'PSAR_5m', 'PSAR_Flip_Signal_5m',\n",
    "    'VWAP_5m', 'Price_vs_VWAP_5m', 'Volume_SMA_20_5m',\n",
    "\n",
    "    'SMA_10_15m', 'SMA_20_15m', 'SMA_50_15m', 'EMA_10_15m', 'EMA_20_15m', 'EMA_50_15m',\n",
    "    'Price_vs_EMA20_15m', 'EMA10_vs_EMA20_15m', 'EMA_Slope_10_15m',\n",
    "    'RSI_14_15m', 'RSI_Is_Overbought_70_15m', 'RSI_Is_Oversold_30_15m',\n",
    "    'Stoch_K_14_3_15m', 'Stoch_D_3_15m', 'Stoch_Is_Overbought_80_15m', 'Stoch_Is_Oversold_20_15m',\n",
    "    'CCI_20_15m', 'MACD_12_26_9_15m', 'MACD_Signal_12_26_9_15m', 'MACD_Hist_12_26_9_15m',\n",
    "    'MACD_Cross_Signal_15m', 'PPO_12_26_9_15m', 'ROC_10_15m',\n",
    "    'ATR_14_15m', 'BB_Upper_20_2_15m', 'BB_Lower_20_2_15m', 'BB_Mid_20_15m',\n",
    "    'BB_Width_20_2_15m', 'Price_vs_BB_Upper_15m', 'Price_vs_BB_Lower_15m', 'Percent_B_20_2_15m',\n",
    "    'ADX_14_15m', 'Plus_DI_14_15m', 'Minus_DI_14_15m', 'PSAR_15m', 'PSAR_Flip_Signal_15m',\n",
    "    'VWAP_15m', 'Price_vs_VWAP_15m', 'Volume_SMA_20_15m',\n",
    "\n",
    "    # II. Price Action & Basic Features (Mostly 5m, some 15m)\n",
    "    'Candle_Range_5m', 'Candle_Body_5m', 'Upper_Wick_5m', 'Lower_Wick_5m', 'Body_vs_Range_5m',\n",
    "    'Log_Return_1_5m', 'Log_Return_3_5m', 'Log_Return_6_5m', 'Simple_Return_1_5m',\n",
    "    'Is_Doji_5m', 'Is_Hammer_5m', 'Is_Engulfing_Bullish_5m', 'Is_Engulfing_Bearish_5m',\n",
    "\n",
    "    'Candle_Range_15m', 'Candle_Body_15m', 'Upper_Wick_15m', 'Lower_Wick_15m', 'Body_vs_Range_15m',\n",
    "    'Log_Return_1_15m', # This is a 15-min return\n",
    "    'Simple_Return_1_15m',\n",
    "\n",
    "    # III. Statistical Features (5m & 15m)\n",
    "    'Rolling_Std_Dev_14_5m', 'Rolling_Skew_30_5m', 'Rolling_Kurtosis_30_5m',\n",
    "    'Close_Lag_1_5m', 'Close_Lag_3_5m', 'Close_Lag_6_5m', 'RSI_Lag_1_5m',\n",
    "\n",
    "    'Rolling_Std_Dev_14_15m', 'Rolling_Skew_30_15m', 'Rolling_Kurtosis_30_15m',\n",
    "    'Close_Lag_1_15m', 'Close_Lag_3_15m', 'RSI_Lag_1_15m',\n",
    "\n",
    "    # IV. Time & Session Features (No suffix needed - apply to 5m base)\n",
    "    'Hour_of_Day', 'Minute_of_Hour',\n",
    "    'Time_Sin', 'Time_Cos',\n",
    "    'Day_of_Week', 'Day_Sin', 'Day_Cos',\n",
    "    'Is_Asian_Session', 'Is_London_Session', 'Is_NY_Session', 'Is_Overlap',\n",
    "    'Is_US_Open_Hour', 'Is_US_Close_Hour'\n",
    "]\n",
    "\n",
    "print(f\"Total number of potential features listed: {len(feature_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_merged.dropna()\n",
    "print(f\"Shape before dropna: {df_merged.shape}, after dropna: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Separating Features (X) and Targets (y) ---\")\n",
    "\n",
    "# DEFINE THE ACTUAL TARGET COLUMN NAMES AS THEY ARE IN df_final\n",
    "lookahead_val = 6  # Ensure this matches the lookahead used\n",
    "pt_val = 2.0       # Ensure this matches pt_atr_mult\n",
    "sl_val = 1.5       # Ensure this matches sl_atr_mult\n",
    "vb_val = 12        # Ensure this matches vertical_barrier_periods\n",
    "\n",
    "actual_regression_target_col = f'reg_target_lookahead{lookahead_val}'\n",
    "actual_classification_target_col = f'clf_target_numba_pt{pt_val}sl{sl_val}vb{vb_val}'\n",
    "\n",
    "# Check if these columns actually exist in df_final\n",
    "if actual_regression_target_col not in df_final.columns:\n",
    "    raise KeyError(f\"Regression target column '{actual_regression_target_col}' NOT FOUND in df_final. Available: {df_final.columns.tolist()}\")\n",
    "if actual_classification_target_col not in df_final.columns:\n",
    "    raise KeyError(f\"Classification target column '{actual_classification_target_col}' NOT FOUND in df_final. Available: {df_final.columns.tolist()}\")\n",
    "    \n",
    "target_cols_for_exclusion = [actual_regression_target_col, actual_classification_target_col]\n",
    "\n",
    "# Define columns to exclude from features (targets + base OHLCV from original 5-min data)\n",
    "exclude_cols = target_cols_for_exclusion + ['open', 'high', 'low', 'close', 'volume']\n",
    "feature_columns = [col for col in df_final.columns if col not in exclude_cols]\n",
    "\n",
    "if not feature_columns:\n",
    "    raise ValueError(\"No feature columns found after exclusion. Check column names and logic.\")\n",
    "# --- Critical Check: Ensure target is not in features ---\n",
    "if actual_classification_target_col in feature_columns:\n",
    "    raise ValueError(f\"CRITICAL ERROR: Target column '{actual_classification_target_col}' found in feature_columns! Leakage will occur.\")\n",
    "if actual_regression_target_col in feature_columns:\n",
    "    raise ValueError(f\"CRITICAL ERROR: Target column '{actual_regression_target_col}' found in feature_columns! Leakage will occur.\")\n",
    "# --- End of Critical Check ---\n",
    "\n",
    "X = df_final[feature_columns]\n",
    "y_reg = df_final[actual_regression_target_col]\n",
    "y_clf = df_final[actual_classification_target_col]\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Number of features: {len(feature_columns)}\") # Should be less than before\n",
    "print(f\"Regression Target (y_reg) shape: {y_reg.shape}, Name: {y_reg.name}\")\n",
    "print(f\"Classification Target (y_clf) shape: {y_clf.shape}, Name: {y_clf.name}\")\n",
    "\n",
    "print(\"\\n--- Splitting Data (Train-Validation-Test) ---\")\n",
    "\n",
    "# 3. Split data into training, validation, and testing sets (Time-Based Split)\n",
    "#    We use a simple percentage split based on time. NO SHUFFLING!\n",
    "train_pct = 0.70\n",
    "val_pct = 0.15\n",
    "# test_pct = 0.15 (implicit)\n",
    "\n",
    "data_len = len(X)\n",
    "train_end_idx = int(data_len * train_pct)\n",
    "val_end_idx = train_end_idx + int(data_len * val_pct)\n",
    "\n",
    "X_train, X_val, X_test = X[:train_end_idx], X[train_end_idx:val_end_idx], X[val_end_idx:]\n",
    "y_reg_train, y_reg_val, y_reg_test = y_reg[:train_end_idx], y_reg[train_end_idx:val_end_idx], y_reg[val_end_idx:]\n",
    "y_clf_train, y_clf_val, y_clf_test = y_clf[:train_end_idx], y_clf[train_end_idx:val_end_idx], y_clf[val_end_idx:]\n",
    "\n",
    "print(f\"Training set shapes: X={X_train.shape}, y_clf={y_clf_train.shape}\")\n",
    "print(f\"Validation set shapes: X={X_val.shape}, y_clf={y_clf_val.shape}\")\n",
    "print(f\"Test set shapes: X={X_test.shape}, y_clf={y_clf_test.shape}\")\n",
    "\n",
    "# Verify the date ranges (optional but recommended)\n",
    "print(f\"\\nTraining data runs from {X_train.index.min()} to {X_train.index.max()}\")\n",
    "print(f\"Validation data runs from {X_val.index.min()} to {X_val.index.max()}\")\n",
    "print(f\"Test data runs from {X_test.index.min()} to {X_test.index.max()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' not in locals() or 'y_clf_train' not in locals():\n",
    "    print(\"Dummy data for X_train, y_clf_train etc. is being created as they were not found.\")\n",
    "    num_train_samples = 1000\n",
    "    num_val_samples = 200\n",
    "    num_features = X.shape[1] if 'X' in locals() else 140 # Get num_features from previous X\n",
    "    \n",
    "    X_train = pd.DataFrame(np.random.rand(num_train_samples, num_features), columns=[f'feature_{i}' for i in range(num_features)])\n",
    "    y_clf_train = pd.Series(np.random.randint(0, 3, num_train_samples))\n",
    "    \n",
    "    X_val = pd.DataFrame(np.random.rand(num_val_samples, num_features), columns=[f'feature_{i}' for i in range(num_features)])\n",
    "    y_clf_val = pd.Series(np.random.randint(0, 3, num_val_samples))\n",
    "    \n",
    "    X_test = pd.DataFrame(np.random.rand(num_val_samples, num_features), columns=[f'feature_{i}' for i in range(num_features)])\n",
    "    y_clf_test = pd.Series(np.random.randint(0, 3, num_val_samples))\n",
    "    print(\"Dummy data created. Shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_clf_train: {y_clf_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_clf_val: {y_clf_val.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Training LightGBM Classifier ---\")\n",
    "\n",
    "# 1. Instantiate the LGBMClassifier\n",
    "#    We'll start with some basic parameters. Hyperparameter tuning is crucial later.\n",
    "lgbm_clf = lgb.LGBMClassifier(\n",
    "    objective='multiclass', # For 0, 1, 2 labels\n",
    "    metric='multi_logloss', # Common metric for multiclass\n",
    "    n_estimators=1000,      # Number of boosting trees\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,           # No limit on depth\n",
    "    random_state=42,        # For reproducibility\n",
    "    n_jobs=-1,              # Use all available cores\n",
    "    # Early stopping can be added during fit for more robust training\n",
    "    # class_weight='balanced' # Consider if your classes are imbalanced\n",
    ")\n",
    "\n",
    "# Check class distribution in training data\n",
    "print(\"\\nTraining data class distribution:\")\n",
    "print(y_clf_train.value_counts(normalize=True))\n",
    "\n",
    "# Handle potential class imbalance if significant\n",
    "# If class_weight='balanced' is not enough, or you want to try SMOTE (later)\n",
    "# For now, let's proceed.\n",
    "print(\"--- Dtypes in X_train before fitting ---\")\n",
    "problem_cols = ['PPO_12_26_9_5m', 'ROC_10_5m', 'PPO_12_26_9_15m', 'ROC_10_15m']\n",
    "for col in problem_cols:\n",
    "    if col in X_train.columns:\n",
    "        print(f\"Dtype of {col}: {X_train[col].dtype}\")\n",
    "    else:\n",
    "        print(f\"Column {col} not found in X_train.\")\n",
    "\n",
    "# This is the existing line:\n",
    "lgbm_clf.fit(\n",
    "    X_train, y_clf_train,\n",
    "    # ... rest of the parameters\n",
    ")\n",
    "# 2. Train the model\n",
    "print(\"\\nStarting model training...\")\n",
    "lgbm_clf.fit(\n",
    "    X_train, y_clf_train,\n",
    "    eval_set=[(X_val, y_clf_val)], # Evaluate on validation set during training\n",
    "    eval_metric='multi_logloss',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=1)] # Stop if no improvement for 50 rounds\n",
    ")\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# 3. Make predictions on the validation set\n",
    "print(\"\\nMaking predictions on validation set...\")\n",
    "y_pred_val = lgbm_clf.predict(X_val)\n",
    "y_pred_proba_val = lgbm_clf.predict_proba(X_val) # Get probabilities\n",
    "\n",
    "# 4. Evaluate the model\n",
    "print(\"\\n--- Model Evaluation on Validation Set ---\")\n",
    "accuracy_val = accuracy_score(y_clf_val, y_pred_val)\n",
    "print(f\"Validation Accuracy: {accuracy_val:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "# target_names=['No Trade (0)', 'Short (1)', 'Long (2)'] # For better readability\n",
    "# Ensure your labels are 0, 1, 2 and map accordingly if different\n",
    "class_names = ['No Trade (0)', 'Short (1)', 'Long (2)'] \n",
    "# Check unique values in y_clf_val and y_pred_val to ensure they are all present for the report\n",
    "present_labels = sorted(list(set(y_clf_val) | set(y_pred_val)))\n",
    "current_class_names = [class_names[i] for i in present_labels if i < len(class_names)]\n",
    "\n",
    "if not current_class_names: # Fallback if labels are unexpected\n",
    "    current_class_names = [f\"Class {i}\" for i in present_labels]\n",
    "    \n",
    "print(classification_report(y_clf_val, y_pred_val, target_names=current_class_names, labels=present_labels, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "cm = confusion_matrix(y_clf_val, y_pred_val, labels=present_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=current_class_names, yticklabels=current_class_names)\n",
    "plt.title('Confusion Matrix (Validation Set)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# 5. Feature Importance (Optional but very useful)\n",
    "print(\"\\n--- Feature Importances ---\")\n",
    "feature_importances = pd.Series(lgbm_clf.feature_importances_, index=X_train.columns)\n",
    "top_n = 20\n",
    "print(f\"Top {top_n} features:\")\n",
    "print(feature_importances.sort_values(ascending=False).head(top_n))\n",
    "\n",
    "plt.figure(figsize=(10, top_n // 2 if top_n > 10 else 5))\n",
    "feature_importances.sort_values(ascending=False).head(top_n).plot(kind='barh')\n",
    "plt.title(f'Top {top_n} Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' not in locals() or 'y_clf_train' not in locals() or \\\n",
    "   'X_val' not in locals() or 'y_clf_val' not in locals() or \\\n",
    "   X_train.empty or y_clf_train.empty or X_val.empty or y_clf_val.empty:\n",
    "    \n",
    "    print(\"WARNING: Real training/validation data not found or empty. Creating DUMMY data for Optuna demonstration.\")\n",
    "    num_train_samples = 1000\n",
    "    num_val_samples = 200\n",
    "    # Try to get num_features from a global X if it exists and has been shaped, otherwise default\n",
    "    num_features = X.shape[1] if 'X' in locals() and hasattr(X, 'shape') else 140 \n",
    "    \n",
    "    X_train = pd.DataFrame(np.random.rand(num_train_samples, num_features), columns=[f'feature_{i}' for i in range(num_features)])\n",
    "    y_clf_train = pd.Series(np.random.randint(0, 3, num_train_samples))\n",
    "    \n",
    "    X_val = pd.DataFrame(np.random.rand(num_val_samples, num_features), columns=[f'feature_{i}' for i in range(num_features)])\n",
    "    y_clf_val = pd.Series(np.random.randint(0, 3, num_val_samples))\n",
    "    print(\"Dummy data created for Optuna. Shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_clf_train: {y_clf_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_clf_val: {y_clf_val.shape}\")\n",
    "else:\n",
    "    print(\"Using existing X_train, y_clf_train, X_val, y_clf_val for Optuna.\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_clf_train shape: {y_clf_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}, y_clf_val shape: {y_clf_val.shape}\")\n",
    "\n",
    "\n",
    "# 1. Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'metric': 'multi_logloss', # Internal metric for LightGBM training\n",
    "        'verbosity': -1,          # Suppress LightGBM's own training output during Optuna search\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1, # Use all cores\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 2000, step=100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0, step=0.05), # For bagging\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0, step=0.05), # Feature fraction\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True), # L1 regularization\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True), # L2 regularization\n",
    "        # 'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']) # Optional\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_clf_train,\n",
    "        eval_set=[(X_val, y_clf_val)],\n",
    "        eval_metric='multi_logloss', # Or your preferred LightGBM eval metric\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)] # Use non-verbose early stopping\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    # We want to maximize F1-score, so Optuna will try to maximize this value\n",
    "    # Using 'macro' average for F1-score as it considers all classes equally\n",
    "    f1 = f1_score(y_clf_val, preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return f1 # Optuna will try to maximize this\n",
    "\n",
    "# 2. Create an Optuna study\n",
    "#    We specify direction=\"maximize\" because we want to maximize F1-score.\n",
    "#    If you were minimizing logloss, it would be \"minimize\".\n",
    "study_name = 'lgbm_clf_optimization_v1' # You can name your study\n",
    "storage_name = f\"sqlite:///{study_name}.db\" # Optional: Save study to a database to resume later\n",
    "\n",
    "# Check if a study with this name already exists (if using SQLite storage)\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    print(f\"Resuming existing study: {study_name}\")\n",
    "except KeyError: # Study not found\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=storage_name)\n",
    "    print(f\"Starting new study: {study_name}\")\n",
    "\n",
    "\n",
    "# 3. Run the optimization\n",
    "#    n_trials is the number of different hyperparameter combinations Optuna will try.\n",
    "#    Increase this for a more thorough search, but it will take longer.\n",
    "print(f\"\\nStarting Optuna optimization with, for example, 50 trials...\")\n",
    "study.optimize(objective, n_trials=50, timeout=None) # Set a timeout in seconds if needed, e.g., timeout=3600 for 1 hour\n",
    "\n",
    "# 4. Print the best trial results\n",
    "print(\"\\n--- Optuna Optimization Finished ---\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial F1-score (macro): {best_trial.value:.4f}\")\n",
    "print(\"Best hyperparameters found:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# You can now train a final model using these best hyperparameters on the full training set (X_train + X_val)\n",
    "# or just use the model from the best trial if LightGBM stored it internally (check Optuna/LightGBM docs for that).\n",
    "# For simplicity, let's instantiate and show how to train a model with best params:\n",
    "\n",
    "print(\"\\nTraining a new model with the best hyperparameters found by Optuna...\")\n",
    "best_params = best_trial.params\n",
    "# Add fixed params back if they were not part of the search space but are needed by LGBMClassifier\n",
    "best_params['objective'] = 'multiclass'\n",
    "best_params['metric'] = 'multi_logloss' \n",
    "best_params['verbosity'] = -1\n",
    "best_params['random_state'] = 42\n",
    "best_params['n_jobs'] = -1\n",
    "# If you had class_weight in objective and it's not 'balanced', you might need to handle it or remove it\n",
    "# if 'class_weight' in best_params and best_params['class_weight'] is None:\n",
    "#     del best_params['class_weight']\n",
    "\n",
    "\n",
    "final_lgbm_clf = lgb.LGBMClassifier(**best_params)\n",
    "\n",
    "# Optional: Combine train and validation for final model training, then evaluate on test\n",
    "# X_train_full = pd.concat([X_train, X_val])\n",
    "# y_clf_train_full = pd.concat([y_clf_train, y_clf_val])\n",
    "# final_lgbm_clf.fit(X_train_full, y_clf_train_full, callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
    "# print(\"Final model trained on combined train+validation data.\")\n",
    "\n",
    "# For now, just retrain on X_train and evaluate on X_val to see effect of best params\n",
    "final_lgbm_clf.fit(X_train, y_clf_train, eval_set=[(X_val, y_clf_val)], \n",
    "                   callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
    "y_pred_val_best = final_lgbm_clf.predict(X_val)\n",
    "\n",
    "print(\"\\n--- Evaluation of Model with Best Hyperparameters (on Validation Set) ---\")\n",
    "accuracy_val_best = accuracy_score(y_clf_val, y_pred_val_best)\n",
    "print(f\"Validation Accuracy with Best Params: {accuracy_val_best:.4f}\")\n",
    "print(\"\\nClassification Report with Best Params (Validation):\")\n",
    "class_names = ['No Trade (0)', 'Short (1)', 'Long (2)'] \n",
    "present_labels_best = sorted(list(set(y_clf_val) | set(y_pred_val_best)))\n",
    "current_class_names_best = [class_names[i] for i in present_labels_best if i < len(class_names)]\n",
    "if not current_class_names_best: \n",
    "    current_class_names_best = [f\"Class {i}\" for i in present_labels_best]\n",
    "print(classification_report(y_clf_val, y_pred_val_best, target_names=current_class_names_best, labels=present_labels_best, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' not in locals() or 'y_reg_train' not in locals() or \\\n",
    "   'X_val' not in locals() or 'y_reg_val' not in locals() or \\\n",
    "   X_train.empty or y_reg_train.empty or X_val.empty or y_reg_val.empty:\n",
    "    \n",
    "    print(\"WARNING: Real training/validation data not found or empty. Creating DUMMY data for Optuna Regressor demonstration.\")\n",
    "    num_train_samples = 1000\n",
    "    num_val_samples = 200\n",
    "    num_features = X.shape[1] if 'X' in locals() and hasattr(X, 'shape') else 140\n",
    "    \n",
    "    X_train = pd.DataFrame(np.random.rand(num_train_samples, num_features), columns=[f'feature_{i}' for i in range(num_features)])\n",
    "    y_reg_train = pd.Series(np.random.rand(num_train_samples) * 0.001 - 0.0005) # Small random returns\n",
    "    \n",
    "    X_val = pd.DataFrame(np.random.rand(num_val_samples, num_features), columns=[f'feature_{i}' for i in range(num_features)])\n",
    "    y_reg_val = pd.Series(np.random.rand(num_val_samples) * 0.001 - 0.0005)\n",
    "    print(\"Dummy data created for Optuna Regressor. Shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_reg_train: {y_reg_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_reg_val: {y_reg_val.shape}\")\n",
    "else:\n",
    "    print(\"Using existing X_train, y_reg_train, X_val, y_reg_val for Optuna Regressor.\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_reg_train shape: {y_reg_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}, y_reg_val shape: {y_reg_val.shape}\")\n",
    "\n",
    "# 1. Define the objective function for Optuna Regressor\n",
    "def objective_regressor(trial):\n",
    "    params = {\n",
    "        'objective': 'regression_l1', # MAE, often more robust to outliers than L2 (MSE)\n",
    "        'metric': 'rmse',             # LightGBM will use RMSE for its internal evaluation and early stopping\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 2000, step=100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0, step=0.1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 5.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 5.0, log=True),\n",
    "    }\n",
    "\n",
    "    model_reg = lgb.LGBMRegressor(**params)\n",
    "    \n",
    "    model_reg.fit(\n",
    "        X_train, y_reg_train,\n",
    "        eval_set=[(X_val, y_reg_val)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    preds_val = model_reg.predict(X_val)\n",
    "    mse = mean_squared_error(y_reg_val, preds_val) # Calculate Mean Squared Error\n",
    "    rmse = np.sqrt(mse) # Calculate RMSE by taking the square root\n",
    "    \n",
    "    return rmse # Optuna will try to MINIMIZE this\n",
    "\n",
    "# 2. Create an Optuna study for the regressor\n",
    "study_name_reg = 'lgbm_regressor_optimization_v1'\n",
    "storage_name_reg = f\"sqlite:///{study_name_reg}.db\" \n",
    "\n",
    "try:\n",
    "    study_reg = optuna.load_study(study_name=study_name_reg, storage=storage_name_reg)\n",
    "    print(f\"Resuming existing regressor study: {study_name_reg}\")\n",
    "except KeyError: \n",
    "    study_reg = optuna.create_study(direction=\"minimize\", study_name=study_name_reg, storage=storage_name_reg) # We want to MINIMIZE RMSE\n",
    "    print(f\"Starting new regressor study: {study_name_reg}\")\n",
    "\n",
    "# 3. Run the optimization\n",
    "print(f\"\\nStarting Optuna optimization for LGBMRegressor (e.g., 50 trials)...\")\n",
    "study_reg.optimize(objective_regressor, n_trials=50, timeout=None) \n",
    "\n",
    "# 4. Print the best regressor trial results\n",
    "print(\"\\n--- Optuna Regressor Optimization Finished ---\")\n",
    "print(f\"Number of finished trials: {len(study_reg.trials)}\")\n",
    "\n",
    "best_trial_reg = study_reg.best_trial\n",
    "print(f\"Best trial RMSE: {best_trial_reg.value:.6f}\") # RMSE is typically small for normalized returns\n",
    "print(\"Best hyperparameters found for LGBMRegressor:\")\n",
    "for key, value in best_trial_reg.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Store the best parameters for later use\n",
    "best_regressor_params = best_trial_reg.params\n",
    "# Add fixed params back\n",
    "best_regressor_params['objective'] = 'regression_l1'\n",
    "best_regressor_params['metric'] = 'rmse'\n",
    "best_regressor_params['verbosity'] = -1\n",
    "best_regressor_params['random_state'] = 42\n",
    "best_regressor_params['n_jobs'] = -1\n",
    "\n",
    "print(\"\\nBest regressor parameters stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'best_regressor_params' not in locals():\n",
    "    print(\"WARNING: 'best_regressor_params' not found. Using placeholder default parameters for LGBMRegressor.\")\n",
    "    best_regressor_params = {\n",
    "        'objective': 'regression_l1', 'metric': 'rmse', 'verbosity': -1, \n",
    "        'random_state': 42, 'n_jobs': -1, 'n_estimators': 500, \n",
    "        'learning_rate': 0.05, 'num_leaves': 31, 'boosting_type':'gbdt',\n",
    "        'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0,\n",
    "        'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0\n",
    "    }\n",
    "\n",
    "# Fallback for data if not defined\n",
    "if 'X_train' not in locals() or X_train.empty:\n",
    "    print(\"WARNING: Real X_train not found or empty. Creating DUMMY data for demonstration.\")\n",
    "    num_features = 140 \n",
    "    X_train = pd.DataFrame(np.random.rand(1000, num_features), columns=[f'feature_{i}' for i in range(num_features)], index=pd.date_range(start='2023-01-01', periods=1000, freq='5min'))\n",
    "    y_reg_train = pd.Series(np.random.rand(1000) * 0.001 - 0.0005, index=X_train.index)\n",
    "    X_val = pd.DataFrame(np.random.rand(200, num_features), columns=[f'feature_{i}' for i in range(num_features)], index=pd.date_range(start=X_train.index[-1] + pd.Timedelta(minutes=5), periods=200, freq='5min'))\n",
    "    X_test = pd.DataFrame(np.random.rand(200, num_features), columns=[f'feature_{i}' for i in range(num_features)], index=pd.date_range(start=X_val.index[-1] + pd.Timedelta(minutes=5), periods=200, freq='5min'))\n",
    "    print(\"Dummy X_train, y_reg_train, X_val, X_test created.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Generating Out-of-Sample Regressor Predictions ---\")\n",
    "\n",
    "# 1. Get out-of-sample predictions for the training set (X_train) using a manual loop\n",
    "n_splits_cv = 5 \n",
    "tscv = TimeSeriesSplit(n_splits=n_splits_cv)\n",
    "\n",
    "# Initialize a Series to store OOS predictions for X_train, filled with NaNs\n",
    "y_pred_reg_train_oos = pd.Series(np.nan, index=X_train.index, name='predicted_regression_target')\n",
    "\n",
    "print(f\"Generating OOS predictions for X_train using manual TimeSeriesSplit (n_splits={n_splits_cv})...\")\n",
    "for fold_num, (train_index, test_index) in enumerate(tscv.split(X_train, y_reg_train)):\n",
    "    print(f\"  Processing Fold {fold_num + 1}/{n_splits_cv}...\")\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_reg_train.iloc[train_index], y_reg_train.iloc[test_index]\n",
    "    \n",
    "    regressor_fold = lgb.LGBMRegressor(**best_regressor_params)\n",
    "    regressor_fold.fit(X_train_fold, y_train_fold,\n",
    "                       eval_set=[(X_test_fold, y_test_fold)], # Optional: use test_fold as eval_set for early stopping\n",
    "                       eval_metric='rmse', # Make sure this matches objective_regressor\n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)] # Reduced stopping_rounds for CV\n",
    "                      )\n",
    "    \n",
    "    fold_predictions = regressor_fold.predict(X_test_fold)\n",
    "    y_pred_reg_train_oos.iloc[test_index] = fold_predictions\n",
    "\n",
    "# Any initial segments not covered by a test set in TimeSeriesSplit will remain NaN.\n",
    "# This is expected and correct for OOS predictions.\n",
    "print(\"Finished OOS predictions for X_train.\")\n",
    "print(f\"Number of NaNs in y_pred_reg_train_oos (expected for initial folds): {y_pred_reg_train_oos.isnull().sum()}\")\n",
    "\n",
    "\n",
    "# 2. Train a final regressor on the full X_train to predict on X_val and X_test\n",
    "final_regressor = lgb.LGBMRegressor(**best_regressor_params)\n",
    "print(\"\\nTraining final regressor on full X_train...\")\n",
    "# For the final model to predict on X_val/X_test, we can use X_val for early stopping if y_reg_val is available\n",
    "if 'y_reg_val' in locals() and not y_reg_val.empty:\n",
    "    print(\"Using X_val, y_reg_val for early stopping of final_regressor.\")\n",
    "    final_regressor.fit(X_train, y_reg_train,\n",
    "                        eval_set=[(X_val, y_reg_val)],\n",
    "                        eval_metric='rmse',\n",
    "                        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
    "else:\n",
    "    print(\"y_reg_val not available for early stopping, fitting final_regressor on X_train only.\")\n",
    "    final_regressor.fit(X_train, y_reg_train) \n",
    "print(\"Finished training final regressor.\")\n",
    "\n",
    "\n",
    "print(\"\\nGenerating predictions for X_val and X_test...\")\n",
    "y_pred_reg_val_values = final_regressor.predict(X_val)\n",
    "y_pred_reg_val = pd.Series(y_pred_reg_val_values, index=X_val.index, name='predicted_regression_target')\n",
    "\n",
    "# Ensure X_test is available for prediction\n",
    "if 'X_test' in locals() and not X_test.empty:\n",
    "    y_pred_reg_test_values = final_regressor.predict(X_test)\n",
    "    y_pred_reg_test = pd.Series(y_pred_reg_test_values, index=X_test.index, name='predicted_regression_target')\n",
    "    print(\"Finished predictions for X_val and X_test.\")\n",
    "else:\n",
    "    print(\"X_test not found or empty. Skipping predictions for X_test.\")\n",
    "    y_pred_reg_test = pd.Series(dtype='float64', name='predicted_regression_target') # Empty series if no X_test\n",
    "\n",
    "\n",
    "# --- Display some info ---\n",
    "print(\"\\nSample of OOS Regressor Predictions for X_train:\")\n",
    "print(y_pred_reg_train_oos.head())\n",
    "print(f\"Shape: {y_pred_reg_train_oos.shape}\")\n",
    "\n",
    "print(\"\\nSample of Regressor Predictions for X_val:\")\n",
    "print(y_pred_reg_val.head())\n",
    "print(f\"Shape: {y_pred_reg_val.shape}\")\n",
    "if not y_pred_reg_test.empty:\n",
    "    print(\"\\nSample of Regressor Predictions for X_test:\")\n",
    "    print(y_pred_reg_test.head())\n",
    "    print(f\"Shape: {y_pred_reg_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' not in locals() or X_train.empty:\n",
    "    print(\"WARNING: Real X_train not found or empty. Creating DUMMY X_train for demonstration.\")\n",
    "    # Infer num_features from y_pred_reg_train_oos if possible, else default\n",
    "    num_features_orig = X_train.shape[1] if ('X_train' in locals() and hasattr(X_train, 'shape')) else 140\n",
    "    \n",
    "    X_train = pd.DataFrame(np.random.rand(len(y_pred_reg_train_oos) if 'y_pred_reg_train_oos' in locals() and not y_pred_reg_train_oos.empty else 200, num_features_orig), \n",
    "                           columns=[f'feature_{i}' for i in range(num_features_orig)], \n",
    "                           index=y_pred_reg_train_oos.index if 'y_pred_reg_train_oos' in locals() and not y_pred_reg_train_oos.empty else pd.date_range('2023-01-01', periods=200, freq='5min'))\n",
    "    X_val = pd.DataFrame(np.random.rand(len(y_pred_reg_val) if 'y_pred_reg_val' in locals() and not y_pred_reg_val.empty else 50, num_features_orig), \n",
    "                         columns=[f'feature_{i}' for i in range(num_features_orig)], \n",
    "                         index=y_pred_reg_val.index if 'y_pred_reg_val' in locals() and not y_pred_reg_val.empty else pd.date_range('2023-02-01', periods=50, freq='5min'))\n",
    "    X_test = pd.DataFrame(np.random.rand(len(y_pred_reg_test) if 'y_pred_reg_test' in locals() and not y_pred_reg_test.empty else 50, num_features_orig), \n",
    "                          columns=[f'feature_{i}' for i in range(num_features_orig)], \n",
    "                          index=y_pred_reg_test.index if 'y_pred_reg_test' in locals() and not y_pred_reg_test.empty else pd.date_range('2023-03-01', periods=50, freq='5min'))\n",
    "    print(\"Dummy X_train, X_val, X_test created.\")\n",
    "\n",
    "# Check if prediction Series exist\n",
    "if 'y_pred_reg_train_oos' not in locals() or \\\n",
    "   'y_pred_reg_val' not in locals() or \\\n",
    "   'y_pred_reg_test' not in locals():\n",
    "    raise NameError(\"Regressor predictions (e.g., y_pred_reg_train_oos, y_pred_reg_val, y_pred_reg_test) not found. \"\n",
    "                    \"Ensure the previous cell (generating OOS predictions) was run successfully.\")\n",
    "\n",
    "print(\"\\n--- Adding Regressor Predictions as Features ---\")\n",
    "\n",
    "# It's good practice to work on copies if you might re-run cells\n",
    "X_train_augmented = X_train.copy()\n",
    "X_val_augmented = X_val.copy()\n",
    "X_test_augmented = X_test.copy()\n",
    "\n",
    "new_feature_name = 'predicted_regression_target' # This is the name used in y_pred_reg_* Series\n",
    "\n",
    "# Add the predictions as a new column\n",
    "# The indices should align as the predictions were created using the X sets' indices\n",
    "X_train_augmented[new_feature_name] = y_pred_reg_train_oos\n",
    "X_val_augmented[new_feature_name] = y_pred_reg_val\n",
    "X_test_augmented[new_feature_name] = y_pred_reg_test\n",
    "\n",
    "print(\"\\nShapes after adding regressor prediction feature:\")\n",
    "print(f\"X_train_augmented shape: {X_train_augmented.shape} (Original X_train shape: {X_train.shape})\")\n",
    "print(f\"X_val_augmented shape: {X_val_augmented.shape}   (Original X_val shape: {X_val.shape})\")\n",
    "print(f\"X_test_augmented shape: {X_test_augmented.shape}  (Original X_test shape: {X_test.shape})\")\n",
    "\n",
    "print(f\"\\nHead of X_train_augmented with new '{new_feature_name}' feature:\")\n",
    "# Display the new feature and a couple of existing features for verification\n",
    "cols_to_display = [new_feature_name]\n",
    "if len(X_train.columns) > 0: cols_to_display.append(X_train.columns[0])\n",
    "if len(X_train.columns) > 1: cols_to_display.append(X_train.columns[1])\n",
    "# Ensure all columns in cols_to_display actually exist in X_train_augmented\n",
    "cols_to_display = [col for col in cols_to_display if col in X_train_augmented.columns]\n",
    "if cols_to_display:\n",
    "    print(X_train_augmented[cols_to_display].head())\n",
    "else:\n",
    "    print(\"Could not display head, original columns not found (likely due to dummy data path).\")\n",
    "\n",
    "\n",
    "print(f\"\\nChecking for NaNs in '{new_feature_name}' feature after adding:\")\n",
    "print(f\"NaNs in X_train_augmented['{new_feature_name}']: {X_train_augmented[new_feature_name].isnull().sum()}\")\n",
    "print(f\"NaNs in X_val_augmented['{new_feature_name}']: {X_val_augmented[new_feature_name].isnull().sum()}\") # Should be 0\n",
    "print(f\"NaNs in X_test_augmented['{new_feature_name}']: {X_test_augmented[new_feature_name].isnull().sum()}\")   # Should be 0\n",
    "\n",
    "# IMPORTANT: Handle NaNs in X_train_augmented before training the classifier\n",
    "# The NaNs in 'predicted_regression_target' for X_train_augmented come from the initial\n",
    "# folds of TimeSeriesSplit where no OOS prediction was made.\n",
    "# You must align y_clf_train if you drop these rows.\n",
    "\n",
    "# Example of handling NaNs:\n",
    "# print(\"\\n--- Handling NaNs in Augmented Training Data ---\")\n",
    "# print(f\"Shape of X_train_augmented before NaN drop: {X_train_augmented.shape}\")\n",
    "# print(f\"Shape of y_clf_train before NaN drop: {y_clf_train.shape}\")\n",
    "\n",
    "# # Identify rows in X_train_augmented where the new feature is NaN\n",
    "# nan_mask_train = X_train_augmented[new_feature_name].isnull()\n",
    "\n",
    "# # Drop these rows from both X_train_augmented and y_clf_train\n",
    "# X_train_augmented_final = X_train_augmented[~nan_mask_train]\n",
    "# y_clf_train_final = y_clf_train[~nan_mask_train] # Ensure y_clf_train is available and aligned\n",
    "\n",
    "# print(f\"Shape of X_train_augmented_final after NaN drop: {X_train_augmented_final.shape}\")\n",
    "# print(f\"Shape of y_clf_train_final after NaN drop: {y_clf_train_final.shape}\")\n",
    "# print(f\"Number of NaNs dropped from training data: {nan_mask_train.sum()}\")\n",
    "\n",
    "# Now, X_train_augmented_final and y_clf_train_final would be used for re-tuning/re-training the classifier.\n",
    "# X_val_augmented and X_test_augmented should not have NaNs in this new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train_augmented' not in locals() or 'y_clf_train' not in locals() or \\\n",
    "   'X_val_augmented' not in locals() or 'y_clf_val' not in locals() or \\\n",
    "   X_train_augmented.empty or y_clf_train.empty or X_val_augmented.empty or y_clf_val.empty:\n",
    "    \n",
    "    print(\"WARNING: Real augmented training/validation data not found or empty. Creating DUMMY data for Optuna demonstration.\")\n",
    "    num_train_samples_aug = 1000\n",
    "    num_val_samples_aug = 200\n",
    "    num_features_aug = X_train_augmented.shape[1] if ('X_train_augmented' in locals() and hasattr(X_train_augmented, 'shape')) else 148\n",
    "    \n",
    "    # Create dummy X_train_augmented with the new feature\n",
    "    X_train_augmented_cols = [f'feature_{i}' for i in range(num_features_aug -1)] + ['predicted_regression_target']\n",
    "    X_train_augmented = pd.DataFrame(np.random.rand(num_train_samples_aug, num_features_aug), columns=X_train_augmented_cols)\n",
    "    # Introduce some NaNs into the dummy 'predicted_regression_target'\n",
    "    nan_indices = np.random.choice(X_train_augmented.index, size=int(num_train_samples_aug * 0.15), replace=False)\n",
    "    X_train_augmented.loc[nan_indices, 'predicted_regression_target'] = np.nan\n",
    "    \n",
    "    y_clf_train = pd.Series(np.random.randint(0, 3, num_train_samples_aug)) # Ensure y_clf_train aligns with X_train_augmented initially\n",
    "    \n",
    "    X_val_augmented_cols = [f'feature_{i}' for i in range(num_features_aug -1)] + ['predicted_regression_target']\n",
    "    X_val_augmented = pd.DataFrame(np.random.rand(num_val_samples_aug, num_features_aug), columns=X_val_augmented_cols)\n",
    "    y_clf_val = pd.Series(np.random.randint(0, 3, num_val_samples_aug))\n",
    "    \n",
    "    print(\"Dummy augmented data created. Shapes before NaN handling:\")\n",
    "    print(f\"X_train_augmented: {X_train_augmented.shape}, y_clf_train: {y_clf_train.shape}\")\n",
    "    print(f\"X_val_augmented: {X_val_augmented.shape}, y_clf_val: {y_clf_val.shape}\")\n",
    "\n",
    "\n",
    "# 1. Handle NaNs in X_train_augmented (from the 'predicted_regression_target' feature)\n",
    "#    and align y_clf_train.\n",
    "print(\"\\n--- Handling NaNs in Augmented Training Data ---\")\n",
    "new_feature_name = 'predicted_regression_target'\n",
    "\n",
    "if new_feature_name not in X_train_augmented.columns:\n",
    "    raise KeyError(f\"Column '{new_feature_name}' not found in X_train_augmented. Ensure previous cell ran correctly.\")\n",
    "\n",
    "print(f\"Shape of X_train_augmented before NaN drop: {X_train_augmented.shape}\")\n",
    "print(f\"Shape of y_clf_train before NaN drop: {y_clf_train.shape}\")\n",
    "print(f\"NaNs in X_train_augmented['{new_feature_name}'] before drop: {X_train_augmented[new_feature_name].isnull().sum()}\")\n",
    "\n",
    "# Create a mask for rows where the new feature is NaN\n",
    "nan_mask_train = X_train_augmented[new_feature_name].isnull()\n",
    "\n",
    "# Drop these rows from both X_train_augmented and y_clf_train\n",
    "# It's crucial that y_clf_train has the same index as X_train_augmented before this step.\n",
    "# If y_clf_train is a numpy array or has a different index, this alignment will fail.\n",
    "# Assuming y_clf_train's index matches X_train's (and thus X_train_augmented's) initial index.\n",
    "if not X_train_augmented.index.equals(y_clf_train.index):\n",
    "    print(\"WARNING: Index of X_train_augmented and y_clf_train do not match! Realigning y_clf_train. This might happen if y_clf_train was not from df_final.\")\n",
    "    # This is a fallback. Ideally, y_clf_train comes from df_final and aligns with X_train.\n",
    "    # If X_train was created from df_final, and y_clf_train from df_final, their original indices match.\n",
    "    # The issue might be if y_clf_train was converted to numpy array and lost its index.\n",
    "    # Let's assume for now they were created correctly and align.\n",
    "    # If y_clf_train is a numpy array, you'd need to do:\n",
    "    # y_clf_train_np = y_clf_train[~nan_mask_train.values] # if y_clf_train is numpy\n",
    "    # X_train_augmented_final = X_train_augmented[~nan_mask_train]\n",
    "    pass # Assuming pandas Series with matching index for now.\n",
    "\n",
    "X_train_augmented_final = X_train_augmented[~nan_mask_train]\n",
    "y_clf_train_final = y_clf_train[~nan_mask_train]\n",
    "\n",
    "print(f\"\\nShape of X_train_augmented_final after NaN drop: {X_train_augmented_final.shape}\")\n",
    "print(f\"Shape of y_clf_train_final after NaN drop: {y_clf_train_final.shape}\")\n",
    "num_nans_dropped = nan_mask_train.sum()\n",
    "print(f\"Number of rows dropped from training data due to NaNs in '{new_feature_name}': {num_nans_dropped}\")\n",
    "\n",
    "# X_val_augmented and X_test_augmented should not have NaNs in this new feature,\n",
    "# but it's good practice to check and ensure alignment if needed.\n",
    "if X_val_augmented[new_feature_name].isnull().any():\n",
    "    print(f\"WARNING: NaNs found in '{new_feature_name}' of X_val_augmented. This is unexpected.\")\n",
    "    # Handle them if necessary, e.g., X_val_augmented = X_val_augmented.dropna(subset=[new_feature_name])\n",
    "    # and align y_clf_val accordingly. For now, assuming it's clean.\n",
    "\n",
    "# 2. Define the objective function for Optuna (for the classifier with augmented features)\n",
    "def objective_classifier_augmented(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'metric': 'multi_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators': trial.suggest_int('n_estimators_aug', 200, 2500, step=100), # Wider range\n",
    "        'learning_rate': trial.suggest_float('learning_rate_aug', 0.005, 0.1, log=True), # Smaller LR\n",
    "        'num_leaves': trial.suggest_int('num_leaves_aug', 20, 200), # Wider range\n",
    "        'max_depth': trial.suggest_int('max_depth_aug', 4, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples_aug', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample_aug', 0.5, 1.0, step=0.05),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree_aug', 0.5, 1.0, step=0.05),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha_aug', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda_aug', 1e-3, 10.0, log=True),\n",
    "        'class_weight': None # Include class_weight\n",
    "    }\n",
    "\n",
    "    model_clf_aug = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    model_clf_aug.fit(\n",
    "        X_train_augmented_final, y_clf_train_final, # Use NaN-handled training data\n",
    "        eval_set=[(X_val_augmented, y_clf_val)],    # Use augmented validation data\n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    preds_val_aug = model_clf_aug.predict(X_val_augmented)\n",
    "    f1_macro_aug = f1_score(y_clf_val, preds_val_aug, average='macro', zero_division=0)\n",
    "    \n",
    "    return f1_macro_aug\n",
    "\n",
    "# 3. Create and run the Optuna study for the augmented classifier\n",
    "study_name_clf_aug = 'lgbm_clf_augmented_optimization_cw_none_v1'\n",
    "storage_name_clf_aug = f\"sqlite:///{study_name_clf_aug}.db\" \n",
    "\n",
    "try:\n",
    "    study_clf_aug = optuna.load_study(study_name=study_name_clf_aug, storage=storage_name_clf_aug)\n",
    "    print(f\"\\nResuming existing augmented classifier study: {study_name_clf_aug}\")\n",
    "except KeyError: \n",
    "    study_clf_aug = optuna.create_study(direction=\"maximize\", study_name=study_name_clf_aug, storage=storage_name_clf_aug)\n",
    "    print(f\"\\nStarting new augmented classifier study: {study_name_clf_aug}\")\n",
    "\n",
    "print(f\"\\nStarting Optuna optimization for Augmented LGBMClassifier (e.g., 50 trials)...\")\n",
    "# Adjust n_trials as needed\n",
    "study_clf_aug.optimize(objective_classifier_augmented, n_trials=50, timeout=None) \n",
    "\n",
    "# 4. Print the best trial results for the augmented classifier\n",
    "print(\"\\n--- Optuna Augmented Classifier Optimization Finished ---\")\n",
    "print(f\"Number of finished trials: {len(study_clf_aug.trials)}\")\n",
    "\n",
    "best_trial_clf_aug = study_clf_aug.best_trial\n",
    "print(f\"Best trial F1-score (macro) for augmented classifier: {best_trial_clf_aug.value:.4f}\")\n",
    "print(\"Best hyperparameters found for Augmented LGBMClassifier:\")\n",
    "best_classifier_params_augmented = {} # Initialize\n",
    "for key, value in best_trial_clf_aug.params.items():\n",
    "    # Remove the '_aug' suffix for actual model parameter names\n",
    "    param_name = key.replace('_aug', '')\n",
    "    best_classifier_params_augmented[param_name] = value\n",
    "    print(f\"    {param_name}: {value}\")\n",
    "\n",
    "\n",
    "# Add fixed params back\n",
    "best_classifier_params_augmented['objective'] = 'multiclass'\n",
    "best_classifier_params_augmented['metric'] = 'multi_logloss' \n",
    "best_classifier_params_augmented['verbosity'] = -1\n",
    "best_classifier_params_augmented['random_state'] = 42\n",
    "best_classifier_params_augmented['n_jobs'] = -1\n",
    "\n",
    "print(\"\\nBest augmented classifier parameters stored in 'best_classifier_params_augmented'.\")\n",
    "\n",
    "# 5. Train and evaluate the final augmented classifier\n",
    "print(\"\\nTraining a new model with the best hyperparameters for the augmented classifier...\")\n",
    "final_lgbm_clf_augmented = lgb.LGBMClassifier(**best_classifier_params_augmented)\n",
    "\n",
    "# Using NaN-handled training data and augmented validation data\n",
    "final_lgbm_clf_augmented.fit(X_train_augmented_final, y_clf_train_final, \n",
    "                             eval_set=[(X_val_augmented, y_clf_val)], \n",
    "                             callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
    "\n",
    "y_pred_val_best_aug = final_lgbm_clf_augmented.predict(X_val_augmented)\n",
    "\n",
    "print(\"\\n--- Evaluation of Augmented Model with Best Hyperparameters (on Validation Set) ---\")\n",
    "accuracy_val_best_aug = accuracy_score(y_clf_val, y_pred_val_best_aug)\n",
    "print(f\"Validation Accuracy with Best Params (Augmented): {accuracy_val_best_aug:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report with Best Params (Augmented Validation):\")\n",
    "class_names = ['No Trade (0)', 'Short (1)', 'Long (2)'] \n",
    "present_labels_best_aug = sorted(list(set(y_clf_val) | set(y_pred_val_best_aug)))\n",
    "current_class_names_best_aug = [class_names[i] for i in present_labels_best_aug if i < len(class_names)]\n",
    "if not current_class_names_best_aug: \n",
    "    current_class_names_best_aug = [f\"Class {i}\" for i in present_labels_best_aug]\n",
    "print(classification_report(y_clf_val, y_pred_val_best_aug, target_names=current_class_names_best_aug, labels=present_labels_best_aug, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Augmented Validation):\")\n",
    "cm_aug = confusion_matrix(y_clf_val, y_pred_val_best_aug, labels=present_labels_best_aug)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_aug, annot=True, fmt='d', cmap='Blues', xticklabels=current_class_names_best_aug, yticklabels=current_class_names_best_aug)\n",
    "plt.title('Confusion Matrix - Augmented Model (Validation Set)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Feature Importances (Augmented Model) ---\")\n",
    "if hasattr(final_lgbm_clf_augmented, 'feature_importances_'):\n",
    "    feature_importances_aug = pd.Series(final_lgbm_clf_augmented.feature_importances_, index=X_train_augmented_final.columns)\n",
    "    top_n_aug = 20\n",
    "    print(f\"Top {top_n_aug} features (Augmented Model):\")\n",
    "    print(feature_importances_aug.sort_values(ascending=False).head(top_n_aug))\n",
    "\n",
    "    plt.figure(figsize=(10, top_n_aug // 2 if top_n_aug > 10 else 5)) # Adjust figure size\n",
    "    feature_importances_aug.sort_values(ascending=False).head(top_n_aug).plot(kind='barh')\n",
    "    plt.title(f'Top {top_n_aug} Feature Importances (Augmented Model)')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.gca().invert_yaxis() # To display the most important feature at the top\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Could not retrieve feature importances from the augmented model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training Final Augmented Model on Full Train+Val Data ---\")\n",
    "# Ensure y_clf_train_final and y_clf_val are aligned with X_train_augmented_final and X_val_augmented\n",
    "X_train_val_augmented = pd.concat([X_train_augmented_final, X_val_augmented])\n",
    "y_clf_train_val = pd.concat([y_clf_train_final, y_clf_val])\n",
    "\n",
    "final_model_for_test = lgb.LGBMClassifier(**best_classifier_params_augmented)\n",
    "final_model_for_test.fit(X_train_val_augmented, y_clf_train_val) # No early stopping here, train on all available data before test\n",
    "\n",
    "print(\"\\n--- Final Evaluation on Test Set (Augmented Model) ---\")\n",
    "y_pred_test_aug = final_model_for_test.predict(X_test_augmented)\n",
    "accuracy_test_aug = accuracy_score(y_clf_test, y_pred_test_aug)\n",
    "print(f\"Test Accuracy (Augmented): {accuracy_test_aug:.4f}\")\n",
    "print(\"\\nClassification Report (Augmented Test Set):\")\n",
    "# (Use your class_names and present_labels logic for the report)\n",
    "print(classification_report(y_clf_test, y_pred_test_aug, target_names=class_names, labels=present_labels_best_aug, zero_division=0)) # Adjust labels if needed\n",
    "# Plot confusion matrix for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'final_lgbm_clf_augmented' is your best tuned classifier\n",
    "# And X_val_augmented, y_clf_val are your validation sets\n",
    "\n",
    "print(\"\\n--- Applying Custom Probability Thresholds ---\")\n",
    "\n",
    "# Get probabilities from the validation set\n",
    "y_pred_proba_val_aug = final_lgbm_clf_augmented.predict_proba(X_val_augmented)\n",
    "\n",
    "# Define your custom thresholds. These are examples; you'll need to tune them.\n",
    "# The idea is to require a higher confidence for classes where precision is low.\n",
    "# Format: {class_0_threshold, class_1_threshold, class_2_threshold}\n",
    "# If no probability exceeds its threshold, it can default to 'No Trade' or the class with max prob.\n",
    "# A simple approach: if max_proba < threshold_for_that_class -> predict No Trade (0)\n",
    "# More complex: specific threshold for each class to be chosen.\n",
    "\n",
    "# Example thresholds (these need to be found through experimentation on validation set output):\n",
    "# Let's say class 0 is 'No Trade', 1 is 'Short', 2 is 'Long'\n",
    "# thresholds = {\n",
    "#     0: 0.40,  # If max_proba is for class 0, and it's > 0.40, predict 0\n",
    "#     1: 0.35,  # If max_proba is for class 1, and it's > 0.35, predict 1\n",
    "#     2: 0.45   # If max_proba is for class 2, and it's > 0.45, predict 2\n",
    "#}\n",
    "# A simpler approach: a single confidence threshold for taking any trade.\n",
    "# If max_proba < general_confidence_threshold, predict 'No Trade'.\n",
    "\n",
    "# Let's try a strategy:\n",
    "# Predict the class with max probability, BUT if that max probability\n",
    "# is less than a specific threshold for THAT class, override to 'No Trade (0)'.\n",
    "# This aims to increase precision for 'Short' and 'Long' by being more selective.\n",
    "\n",
    "# These thresholds would be determined by analyzing predict_proba output vs actuals on validation\n",
    "# For example, for \"Long\" (class 2), you might find that when probability is > 0.55, precision is much better.\n",
    "custom_thresholds = {\n",
    "    0: 0.0,  # No minimum confidence for predicting 'No Trade' if it's already the highest\n",
    "    1: 0.38, # Example: require at least 38% confidence to call a 'Short'\n",
    "    2: 0.38  # Example: require at least 38% confidence to call a 'Long'\n",
    "}\n",
    "# The default 'No Trade' threshold (0.0) means if class 0 has highest prob, it's chosen.\n",
    "# If class 1 has highest prob but < 0.38, it becomes 0.\n",
    "# If class 2 has highest prob but < 0.38, it becomes 0.\n",
    "\n",
    "y_pred_val_custom_thresholds = np.zeros(len(y_pred_proba_val_aug), dtype=int)\n",
    "for i in range(len(y_pred_proba_val_aug)):\n",
    "    probs = y_pred_proba_val_aug[i]\n",
    "    predicted_class = np.argmax(probs) # Class with highest probability\n",
    "    \n",
    "    if probs[predicted_class] >= custom_thresholds.get(predicted_class, 0.0): # Check against threshold for that class\n",
    "        y_pred_val_custom_thresholds[i] = predicted_class\n",
    "    else:\n",
    "        y_pred_val_custom_thresholds[i] = 0 # Default to 'No Trade' if confidence is too low\n",
    "\n",
    "print(\"\\nClassification Report with Custom Thresholds (Validation):\")\n",
    "# (Use your class_names and present_labels logic for the report)\n",
    "present_labels_custom = sorted(list(set(y_clf_val) | set(y_pred_val_custom_thresholds)))\n",
    "current_class_names_custom = [class_names[i] for i in present_labels_custom if i < len(class_names)]\n",
    "if not current_class_names_custom: \n",
    "    current_class_names_custom = [f\"Class {i}\" for i in present_labels_custom]\n",
    "\n",
    "print(classification_report(y_clf_val, y_pred_val_custom_thresholds, target_names=current_class_names_custom, labels=present_labels_custom, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix with Custom Thresholds (Validation):\")\n",
    "cm_custom = confusion_matrix(y_clf_val, y_pred_val_custom_thresholds, labels=present_labels_custom)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_custom, annot=True, fmt='d', cmap='Blues', xticklabels=current_class_names_custom, yticklabels=current_class_names_custom)\n",
    "plt.title('Confusion Matrix - Custom Thresholds (Validation Set)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of iterating thresholds\n",
    "# You would run this on your validation set probabilities\n",
    "\n",
    "possible_thresholds = [0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65]\n",
    "best_f1_long = 0\n",
    "best_thresholds_for_long_f1 = {}\n",
    "\n",
    "# Assuming final_lgbm_clf_augmented is your trained model from the class_weight=None Optuna run\n",
    "y_pred_proba_val_aug = final_lgbm_clf_augmented.predict_proba(X_val_augmented)\n",
    "\n",
    "print(\"--- Searching for Optimal Thresholds (Example for Long F1) ---\")\n",
    "for short_thresh in possible_thresholds:\n",
    "    for long_thresh in possible_thresholds:\n",
    "\n",
    "        custom_thresholds = {\n",
    "            0: 0.0, # Default for 'No Trade' if it's highest prob\n",
    "            1: short_thresh,\n",
    "            2: long_thresh\n",
    "        }\n",
    "\n",
    "        y_pred_val_custom = np.zeros(len(y_pred_proba_val_aug), dtype=int)\n",
    "        default_no_trade_class = 0\n",
    "\n",
    "        for i in range(len(y_pred_proba_val_aug)):\n",
    "            probs = y_pred_proba_val_aug[i]\n",
    "            predicted_class_raw = np.argmax(probs)\n",
    "\n",
    "            if probs[predicted_class_raw] >= custom_thresholds.get(predicted_class_raw, 0.0):\n",
    "                y_pred_val_custom[i] = predicted_class_raw\n",
    "            else:\n",
    "                y_pred_val_custom[i] = default_no_trade_class\n",
    "\n",
    "        # Evaluate based on your priority\n",
    "        # For example, let's look at precision for Long, recall for Long, F1 for Long\n",
    "        report = classification_report(y_clf_val, y_pred_val_custom, output_dict=True, zero_division=0)\n",
    "\n",
    "        f1_long = report.get('2', {}).get('f1-score', 0) # Get F1 for class '2' (Long)\n",
    "        precision_long = report.get('2', {}).get('precision', 0)\n",
    "        recall_long = report.get('2', {}).get('recall', 0)\n",
    "\n",
    "        precision_short = report.get('1', {}).get('precision', 0)\n",
    "        recall_short = report.get('1', {}).get('recall', 0)\n",
    "\n",
    "        # Example: Find thresholds that maximize F1 for Longs while keeping its precision above a certain level\n",
    "        if f1_long > best_f1_long and precision_long > 0.55: # Example criteria\n",
    "             best_f1_long = f1_long\n",
    "             best_thresholds_for_long_f1 = {'short': short_thresh, 'long': long_thresh, \n",
    "                                            'long_f1': f1_long, 'long_precision': precision_long, 'long_recall': recall_long,\n",
    "                                            'short_precision': precision_short, 'short_recall': recall_short}\n",
    "\n",
    "        # You could print or store results for each combination to analyze\n",
    "        # print(f\"Thresh S:{short_thresh:.2f}/L:{long_thresh:.2f} -> P_L:{precision_long:.2f} R_L:{recall_long:.2f} F1_L:{f1_long:.2f} | P_S:{precision_short:.2f} R_S:{recall_short:.2f}\")\n",
    "\n",
    "print(\"\\nBest thresholds found based on example criteria (max Long F1 with P_L > 0.55):\")\n",
    "print(best_thresholds_for_long_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'final_lgbm_clf_augmented' not in locals():\n",
    "    print(\"WARNING: 'final_lgbm_clf_augmented' not found. Loading a dummy model for demonstration.\")\n",
    "    # This would typically be your trained LightGBM model\n",
    "    class DummyModel:\n",
    "        def predict_proba(self, X):\n",
    "            return np.random.rand(len(X), 3) / np.sum(np.random.rand(len(X), 3), axis=1, keepdims=True)\n",
    "    final_lgbm_clf_augmented = DummyModel()\n",
    "\n",
    "if 'X_test_augmented' not in locals() or 'y_clf_test' not in locals() or X_test_augmented.empty:\n",
    "    print(\"WARNING: Real X_test_augmented or y_clf_test not found. Creating DUMMY data.\")\n",
    "    num_test_samples = 1000\n",
    "    num_features_aug = X_test_augmented.shape[1] if ('X_test_augmented' in locals() and hasattr(X_test_augmented, 'shape')) else 148\n",
    "    X_test_augmented_cols = [f'feature_{i}' for i in range(num_features_aug -1)] + ['predicted_regression_target']\n",
    "    X_test_augmented = pd.DataFrame(np.random.rand(num_test_samples, num_features_aug), columns=X_test_augmented_cols)\n",
    "    y_clf_test = pd.Series(np.random.randint(0, 3, num_test_samples))\n",
    "    print(\"Dummy test data created.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Applying Chosen Thresholds to Test Set Predictions ---\")\n",
    "\n",
    "# 1. Get probability predictions on the TEST SET\n",
    "y_pred_proba_test_aug = final_lgbm_clf_augmented.predict_proba(X_test_augmented)\n",
    "\n",
    "# 2. Define the chosen thresholds from your validation set tuning\n",
    "chosen_thresholds = {\n",
    "    # class_label: min_probability_required\n",
    "    0: 0.0,          # If 'No Trade' has highest prob, accept it (or your tuned value)\n",
    "    1: 0.35,         # Your 'best' threshold for Short\n",
    "    2: 0.35          # Your 'best' threshold for Long\n",
    "}\n",
    "default_no_trade_class = 0\n",
    "\n",
    "print(f\"Using custom thresholds: {chosen_thresholds}\")\n",
    "\n",
    "# 3. Apply thresholds to get final class predictions for the test set\n",
    "y_pred_test_custom_thresholds = np.zeros(len(y_pred_proba_test_aug), dtype=int)\n",
    "for i in range(len(y_pred_proba_test_aug)):\n",
    "    probs = y_pred_proba_test_aug[i]\n",
    "    predicted_class_raw = np.argmax(probs) \n",
    "    \n",
    "    if probs[predicted_class_raw] >= chosen_thresholds.get(predicted_class_raw, 0.0):\n",
    "        y_pred_test_custom_thresholds[i] = predicted_class_raw\n",
    "    else:\n",
    "        y_pred_test_custom_thresholds[i] = default_no_trade_class\n",
    "\n",
    "# 4. Evaluate performance on the TEST SET with these custom thresholds\n",
    "print(\"\\n--- Final Evaluation on Test Set with Custom Thresholds ---\")\n",
    "accuracy_test_custom = accuracy_score(y_clf_test, y_pred_test_custom_thresholds)\n",
    "print(f\"Test Accuracy with Custom Thresholds: {accuracy_test_custom:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report with Custom Thresholds (Test Set):\")\n",
    "class_names = ['No Trade (0)', 'Short (1)', 'Long (2)'] \n",
    "present_labels_test_custom = sorted(list(set(y_clf_test) | set(y_pred_test_custom_thresholds)))\n",
    "current_class_names_test_custom = [class_names[i] for i in present_labels_test_custom if i < len(class_names)]\n",
    "if not current_class_names_test_custom: \n",
    "    current_class_names_test_custom = [f\"Class {i}\" for i in present_labels_test_custom]\n",
    "\n",
    "print(classification_report(y_clf_test, y_pred_test_custom_thresholds, target_names=current_class_names_test_custom, labels=present_labels_test_custom, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix with Custom Thresholds (Test Set):\")\n",
    "cm_test_custom = confusion_matrix(y_clf_test, y_pred_test_custom_thresholds, labels=present_labels_test_custom)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_test_custom, annot=True, fmt='d', cmap='Blues', xticklabels=current_class_names_test_custom, yticklabels=current_class_names_test_custom)\n",
    "plt.title('Confusion Matrix - Custom Thresholds (Test Set)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "if 'final_lgbm_clf_augmented' not in locals():\n",
    "    raise NameError(\"Trained model 'final_lgbm_clf_augmented' not found.\")\n",
    "if 'X_val_augmented' not in locals() or 'y_clf_val' not in locals():\n",
    "    raise NameError(\"Validation data (X_val_augmented, y_clf_val) not found.\")\n",
    "\n",
    "print(\"--- Starting Systematic Threshold Optimization on Validation Set ---\")\n",
    "\n",
    "# Get probability predictions on the validation set\n",
    "y_pred_proba_val = final_lgbm_clf_augmented.predict_proba(X_val_augmented)\n",
    "\n",
    "# Define ranges for thresholds to search\n",
    "# You can make these ranges and steps more or less granular\n",
    "short_thresholds_to_try = np.arange(0.30, 0.70, 0.05) \n",
    "long_thresholds_to_try = np.arange(0.30, 0.70, 0.05)\n",
    "default_no_trade_class = 0 # Your 'No Trade' class label\n",
    "\n",
    "best_overall_score = -1  # Initialize with a value that will be easily beaten\n",
    "best_thresholds = {}\n",
    "best_metrics = {}\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for short_thresh in short_thresholds_to_try:\n",
    "    for long_thresh in long_thresholds_to_try:\n",
    "        current_custom_thresholds = {\n",
    "            0: 0.0, # Or a minimum threshold for 'No Trade' if desired, e.g., 0.34\n",
    "            1: short_thresh,\n",
    "            2: long_thresh\n",
    "        }\n",
    "        \n",
    "        y_pred_val_custom = np.zeros(len(y_pred_proba_val), dtype=int)\n",
    "        for i in range(len(y_pred_proba_val)):\n",
    "            probs = y_pred_proba_val[i]\n",
    "            predicted_class_raw = np.argmax(probs)\n",
    "            \n",
    "            if probs[predicted_class_raw] >= current_custom_thresholds.get(predicted_class_raw, 0.0):\n",
    "                y_pred_val_custom[i] = predicted_class_raw\n",
    "            else:\n",
    "                y_pred_val_custom[i] = default_no_trade_class\n",
    "        \n",
    "        # Calculate metrics - focus on precision and F1 for actionable classes\n",
    "        precision_s = precision_score(y_clf_val, y_pred_val_custom, labels=[1], average='macro', zero_division=0)\n",
    "        recall_s = recall_score(y_clf_val, y_pred_val_custom, labels=[1], average='macro', zero_division=0)\n",
    "        f1_s = f1_score(y_clf_val, y_pred_val_custom, labels=[1], average='macro', zero_division=0)\n",
    "        \n",
    "        precision_l = precision_score(y_clf_val, y_pred_val_custom, labels=[2], average='macro', zero_division=0)\n",
    "        recall_l = recall_score(y_clf_val, y_pred_val_custom, labels=[2], average='macro', zero_division=0)\n",
    "        f1_l = f1_score(y_clf_val, y_pred_val_custom, labels=[2], average='macro', zero_division=0)\n",
    "        \n",
    "        # Count number of trades signaled\n",
    "        num_short_trades = np.sum(y_pred_val_custom == 1)\n",
    "        num_long_trades = np.sum(y_pred_val_custom == 2)\n",
    "\n",
    "        results_list.append({\n",
    "            'short_thresh': short_thresh, 'long_thresh': long_thresh,\n",
    "            'P_Short': precision_s, 'R_Short': recall_s, 'F1_Short': f1_s, 'N_Short': num_short_trades,\n",
    "            'P_Long': precision_l, 'R_Long': recall_l, 'F1_Long': f1_l, 'N_Long': num_long_trades,\n",
    "            'Overall_F1_Macro': f1_score(y_clf_val, y_pred_val_custom, average='macro', zero_division=0)\n",
    "        })\n",
    "\n",
    "        # Define your criteria for \"best\" - this is subjective\n",
    "        # Example: Maximize sum of F1_Short and F1_Long, but only if P_Short and P_Long > 0.55\n",
    "        current_score = f1_s + f1_l \n",
    "        if precision_s > 0.55 and precision_l > 0.55: # Minimum precision constraint\n",
    "            if current_score > best_overall_score:\n",
    "                best_overall_score = current_score\n",
    "                best_thresholds = {'short': short_thresh, 'long': long_thresh}\n",
    "                best_metrics = {\n",
    "                    'P_Short': precision_s, 'R_Short': recall_s, 'F1_Short': f1_s, 'N_Short': num_short_trades,\n",
    "                    'P_Long': precision_l, 'R_Long': recall_l, 'F1_Long': f1_l, 'N_Long': num_long_trades\n",
    "                }\n",
    "\n",
    "# Create a DataFrame from results for easier analysis\n",
    "df_threshold_results = pd.DataFrame(results_list)\n",
    "print(\"\\n--- Threshold Tuning Results (Validation Set) ---\")\n",
    "# Sort by a metric you care about, e.g., highest F1_Long with P_Long > 0.55\n",
    "print(df_threshold_results.sort_values(by=['F1_Long', 'P_Long'], ascending=[False, False]).head(10))\n",
    "\n",
    "\n",
    "print(\"\\nBest Thresholds Found (example criteria: Maximize Short_F1+Long_F1 with P_Short & P_Long > 0.55):\")\n",
    "if best_thresholds:\n",
    "    print(f\"Thresholds: Short={best_thresholds['short']:.2f}, Long={best_thresholds['long']:.2f}\")\n",
    "    print(\"Metrics with these thresholds on Validation Set:\")\n",
    "    for metric, value in best_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")\n",
    "else:\n",
    "    print(\"No thresholds met the example criteria (e.g., P_Short & P_Long > 0.55). Adjust criteria or threshold range.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'final_lgbm_clf_augmented' not in locals():\n",
    "    print(\"WARNING: 'final_lgbm_clf_augmented' not found. Loading a dummy model for demonstration.\")\n",
    "    class DummyModel: # Replace with loading your actual trained model if running in a new session\n",
    "        def predict_proba(self, X):\n",
    "            # Simulate probabilities that are somewhat skewed\n",
    "            raw_probs = np.random.rand(len(X), 3)\n",
    "            raw_probs[:,1] *= 1.1 # Slightly higher chance for short\n",
    "            return raw_probs / np.sum(raw_probs, axis=1, keepdims=True)\n",
    "    final_lgbm_clf_augmented = DummyModel()\n",
    "\n",
    "if 'X_test_augmented' not in locals() or 'y_clf_test' not in locals() or X_test_augmented.empty:\n",
    "    print(\"WARNING: Real X_test_augmented or y_clf_test not found. Creating DUMMY data.\")\n",
    "    num_test_samples = 1000\n",
    "    # Attempt to get num_features from X_test_augmented if it partially exists, or default\n",
    "    num_features_aug = X_test_augmented.shape[1] if ('X_test_augmented' in locals() and hasattr(X_test_augmented, 'shape')) else 148\n",
    "    \n",
    "    X_test_augmented_cols = [f'feature_{i}' for i in range(num_features_aug -1)] + ['predicted_regression_target']\n",
    "    X_test_augmented = pd.DataFrame(np.random.rand(num_test_samples, num_features_aug), columns=X_test_augmented_cols)\n",
    "    y_clf_test = pd.Series(np.random.randint(0, 3, num_test_samples))\n",
    "    print(\"Dummy test data created.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Applying YOUR CHOSEN Thresholds to Test Set Predictions ---\")\n",
    "\n",
    "# 1. Get probability predictions on the TEST SET\n",
    "y_pred_proba_test = final_lgbm_clf_augmented.predict_proba(X_test_augmented)\n",
    "\n",
    "# 2. Define THE CHOSEN thresholds from your validation set tuning\n",
    "chosen_thresholds = {\n",
    "    # class_label: min_probability_required\n",
    "    0: 0.0,  # If 'No Trade' has highest prob, accept it (or your tuned value for No Trade if any)\n",
    "    1: 0.40, # Your 'best' threshold for Short from validation tuning\n",
    "    2: 0.35  # Your 'best' threshold for Long from validation tuning\n",
    "}\n",
    "default_no_trade_class = 0\n",
    "\n",
    "print(f\"Using custom thresholds for Test Set: Short={chosen_thresholds[1]:.2f}, Long={chosen_thresholds[2]:.2f}\")\n",
    "\n",
    "# 3. Apply thresholds to get final class predictions for the test set\n",
    "y_pred_test_custom = np.zeros(len(y_pred_proba_test), dtype=int)\n",
    "for i in range(len(y_pred_proba_test)):\n",
    "    probs = y_pred_proba_test[i]\n",
    "    predicted_class_raw = np.argmax(probs) \n",
    "    \n",
    "    if probs[predicted_class_raw] >= chosen_thresholds.get(predicted_class_raw, 0.0):\n",
    "        y_pred_test_custom[i] = predicted_class_raw\n",
    "    else:\n",
    "        y_pred_test_custom[i] = default_no_trade_class\n",
    "\n",
    "# 4. Evaluate performance on the TEST SET with these custom thresholds\n",
    "print(\"\\n--- Final Evaluation on Test Set with YOUR CHOSEN Custom Thresholds ---\")\n",
    "accuracy_test_custom = accuracy_score(y_clf_test, y_pred_test_custom)\n",
    "print(f\"Test Accuracy with Custom Thresholds: {accuracy_test_custom:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report with Custom Thresholds (Test Set):\")\n",
    "class_names = ['No Trade (0)', 'Short (1)', 'Long (2)'] \n",
    "# Ensure present_labels_test_custom uses y_clf_test and y_pred_test_custom\n",
    "present_labels_test_custom = sorted(list(set(y_clf_test) | set(y_pred_test_custom)))\n",
    "current_class_names_test_custom = [class_names[i] for i in present_labels_test_custom if i < len(class_names)]\n",
    "if not current_class_names_test_custom: \n",
    "    current_class_names_test_custom = [f\"Class {i}\" for i in present_labels_test_custom]\n",
    "\n",
    "print(classification_report(y_clf_test, y_pred_test_custom, target_names=current_class_names_test_custom, labels=present_labels_test_custom, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix with Custom Thresholds (Test Set):\")\n",
    "cm_test_custom = confusion_matrix(y_clf_test, y_pred_test_custom, labels=present_labels_test_custom)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_test_custom, annot=True, fmt='d', cmap='Blues', xticklabels=current_class_names_test_custom, yticklabels=current_class_names_test_custom)\n",
    "plt.title('Confusion Matrix - Custom Thresholds (Test Set)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
