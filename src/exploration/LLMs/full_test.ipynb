{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4718610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orchestrator.py\n",
    "# requirements: pip install openai langchain pandas scikit-learn importlib\n",
    "\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import importlib.util\n",
    "from typing import TypedDict, Annotated, List\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.pregel import Pregel\n",
    "\n",
    "from feature_agent import feature_generator, correlation_inspector, load_data\n",
    "# your existing backtest functions\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from backtest import evaluate_regression, evaluate_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loop configuration ---\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "react_agent = create_react_agent(llm, tools=[correlation_inspector, feature_generator]) \n",
    "\n",
    "MAX_ITERATIONS = 5\n",
    "TARGET_COL = \"reg_target_lookahead6\"\n",
    "DATA_PATH = f\"parquet/labeled_data_6NQ.parquet\"\n",
    "CLASS_TARGET_COL = \"clf_target_numba_6\"\n",
    "\n",
    "# baseline models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_import_and_apply(df: pd.DataFrame, iteration: int) -> pd.DataFrame:\n",
    "    module_name = f\"generated_features_iter_{iteration}\"\n",
    "    file_path   = os.path.abspath(f\"{module_name}.py\")\n",
    "    \n",
    "    # 1) Unload any previous import of that module\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "    \n",
    "    # 2) Load it directly from the .py\n",
    "    spec   = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    \n",
    "    # 3) Apply\n",
    "    if not hasattr(module, \"apply_features\"):\n",
    "        raise AttributeError(f\"{module_name}.py is missing `apply_features(df)`\")\n",
    "    return module.apply_features(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33245124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_by_correlation(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    target_col: str,\n",
    "    top_k: int | None = None,\n",
    "    corr_thresh: float = 0.9\n",
    ") -> tuple[list[str], pd.Series, list[str]]:\n",
    "    \"\"\"\n",
    "    1) Drop zeroâ€variance features\n",
    "    2) Compute absolute Pearson correlation of each feature with target.\n",
    "    3) Take top_k, then drop pairwise correlations > corr_thresh\n",
    "    4) Fill any NaNs with 0 so no invalid values remain.\n",
    "    \"\"\"\n",
    "    # --- 1) Drop zeroâ€variance features ---\n",
    "    std = df[feature_cols].std()\n",
    "    non_const = std[std > 0].index.tolist()\n",
    "    \n",
    "    # --- 2) Corr with target (abs) ---\n",
    "    corr = df[non_const].corrwith(df[target_col]).abs()\n",
    "    corr = corr.dropna().sort_values(ascending=False)\n",
    "    if top_k is not None:\n",
    "        corr = corr.head(top_k)\n",
    "\n",
    "    ranked = list(corr.index)\n",
    "\n",
    "    # --- 3) Pairwise correlation matrix on the ranked features ---\n",
    "    #    Use numpy.errstate to suppress warnings just in case\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        mat = df[ranked].corr().abs()\n",
    "    mat = mat.fillna(0)  # turn any NaNs into 0\n",
    "\n",
    "    upper = mat.where(np.triu(np.ones(mat.shape), k=1).astype(bool))\n",
    "\n",
    "    # --- 4) Drop features too correlated with a higherâ€ranked one ---\n",
    "    dropped = [col for col in upper.columns if any(upper[col] > corr_thresh)]\n",
    "    final  = [f for f in ranked if f not in dropped]\n",
    "\n",
    "    return final, corr, dropped\n",
    "\n",
    "def evaluate_performance(df: pd.DataFrame, top_k: int = 20) -> dict:\n",
    "    \"\"\"\n",
    "    1) Pick the top_k features most correlated with the regression target.\n",
    "    2) Pick the top_k features most predictive of the classification target via ANOVA F-test.\n",
    "    3) Train separate models on each feature set and report metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # --- Prepare base X/y ---\n",
    "    # Numeric candidates\n",
    "    feat_cols = [\n",
    "        c for c in df.columns \n",
    "        if pd.api.types.is_numeric_dtype(df[c]) \n",
    "        and c not in [TARGET_COL, CLASS_TARGET_COL]\n",
    "    ]\n",
    "\n",
    "    # Regression target\n",
    "    mask_reg = df[feat_cols].notna().all(axis=1) & df[TARGET_COL].notna()\n",
    "    X_reg_full = df.loc[mask_reg, feat_cols]\n",
    "    y_reg      = df.loc[mask_reg, TARGET_COL]\n",
    "\n",
    "    # Classification target (if present)\n",
    "    mask_cls   = df[feat_cols].notna().all(axis=1) & df[CLASS_TARGET_COL].notna()\n",
    "    X_cls_full = df.loc[mask_cls, feat_cols]\n",
    "    y_cls      = df.loc[mask_cls, CLASS_TARGET_COL]\n",
    "\n",
    "    # --- 1) Regression feature selection by Pearson correlation ---\n",
    "    corr = X_reg_full.corrwith(y_reg).abs().sort_values(ascending=False)\n",
    "    reg_feats = corr.head(top_k).index.tolist()\n",
    "    metrics[\"regression_features\"] = reg_feats\n",
    "\n",
    "    # --- 2) Classification feature selection by ANOVA F-test ---\n",
    "    cls_feats = []\n",
    "    skb = SelectKBest(score_func=f_classif, k=top_k)\n",
    "    # fillna(0) or dropna as appropriate\n",
    "    skb.fit(X_cls_full.fillna(0), y_cls)\n",
    "    cls_feats = X_cls_full.columns[skb.get_support()].tolist()\n",
    "    metrics[\"classification_features\"] = cls_feats\n",
    "\n",
    "    # --- 3) Train & evaluate regression ---\n",
    "    Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "        X_reg_full[reg_feats], y_reg, test_size=0.3, random_state=42\n",
    "    )\n",
    "    reg = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-4, verbose=2)\n",
    "    reg.fit(Xr_train, yr_train)\n",
    "    preds_r = reg.predict(Xr_test)\n",
    "    metrics[\"mse\"] = mean_squared_error(yr_test, preds_r)\n",
    "\n",
    "    # --- 4) Train & evaluate classification ---\n",
    "    Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "        X_cls_full[cls_feats].fillna(0), y_cls, \n",
    "        test_size=0.3, random_state=42, stratify=y_cls\n",
    "    )\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=42, n_jobs=-4, verbose=2\n",
    "    )\n",
    "    clf.fit(Xc_train, yc_train)\n",
    "    preds_c = clf.predict(Xc_test)\n",
    "    metrics[\"f1_score\"] = f1_score(yc_test, preds_c, average=\"weighted\")\n",
    "    metrics[\"accuracy\"] = accuracy_score(yc_test, preds_c)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ba507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "def _clean_and_split(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    drop_cols: List[str]\n",
    ") -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Select numeric columns, drop infinities/NaNs, then split into X (features) and y (target).\n",
    "    drop_cols are removed from X.\n",
    "    \"\"\"\n",
    "    # 1) Numeric only\n",
    "    num = df.select_dtypes(include=\"number\").copy()\n",
    "    # 2) Replace infinities â†’ NaN\n",
    "    num.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # 3) Drop rows with any NaNs in num or target\n",
    "    if target_col not in num:\n",
    "        raise ValueError(f\"Target column '{target_col}' not in DataFrame\")\n",
    "    subset = num.dropna(subset=[target_col])\n",
    "    subset = subset.dropna(axis=1, how=\"all\")\n",
    "    # 4) Split X/y\n",
    "    y = subset[target_col]\n",
    "    X = subset.drop(columns=[target_col] + drop_cols, errors=\"ignore\")\n",
    "    return X, y\n",
    "\n",
    "def evaluate_two_pipelines(\n",
    "    df_reg: pd.DataFrame,\n",
    "    df_cls: pd.DataFrame,\n",
    "    target_reg: str,\n",
    "    target_cls: Optional[str] = None\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Train & evaluate:\n",
    "      - A regressor on df_reg predicting target_reg\n",
    "      - A classifier on df_cls predicting target_cls (if provided)\n",
    "    Returns a metrics dict including mse, r2_score, f1_score, accuracy, \n",
    "    and feature lists used.\n",
    "    \"\"\"\n",
    "    metrics: Dict[str, any] = {}\n",
    "\n",
    "    # --- Regression ---\n",
    "    Xr, yr = _clean_and_split(df_reg, target_col=target_reg, drop_cols=[target_cls] if target_cls else [])\n",
    "    Xr_tr, Xr_te, yr_tr, yr_te = train_test_split(Xr, yr, test_size=0.3, random_state=42)\n",
    "    reg = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-4, verbose=2)\n",
    "    reg.fit(Xr_tr, yr_tr)\n",
    "    preds_r = reg.predict(Xr_te)\n",
    "\n",
    "    metrics[\"mse\"]      = mean_squared_error(yr_te, preds_r)\n",
    "    metrics[\"r2_score\"] = r2_score(yr_te, preds_r)\n",
    "    metrics[\"regression_features\"] = Xr.columns.tolist()\n",
    "\n",
    "    # --- Classification (optional) ---\n",
    "    if target_cls and target_cls in df_cls.columns:\n",
    "        Xc, yc = _clean_and_split(df_cls, target_col=target_cls, drop_cols=[target_reg])\n",
    "        Xc_tr, Xc_te, yc_tr, yc_te = train_test_split(\n",
    "            Xc, yc, test_size=0.3, random_state=42, stratify=yc\n",
    "        )\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=100, class_weight=\"balanced\", random_state=42, verbose=2, n_jobs=-4\n",
    "        )\n",
    "        clf.fit(Xc_tr, yc_tr)\n",
    "        preds_c = clf.predict(Xc_te)\n",
    "\n",
    "        metrics[\"f1_score\"]              = f1_score(yc_te, preds_c, average=\"weighted\")\n",
    "        metrics[\"accuracy\"]              = accuracy_score(yc_te, preds_c)\n",
    "        metrics[\"classification_features\"] = Xc.columns.tolist()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdcc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_features(\n",
    "    df: pd.DataFrame,\n",
    "    missing_thresh: float = 0.2,\n",
    "    variance_thresh: float = 0.01,\n",
    "    corr_thresh: float = 0.9,\n",
    "    preserve: list[str] | None = None\n",
    ") -> tuple[pd.DataFrame, list[str]]:\n",
    "    \"\"\"\n",
    "    Drop bad columnsâ€”but never drop anything in `preserve`.\n",
    "    \"\"\"\n",
    "    if preserve is None:\n",
    "        preserve = []\n",
    "    preserve = set(preserve)\n",
    "\n",
    "    # 1) Missingâ€value drop\n",
    "    miss_frac   = df.isna().mean()\n",
    "    drop_missing = {c for c, f in miss_frac.items() if f > missing_thresh}\n",
    "\n",
    "    # 2) Lowâ€variance drop\n",
    "    numeric     = df.select_dtypes(include=\"number\")\n",
    "    selector    = VarianceThreshold(variance_thresh)\n",
    "    selector.fit(numeric.dropna())\n",
    "    low_var     = set(numeric.columns[~selector.get_support()])\n",
    "\n",
    "    # 3) Highâ€correlation drop\n",
    "    corr        = numeric.corr().abs().fillna(0)\n",
    "    upper       = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    drop_corr   = {c for c in upper.columns if any(upper[c] > corr_thresh)}\n",
    "\n",
    "    # 4) Combine, but never drop anything in preserve\n",
    "    to_drop = list((drop_missing | low_var | drop_corr) - preserve)\n",
    "    pruned  = df.drop(columns=to_drop)\n",
    "    return pruned, to_drop\n",
    "\n",
    "def prune_for_task(\n",
    "    df: pd.DataFrame,\n",
    "    preserve_cols: list[str],\n",
    "    missing_thresh: float = 0.2,\n",
    "    variance_thresh: float = 0.01,\n",
    "    corr_thresh: float = 0.9\n",
    ") -> tuple[pd.DataFrame, list[str]]:\n",
    "    \"\"\"\n",
    "    Prunes df (dropping lowâ€variance or highly correlated features)\n",
    "    but never drops any column in preserve_cols.\n",
    "    Returns the pruned DataFrame and list of dropped columns.\n",
    "    \"\"\"\n",
    "    # Use your existing prune_features but pass preserve\n",
    "    pruned, dropped = prune_features(\n",
    "        df,\n",
    "        missing_thresh=missing_thresh,\n",
    "        variance_thresh=variance_thresh,\n",
    "        corr_thresh=corr_thresh,\n",
    "        preserve=preserve_cols\n",
    "    )\n",
    "    return pruned, dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    iteration: int\n",
    "    task: str\n",
    "    feedback: str\n",
    "    generated_code: str\n",
    "    metrics: dict\n",
    "    # The 'agent_outcome' field is used by create_react_agent\n",
    "    agent_outcome: Annotated[List[BaseMessage], lambda x, y: x + y]\n",
    "\n",
    "# --- LangGraph Nodes ---\n",
    "\n",
    "def code_generation_node(state: AgentState):\n",
    "    \"\"\"Generates feature-engineering code, with a fallback load for df.\"\"\"\n",
    "    # â”€â”€â”€ Guard: pull df in if Pregel didnâ€™t pass it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if \"df\" not in state or state[\"df\"] is None:\n",
    "        print(\"â„¹ï¸ [generate] no df in state, loading from DATA_PATH\")\n",
    "        state[\"df\"] = pd.read_parquet(DATA_PATH)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    print(f\"\\n--- Iteration {state['iteration']}: Generating Code ---\")\n",
    "    print(\"   state keys:\", list(state.keys()))\n",
    "    existing = list(state[\"df\"].columns)\n",
    "    previous     = state.get(\"recent_kept\", [])\n",
    "    prompt = (\n",
    "        f\"Your DataFrame currently has {len(existing)} columns: {existing} â€¦\\n\"\n",
    "        f\"On the last iteration you _kept_ these {len(previous)} features: {previous}\"\n",
    "        \"We prune any feature with variance â‰¤0.01 or correlation â‰¥0.9.\\n\"\n",
    "        \"Generate 5 brand-new features with variance >0.01, corr <0.9 vs every existing column,\\n\"\n",
    "        \"don't just name them features_n, have a name for it to be easy to recognize, \\n\"\n",
    "        \"and with names not already in the DataFrame.\\n\"\n",
    "        \"Return only the raw Python code for `def apply_features(df):` (no markdown fences).\"\n",
    "    )\n",
    "    if state.get(\"feedback\"):\n",
    "        prompt += \"\\n\\nPrevious feedback:\\n\" + state[\"feedback\"]\n",
    "\n",
    "    agent_outcome = react_agent.invoke({\"messages\": [HumanMessage(content=prompt)]})\n",
    "    raw = agent_outcome[\"messages\"][-1].content\n",
    "    code = re.sub(r\"^```(?:python)?\\s*|\\s*```$\", \"\", raw.strip(), flags=re.MULTILINE)\n",
    "    return {\"generated_code\": code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98875355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_validation_and_application_node(state: AgentState):\n",
    "    print(\"--- Validating and Applying Code ---\")\n",
    "\n",
    "    # 1) Bootstrap df if missing\n",
    "    if \"df\" not in state or state[\"df\"] is None:\n",
    "        state[\"df\"] = pd.read_parquet(DATA_PATH)\n",
    "    df_orig   = state[\"df\"]\n",
    "    code      = state.get(\"generated_code\", \"\")\n",
    "    iteration = state[\"iteration\"]\n",
    "    module_name = f\"generated_features_iter_{iteration}\"\n",
    "    file_path   = os.path.abspath(f\"{module_name}.py\")\n",
    "\n",
    "    # 2) Syntax-check the generated code\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        return {\"feedback\": f\"Syntax error: {e.msg} (line {e.lineno})\"}\n",
    "\n",
    "    # 3) **Write** the code to disk so dynamic_import_and_apply can find it\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"# Auto-generated by code_generation_node\\n\")\n",
    "        f.write(\"import pandas as pd, numpy as np\\n\\n\")\n",
    "        f.write(code)\n",
    "\n",
    "    print(f\"âœï¸  Wrote {len(code)} characters to {file_path}\")\n",
    "        \n",
    "# 4) Load & apply\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "    spec   = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    df_new = module.apply_features(df_orig.copy())\n",
    "    added_raw = set(df_new.columns) - set(df_orig.columns)\n",
    "    print(\"ðŸ” Raw added columns:\", added_raw)\n",
    "\n",
    "    # 5) Prune\n",
    "    base_cols   = [\"open\",\"high\",\"low\",\"close\",\"volume\", TARGET_COL]\n",
    "    kept_so_far = list(df_orig.columns)  # after previous iteration\n",
    "    df_pruned, dropped = prune_features(\n",
    "        df_new,\n",
    "        missing_thresh=0.2,\n",
    "        variance_thresh=0.01,\n",
    "        corr_thresh=0.9,\n",
    "        preserve=base_cols + kept_so_far\n",
    "    )\n",
    "    print(\"ðŸ—‘ï¸  Dropped features:\", dropped)\n",
    "\n",
    "    # 3) persist and compare\n",
    "    state[\"df\"] = df_pruned\n",
    "    added = set(df_pruned.columns) - set(df_orig.columns)\n",
    "\n",
    "    # 6) Decide what to return\n",
    "    added_final = set(df_pruned.columns) - set(df_orig.columns)\n",
    "    if added_final:\n",
    "        print(\"ðŸŽ‰ Kept new columns:\", added_final)\n",
    "        return {\n",
    "            \"feedback\": None,\n",
    "            \"df\":       df_pruned,\n",
    "            \"recent_kept\": list(added_final)\n",
    "        }\n",
    "    else:\n",
    "        fb = \"No net new columns after pruning.\"\n",
    "        print(\"âš ï¸ \", fb)\n",
    "        return {\n",
    "            \"feedback\": fb,\n",
    "            \"df\":       df_pruned,\n",
    "            \"recent_kept\": list(added_final)\n",
    "        }\n",
    "\n",
    "def model_evaluation_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    Orchestrates pruning + evaluation on two separate pipelines.\n",
    "    \"\"\"\n",
    "    df_master = state[\"df\"]\n",
    "\n",
    "    # Normalize class target\n",
    "    cls_target = None\n",
    "    if isinstance(CLASS_TARGET_COL, str) and CLASS_TARGET_COL in df_master.columns:\n",
    "        cls_target = CLASS_TARGET_COL\n",
    "\n",
    "    # Base columns never to drop\n",
    "    base_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\", TARGET_COL]\n",
    "    if cls_target:\n",
    "        base_cols.append(cls_target)\n",
    "\n",
    "    # Prune for regression\n",
    "    df_reg, dropped_reg = prune_for_task(df_master.copy(), preserve_cols=base_cols)\n",
    "    print(\"ðŸ—‘ï¸  Dropped from regression pipeline:\", dropped_reg)\n",
    "\n",
    "    # Prune for classification\n",
    "    df_cls, dropped_cls = pd.DataFrame(), []\n",
    "    if cls_target:\n",
    "        df_cls, dropped_cls = prune_for_task(df_master.copy(), preserve_cols=base_cols)\n",
    "        print(\"ðŸ—‘ï¸  Dropped from classification pipeline:\", dropped_cls)\n",
    "\n",
    "    # Evaluate\n",
    "    metrics = evaluate_two_pipelines(df_reg, df_cls, target_reg=TARGET_COL, target_cls=cls_target)\n",
    "    print(\"Evaluation Metrics:\", metrics)\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"df_reg\":  df_reg,\n",
    "        \"df_cls\":  df_cls if cls_target else None\n",
    "    }\n",
    "\n",
    "def decision_node(state: AgentState):\n",
    "    print(\"--- Making a Decision ---\")\n",
    "    \n",
    "    # If feedback â‡’ retry next iteration\n",
    "    if state.get(\"feedback\"):\n",
    "        next_i = state[\"iteration\"] + 1\n",
    "        print(f\"âš ï¸  Feedback present, moving to iteration {next_i}\")\n",
    "        return {\"iteration\": next_i, \"next_step\": \"generate_code\"}\n",
    "    \n",
    "    # Maxâ€iters check\n",
    "    if state[\"iteration\"] >= MAX_ITERATIONS:\n",
    "        print(\"âœ… Reached max iterations â†’ done\")\n",
    "        return END\n",
    "    \n",
    "    # Otherwise continue\n",
    "    next_i = state[\"iteration\"] + 1\n",
    "    print(f\"ðŸ”„ No feedback â†’ iteration {next_i}\")\n",
    "    return {\"iteration\": next_i, \"next_step\": \"generate_code\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "def main_loop(max_iters: int = 10):\n",
    "    # 1) Bootstrap once\n",
    "    state: Dict[str, Any] = {\n",
    "        \"iteration\": 1,\n",
    "        \"task\":     \"Generate 5 new features to predict the target variable. \\n\"\n",
    "                    \"don't just name them features_n, have a name for it to be easy to recognize, \\n\"\n",
    "                    \"Focus on interaction terms and rolling statistics.\",\n",
    "        \"feedback\":  None,\n",
    "        \"df\":        pd.read_parquet(DATA_PATH),\n",
    "        \"metrics\":   {},\n",
    "    }\n",
    "\n",
    "    prev_mse = None\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        print(f\"Initial feature count {len(state['df'].columns)}\")\n",
    "        print(f\"\\nðŸš€ Iteration {state['iteration']}\")\n",
    "\n",
    "        # 2) Generate code\n",
    "        out = code_generation_node(state)\n",
    "        state.update(out)\n",
    "        print(\"  âœ” code_generation_node\")\n",
    "\n",
    "        # 3) Validate & apply\n",
    "        out = code_validation_and_application_node(state)\n",
    "        state.update(out)\n",
    "        print(\"  âœ” code_validation_and_application_node \"\n",
    "              f\"(features now {len(state['df'].columns)})\")\n",
    "\n",
    "        # 4) Evaluate\n",
    "        out = model_evaluation_node(state)\n",
    "        state.update(out)\n",
    "        curr_mse = state[\"metrics\"].get(\"mse\")\n",
    "        print(\"  âœ” model_evaluation_node \"\n",
    "              f\"(metrics={state['metrics']})\")\n",
    "        \n",
    "        if prev_mse is not None and curr_mse is not None:\n",
    "            imp = prev_mse - curr_mse\n",
    "            state[\"metrics\"][\"mse_improvement\"] = imp\n",
    "            print(f\"    â†³ mse_improvement = {imp:.3e}\")\n",
    "        else:\n",
    "            state[\"metrics\"][\"mse_improvement\"] = None\n",
    "            print(\"    â†³ first pass, no mse_improvement\")\n",
    "\n",
    "        # 5) Decide next step\n",
    "        out = decision_node(state)\n",
    "        if out is END:\n",
    "            print(\"ðŸ›‘ decision_node returned END. Exiting.\")\n",
    "            break\n",
    "        state.update(out)\n",
    "        print(f\"  âœ” decision_node â†’ next iteration={state['iteration']}\")\n",
    "\n",
    "        # 6) Earlyâ€stop if no improvement\n",
    "        if state[\"iteration\"] > 2 and state[\"metrics\"].get(\"mse_improvement\", 1e9) <= 0:\n",
    "            print(\"âš ï¸  No improvement after 2+ iterations; stopping.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nâœ… Final feature count:\", len(state[\"df\"].columns))\n",
    "    print(\"Columns:\", state[\"df\"].columns.tolist())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_loop(max_iters=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
