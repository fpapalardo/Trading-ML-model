{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries And Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import platform\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "from scipy.stats.mstats import winsorize\n",
    "import numba\n",
    "import pandas_ta as pta\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add one directory up to sys.path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Internal Libraries\n",
    "from data_loader import load_and_resample_data, apply_feature_engineering\n",
    "from backtest import evaluate_regression\n",
    "from labeling_utils import label_and_save\n",
    "from helpers import check_overfit, generate_oof_predictions, is_same_session\n",
    "#\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import Huber, MeanSquaredError\n",
    "import tensorflow as tf\n",
    "#\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.base import clone, BaseEstimator, RegressorMixin, ClassifierMixin\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, ElasticNet, Ridge\n",
    "from sklearn.metrics import classification_report, root_mean_squared_error, mean_squared_error, mean_absolute_error, r2_score, \\\n",
    "    confusion_matrix, precision_recall_curve, roc_curve, auc, accuracy_score, classification_report, f1_score, precision_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler, LabelEncoder\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n",
    "\n",
    "# Models and Training\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.svm import SVC\n",
    "#\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*There are no meaningful features.*\", category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is your Cell 5: Feature Engineering Function Definitions\n",
    "\n",
    "# --- Helper Functions for Custom Features (Some will be kept, some modified/called differently) ---\n",
    "# These helpers now expect columns to be named as pandas_ta typically names them (e.g., RSI_14, EMA_10, MACD_12_26_9, etc.)\n",
    "\n",
    "def add_price_vs_ma(df, price_col='close', ma_col_name='EMA_20', new_col_name_suffix='_vs_EMA20'):\n",
    "    # Ensure ma_col_name exists (it would have been created by pandas_ta)\n",
    "    if ma_col_name in df.columns and price_col in df.columns:\n",
    "        # Ensure inputs are numeric before division\n",
    "        df[price_col + new_col_name_suffix] = pd.to_numeric(df[price_col], errors='coerce') / pd.to_numeric(df[ma_col_name], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def add_ma_vs_ma(df, ma1_col_name='EMA_10', ma2_col_name='EMA_20', new_col_name_suffix='_vs_EMA20'):\n",
    "    if ma1_col_name in df.columns and ma2_col_name in df.columns:\n",
    "        df[ma1_col_name + new_col_name_suffix] = pd.to_numeric(df[ma1_col_name], errors='coerce') / pd.to_numeric(df[ma2_col_name], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def add_ma_slope(df, ma_col_name='EMA_10', new_col_name_suffix='_Slope_10', periods=1):\n",
    "    if ma_col_name in df.columns:\n",
    "        df[new_col_name_suffix] = pd.to_numeric(df[ma_col_name], errors='coerce').diff(periods) / periods\n",
    "    return df\n",
    "\n",
    "def add_rsi_signals(df, rsi_col_name='RSI_14', ob_level=70, os_level=30):\n",
    "    if rsi_col_name in df.columns:\n",
    "        rsi_series = pd.to_numeric(df[rsi_col_name], errors='coerce')\n",
    "        df[rsi_col_name + f'_Is_Overbought_{ob_level}'] = (rsi_series > ob_level).astype(int)\n",
    "        df[rsi_col_name + f'_Is_Oversold_{os_level}'] = (rsi_series < os_level).astype(int)\n",
    "    return df\n",
    "\n",
    "def add_stoch_signals(df, stoch_k_col_name='STOCHk_14_3_3', ob_level=80, os_level=20): # Default pandas_ta name for k\n",
    "    if stoch_k_col_name in df.columns:\n",
    "        stoch_k_series = pd.to_numeric(df[stoch_k_col_name], errors='coerce')\n",
    "        df[stoch_k_col_name + f'_Is_Overbought_{ob_level}'] = (stoch_k_series > ob_level).astype(int)\n",
    "        df[stoch_k_col_name + f'_Is_Oversold_{os_level}'] = (stoch_k_series < os_level).astype(int)\n",
    "    return df\n",
    "\n",
    "def add_macd_cross_signal(df, macd_col_name='MACD_12_26_9', signal_col_name='MACDs_12_26_9'): # Default pandas_ta name for signal\n",
    "    if macd_col_name in df.columns and signal_col_name in df.columns:\n",
    "        macd_series = pd.to_numeric(df[macd_col_name], errors='coerce')\n",
    "        signal_series = pd.to_numeric(df[signal_col_name], errors='coerce')\n",
    "        crossed_above = (macd_series > signal_series) & (macd_series.shift(1) < signal_series.shift(1))\n",
    "        crossed_below = (macd_series < signal_series) & (macd_series.shift(1) > signal_series.shift(1))\n",
    "        df[macd_col_name + '_Cross_Signal'] = np.where(crossed_above, 1, np.where(crossed_below, -1, 0))\n",
    "    return df\n",
    "\n",
    "def add_price_vs_bb(df, price_col='close', bb_upper_col='BBU_20_2.0', bb_lower_col='BBL_20_2.0'): # Default pandas_ta names\n",
    "    if price_col in df.columns and bb_upper_col in df.columns and bb_lower_col in df.columns:\n",
    "        price_series = pd.to_numeric(df[price_col], errors='coerce')\n",
    "        bb_upper_series = pd.to_numeric(df[bb_upper_col], errors='coerce')\n",
    "        bb_lower_series = pd.to_numeric(df[bb_lower_col], errors='coerce')\n",
    "        df[price_col + '_vs_BB_Upper'] = (price_series > bb_upper_series).astype(int)\n",
    "        df[price_col + '_vs_BB_Lower'] = (price_series < bb_lower_series).astype(int)\n",
    "    return df\n",
    "\n",
    "def add_psar_flip_signal(df, psar_col_name='PSARr_0.02_0.2', close_col='close'): # pandas_ta PSAR reversal column\n",
    "    if psar_col_name in df.columns: # PSARr is 1 for reversal to uptrend, -1 for reversal to downtrend\n",
    "        df['PSAR_Flip_Signal'] = pd.to_numeric(df[psar_col_name], errors='coerce').fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "# Keep these custom functions as they are generally good:\n",
    "def add_daily_vwap(df, high_col='high', low_col='low', close_col='close', volume_col='volume', new_col_name='VWAP_D'): # Changed name to VWAP_D for daily\n",
    "    # ... (your existing robust add_daily_vwap function - ensure it uses .copy() and numeric conversions internally)\n",
    "    # Make sure the final column is named VWAP_D or adjust add_price_vs_ma call later\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"Error: DataFrame index must be DatetimeIndex for daily VWAP.\")\n",
    "        return df\n",
    "    df_temp = df.copy()\n",
    "    for col in [high_col, low_col, close_col, volume_col]:\n",
    "        df_temp[col] = pd.to_numeric(df_temp[col], errors='coerce')\n",
    "    tpv = ((df_temp[high_col] + df_temp[low_col] + df_temp[close_col]) / 3) * df_temp[volume_col]\n",
    "    cumulative_tpv = tpv.groupby(df_temp.index.date).cumsum()\n",
    "    cumulative_volume = df_temp[volume_col].groupby(df_temp.index.date).cumsum()\n",
    "    vwap_series = cumulative_tpv / cumulative_volume\n",
    "    df[new_col_name] = vwap_series.replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_candle_features(df):\n",
    "    # ... (your existing add_candle_features function - ensure numeric conversions) ...\n",
    "    df_temp = df.copy()\n",
    "    for col in ['open', 'high', 'low', 'close']:\n",
    "        df_temp[col] = pd.to_numeric(df_temp[col], errors='coerce')\n",
    "    df['Candle_Range'] = df_temp['high'] - df_temp['low']\n",
    "    df['Candle_Body'] = (df_temp['close'] - df_temp['open']).abs()\n",
    "    df['Upper_Wick'] = df_temp['high'] - np.maximum(df_temp['open'], df_temp['close'])\n",
    "    df['Lower_Wick'] = np.minimum(df_temp['open'], df_temp['close']) - df_temp['low']\n",
    "    df['Body_vs_Range'] = (df['Candle_Body'] / df['Candle_Range'].replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    return df\n",
    "\n",
    "def add_return_features(df, price_col='close'):\n",
    "    # ... (your existing add_return_features function - ensure numeric conversions and inf handling) ...\n",
    "    price_series_num = pd.to_numeric(df[price_col], errors='coerce').replace(0, np.nan)\n",
    "    df[f'Log_Return_1'] = np.log(price_series_num / price_series_num.shift(1))\n",
    "    df[f'Log_Return_3'] = np.log(price_series_num / price_series_num.shift(3))\n",
    "    df[f'Log_Return_6'] = np.log(price_series_num / price_series_num.shift(6))\n",
    "    df[f'Simple_Return_1'] = price_series_num.pct_change(1)\n",
    "    for col_ret in [f'Log_Return_1', f'Log_Return_3', f'Log_Return_6', f'Simple_Return_1']:\n",
    "        if col_ret in df.columns: df[col_ret] = df[col_ret].replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "def add_rolling_stats(df, price_col='close', window1=14, window2=30):\n",
    "    # ... (your existing add_rolling_stats function - ensure numeric conversions and inf handling) ...\n",
    "    returns = pd.to_numeric(df[price_col], errors='coerce').pct_change(1).replace([np.inf, -np.inf], np.nan)\n",
    "    df[f'Rolling_Std_Dev_{window1}'] = returns.rolling(window=window1).std()\n",
    "    df[f'Rolling_Skew_{window2}'] = returns.rolling(window=window2).skew()\n",
    "    df[f'Rolling_Kurtosis_{window2}'] = returns.rolling(window=window2).kurt()\n",
    "    return df\n",
    "\n",
    "def add_lagged_features(df, cols_to_lag, lags=[1, 3, 6]):\n",
    "    # ... (your existing add_lagged_features function - ensure numeric conversions on source col if needed) ...\n",
    "    for col_orig in cols_to_lag:\n",
    "        if col_orig in df.columns:\n",
    "            series_to_lag = pd.to_numeric(df[col_orig], errors='coerce')\n",
    "            for lag in lags:\n",
    "                df[f'{col_orig}_Lag_{lag}'] = series_to_lag.shift(lag)\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Main Feature Generation Function using pandas_ta ---\n",
    "def add_all_features(df_input, suffix=''):\n",
    "    \"\"\"\n",
    "    Adds technical indicators and derived features using pandas_ta.\n",
    "    Assumes df_input has 'open', 'high', 'low', 'close', 'volume' columns (DatetimeIndex).\n",
    "    \"\"\"\n",
    "    if not isinstance(df_input.index, pd.DatetimeIndex):\n",
    "        print(f\"Warning: DataFrame for suffix '{suffix}' does not have a DatetimeIndex.\")\n",
    "    \n",
    "    df = df_input.copy() # Work on a copy\n",
    "\n",
    "    # Ensure base OHLCV columns are numeric and present\n",
    "    base_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    if not all(col in df.columns for col in base_cols):\n",
    "         raise ValueError(f\"DataFrame must contain {base_cols}. Found: {df.columns.tolist()}\")\n",
    "    for col in base_cols: # Ensure correct dtypes for pandas_ta\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df.dropna(subset=base_cols, inplace=True) # Drop rows if OHLCV became NaN\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"DataFrame became empty after coercing OHLCV for suffix '{suffix}'. Returning empty DataFrame.\")\n",
    "        # Return an empty dataframe with expected suffixed columns if possible or raise error\n",
    "        # For simplicity, we'll let it create columns that will be all NaN, then suffixing will apply.\n",
    "        # Or handle more gracefully by creating expected columns with NaNs.\n",
    "        # For now, we will proceed, and suffixing will apply to what gets created.\n",
    "        pass\n",
    "\n",
    "\n",
    "    print(f\"DataFrame shape for pandas_ta (suffix: {suffix}): {df.shape}\")\n",
    "\n",
    "    # I. Technical Indicators using pandas_ta\n",
    "    # Most pandas_ta functions automatically name columns (e.g., SMA_10, RSI_14)\n",
    "    # and handle NaNs internally. `append=True` adds them to df.\n",
    "\n",
    "    # Volume\n",
    "    df.ta.sma(close=df['volume'], length=20, append=True, col_names=('Volume_SMA_20'))\n",
    "    df = add_daily_vwap(df, new_col_name='VWAP_D') # Using your custom daily VWAP, named VWAP_D\n",
    "    df = add_price_vs_ma(df, ma_col_name='VWAP_D', new_col_name_suffix='_vs_VWAP_D')\n",
    "\n",
    "    # Volatility\n",
    "    df.ta.bbands(length=20, std=2, append=True) # Creates BBL_20_2.0, BBM_20_2.0, BBU_20_2.0, BBB_20_2.0, BBP_20_2.0\n",
    "    # Helpers will need these names: BBU_20_2.0, BBL_20_2.0\n",
    "    df = add_price_vs_bb(df, bb_upper_col='BBU_20_2.0', bb_lower_col='BBL_20_2.0')\n",
    "    df.ta.atr(length=14, append=True, col_names=('ATR_14')) # pandas_ta might name it ATRr_14 or similar. We force ATR_14.\n",
    "\n",
    "    # Trend\n",
    "    df.ta.sma(length=10, append=True) # SMA_10\n",
    "    df.ta.sma(length=20, append=True) # SMA_20 (also BBM_20_2.0 from bbands)\n",
    "    df.ta.sma(length=50, append=True) # SMA_50\n",
    "    df.ta.ema(length=10, append=True) # EMA_10\n",
    "    df.ta.ema(length=20, append=True) # EMA_20\n",
    "    df.ta.ema(length=50, append=True) # EMA_50\n",
    "    \n",
    "    df = add_price_vs_ma(df, ma_col_name='EMA_20', new_col_name_suffix='_vs_EMA20')\n",
    "    df = add_ma_vs_ma(df, ma1_col_name='EMA_10', ma2_col_name='EMA_20', new_col_name_suffix='_vs_EMA20')\n",
    "    df = add_ma_slope(df, ma_col_name='EMA_10', new_col_name_suffix='_Slope_10')\n",
    "\n",
    "    df.ta.macd(fast=12, slow=26, signal=9, append=True) # MACD_12_26_9, MACDh_12_26_9, MACDs_12_26_9\n",
    "    df = add_macd_cross_signal(df, macd_col_name='MACD_12_26_9', signal_col_name='MACDs_12_26_9')\n",
    "\n",
    "    df.ta.adx(length=14, append=True) # ADX_14, DMP_14, DMN_14\n",
    "    # Rename DMN_14 and DMP_14 to match your old Minus_DI_14, Plus_DI_14 if helpers depend on it\n",
    "    if 'DMP_14' in df.columns: df.rename(columns={'DMP_14': 'Plus_DI_14'}, inplace=True)\n",
    "    if 'DMN_14' in df.columns: df.rename(columns={'DMN_14': 'Minus_DI_14'}, inplace=True)\n",
    "    \n",
    "    df.ta.psar(append=True) # Creates PSARl_0.02_0.2, PSARs_0.02_0.2, PSARaf_0.02_0.2, PSARr_0.02_0.2\n",
    "    df = add_psar_flip_signal(df, psar_col_name='PSARr_0.02_0.2') # Use reversal column\n",
    "\n",
    "    df.ta.cci(length=20, append=True, col_names=('CCI_20')) # pandas_ta uses CCI_20_0.015 by default\n",
    "\n",
    "    # Momentum\n",
    "    df.ta.rsi(length=14, append=True) # RSI_14\n",
    "    df = add_rsi_signals(df, rsi_col_name='RSI_14')\n",
    "    \n",
    "    df.ta.stoch(k=14, d=3, smooth_k=3, append=True) # STOCHk_14_3_3, STOCHd_14_3_3\n",
    "    df = add_stoch_signals(df, stoch_k_col_name='STOCHk_14_3_3')\n",
    "\n",
    "    df.ta.ppo(fast=12, slow=26, signal=9, append=True) # PPO_12_26_9, PPOh_12_26_9, PPOs_12_26_9\n",
    "    df.ta.roc(length=10, append=True) # ROC_10\n",
    "    \n",
    "    # Explicitly convert PPO and ROC to numeric (belt and braces after pandas_ta)\n",
    "    for col_name in ['PPO_12_26_9', 'ROC_10']: # Check exact names if pandas_ta produces variants\n",
    "        base_name_ppo = [c for c in df.columns if \"PPO_\" in c and \"PPOh\" not in c and \"PPOs\" not in c]\n",
    "        base_name_roc = [c for c in df.columns if \"ROC_\" in c]\n",
    "        \n",
    "        for actual_col_name in base_name_ppo + base_name_roc:\n",
    "            if actual_col_name in df.columns:\n",
    "                df[actual_col_name] = df[actual_col_name].replace([np.inf, -np.inf], np.nan)\n",
    "                df[actual_col_name] = pd.to_numeric(df[actual_col_name], errors='coerce')\n",
    "\n",
    "    # II. Price Action & Basic Features (Keep your custom functions)\n",
    "    df = add_candle_features(df)\n",
    "    # df = add_candlestick_patterns(df) # We'll replace this with pandas_ta candlestick patterns\n",
    "\n",
    "    # --- pandas_ta Candlestick Patterns ---\n",
    "    # Example: Add Doji, Hammer, Engulfing. pandas_ta has many more.\n",
    "    # 'name=\"all\"' would add many columns, so be selective or use a list.\n",
    "    candle_patterns_to_check = [\"doji\", \"hammer\", \"engulfing\"] \n",
    "    df.ta.cdl_pattern(name=candle_patterns_to_check, append=True)\n",
    "    # Rename columns to match your old convention if needed, e.g., CDLDOJI -> Is_Doji\n",
    "    if 'CDLDOJI' in df.columns: df.rename(columns={'CDLDOJI': 'Is_Doji_pta'}, inplace=True) # Add _pta to distinguish\n",
    "    if 'CDLHAMMER' in df.columns: df.rename(columns={'CDLHAMMER': 'Is_Hammer_pta'}, inplace=True)\n",
    "    if 'CDLENGULFING' in df.columns: df.rename(columns={'CDLENGULFING': 'Is_Engulfing_pta'}, inplace=True) # This is a general engulfing signal (+/-)\n",
    "\n",
    "    df = add_return_features(df)\n",
    "\n",
    "    # III. Statistical Features (Keep your custom functions)\n",
    "    df = add_rolling_stats(df)\n",
    "    \n",
    "    # Lagged Features\n",
    "    # Ensure base columns for lagging are the ones created by pandas_ta or your helpers\n",
    "    cols_to_lag_pta = ['close', 'RSI_14', 'Candle_Body', 'Volume_SMA_20'] \n",
    "    # Check if these columns actually exist, as pandas_ta might name them slightly differently\n",
    "    # This valid_cols_to_lag should use the names as they are in df at this point\n",
    "    valid_cols_to_lag = [col for col in cols_to_lag_pta if col in df.columns]\n",
    "    df = add_lagged_features(df, valid_cols_to_lag, lags=[1,2,3])\n",
    "\n",
    "    # --- Suffixing ---\n",
    "    # All columns created by pandas_ta (that were appended) or by helpers\n",
    "    # that are not the original 'open', 'high', 'low', 'close', 'volume' will be suffixed.\n",
    "    current_cols = list(df.columns)\n",
    "    # Identify features generated in this function call (not the original base OHLCV)\n",
    "    generated_feature_cols = [col for col in current_cols if col not in base_cols]\n",
    "    \n",
    "    rename_dict = {col: col + suffix for col in generated_feature_cols}\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Time & Session Features (Keep as is) ---\n",
    "def add_time_session_features(df):\n",
    "    # ... (your existing add_time_session_features function) ...\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"Error: DataFrame index must be DatetimeIndex for time/session features.\")\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df['Hour_of_Day'] = df.index.hour\n",
    "    df['Minute_of_Hour'] = df.index.minute\n",
    "    df['Day_of_Week'] = df.index.dayofweek\n",
    "    time_fraction_of_day = df['Hour_of_Day'] + df['Minute_of_Hour'] / 60.0\n",
    "    df['Time_Sin'] = np.sin(2 * np.pi * time_fraction_of_day / 24.0)\n",
    "    df['Time_Cos'] = np.cos(2 * np.pi * time_fraction_of_day / 24.0)\n",
    "    df['Day_Sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7.0)\n",
    "    df['Day_Cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7.0)\n",
    "    df['Is_Asian_Session'] = ((df['Hour_of_Day'] >= 20) | (df['Hour_of_Day'] < 5)).astype(int)\n",
    "    df['Is_London_Session'] = ((df['Hour_of_Day'] >= 3) & (df['Hour_of_Day'] < 12)).astype(int)\n",
    "    df['Is_NY_Session'] = ((df['Hour_of_Day'] >= 8) & (df['Hour_of_Day'] < 17)).astype(int)\n",
    "    df['Is_Overlap'] = ((df['Hour_of_Day'] >= 8) & (df['Hour_of_Day'] < 12)).astype(int)\n",
    "    df['Is_US_Open_Hour'] = ((df['Hour_of_Day'] == 9) & (df['Minute_of_Hour'] >= 30) | (df['Hour_of_Day'] == 10) & (df['Minute_of_Hour'] < 30)).astype(int)\n",
    "    df['Is_US_Close_Hour'] = ((df['Hour_of_Day'] == 15) | (df['Hour_of_Day'] == 16) & (df['Minute_of_Hour'] == 0)).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_strategy = {\n",
    "    'SL_ATR_MULT': [1.0, 1.5, 0.5],\n",
    "    'TP_ATR_MULT': [2.0, 3.0, 4.0],\n",
    "    'TRAIL_START_MULT': [0.5, 1.0],\n",
    "    'TRAIL_STOP_MULT': [0.5, 1.0],\n",
    "    'TICK_VALUE': [5], \n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid_strategy.items())\n",
    "combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "avoid_funcs = {\n",
    "    #'avoid_hour_18_19': avoid_hour_18_19\n",
    "    #'news_window': avoid_news,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1min, resampled = load_and_resample_data(timeframes=['5min', '15min','1h'])\n",
    "\n",
    "df_5min = resampled['5min']\n",
    "df_15min = resampled['15min']\n",
    "df_1hr = resampled['1h']\n",
    "\n",
    "print(\"5min shape:\", df_5min.shape)\n",
    "print(\"15min shape:\", df_15min.shape)\n",
    "print(\"1h shape:\", df_1hr.shape)\n",
    "\n",
    "\n",
    "df_merged = apply_feature_engineering(\n",
    "    resampled=resampled,\n",
    "    add_all_features=add_all_features,\n",
    "    add_time_session_features=add_time_session_features,\n",
    "    timeframes=['5min', '15min', '1h'],\n",
    "    base_tf='5min'\n",
    ")\n",
    "\n",
    "# Verify Data\n",
    "# def check_feature_alignment(df_merged, tf_suffix, feature_keyword, sample_times):\n",
    "#     \"\"\"\n",
    "#     Check if features from the correct timeframe are available and what values they have at key timestamps.\n",
    "#     \"\"\"\n",
    "#     suffix_str = f\"_{tf_suffix}\"\n",
    "#     matching_cols = [col for col in df_merged.columns if feature_keyword in col and col.endswith(suffix_str)]\n",
    "    \n",
    "#     print(f\"\\nüîç Checking features containing '{feature_keyword}' with suffix '{suffix_str}'\")\n",
    "\n",
    "#     if not matching_cols:\n",
    "#         print(f\"‚ö†Ô∏è No matching columns found. Available columns ending with {suffix_str}:\")\n",
    "#         sample_suffix_cols = [col for col in df_merged.columns if col.endswith(suffix_str)]\n",
    "#         print(sample_suffix_cols[:10])  # Show only first 10 to keep it clean\n",
    "#         return\n",
    "\n",
    "#     for time in sample_times:\n",
    "#         if time not in df_merged.index:\n",
    "#             print(f\"‚ùå Time {time} not found in df_merged index.\")\n",
    "#         else:\n",
    "#             print(f\"\\n‚è∞ At time {time} ‚Äî values:\")\n",
    "#             print(df_merged.loc[time, matching_cols])\n",
    "\n",
    "# sample_times = [\n",
    "#     pd.Timestamp(\"2022-01-05 12:05:00-05:00\"),\n",
    "#     pd.Timestamp(\"2022-01-05 12:15:00-05:00\"),\n",
    "#     pd.Timestamp(\"2022-01-05 12:59:00-05:00\"),\n",
    "#     pd.Timestamp(\"2022-01-05 13:00:00-05:00\")\n",
    "# ]\n",
    "\n",
    "# check_feature_alignment(df_merged, tf_suffix=\"1h\", feature_keyword=\"RSI\", sample_times=sample_times)\n",
    "# check_feature_alignment(df_merged, tf_suffix=\"15min\", feature_keyword=\"MACD\", sample_times=sample_times)\n",
    "# check_feature_alignment(df_merged, tf_suffix=\"5min\", feature_keyword=\"EMA\", sample_times=sample_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = label_and_save(\n",
    "    df_input_features=df_merged,\n",
    "    lookahead_period=12,\n",
    "    vol_col_name='ATR_14_5min',\n",
    "    pt_multiplier=2.0,\n",
    "    sl_multiplier=1.0,\n",
    "    min_return_percentage=0.0005,\n",
    "    output_file_suffix='L12_PT2SL1VB12-model-research',\n",
    "    feature_columns_for_dropna=[]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_shape, units=32, dropout=0.2, lr=0.001, epochs=10, batch_size=32, verbose=0):\n",
    "        self.input_shape = input_shape\n",
    "        self.units = units\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(self.units, input_shape=(self.input_shape, 1)))\n",
    "        model.add(Dropout(self.dropout))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=Adam(learning_rate=self.lr), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(f\"üß† [ImprovedLSTMWrapper] Starting training with {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        self.model = self.build_model()\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "        print(f\"‚úÖ [LSTMWrapper] Finished training.\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        return self.model.predict(X).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1DWrapper:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.model = self.build_model()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=self.input_shape))\n",
    "        model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(1, activation='tanh'))  # outputs in [-1, 1]\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y, epochs=20, batch_size=128, verbose=1):\n",
    "        print(f\"\\nüîß [CNN1DWrapper] Scaling target and starting training...\")\n",
    "        y_scaled = self.scaler.fit_transform(y.reshape(-1, 1)).ravel()\n",
    "        start = time.time()\n",
    "        self.model.fit(X, y_scaled, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "        print(f\"‚úÖ [CNN1DWrapper] Training complete in {time.time() - start:.2f} seconds.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(f\"\\nüîÆ [CNN1DWrapper] Predicting on {X.shape}...\")\n",
    "        start = time.time()\n",
    "        y_scaled_pred = self.model.predict(X).ravel()\n",
    "        y_pred = self.scaler.inverse_transform(y_scaled_pred.reshape(-1, 1)).ravel()\n",
    "        print(f\"‚úÖ [CNN1DWrapper] Prediction done in {time.time() - start:.2f} seconds.\")\n",
    "\n",
    "        print(\"üîç Prediction Stats:\")\n",
    "        print(f\"Min: {y_pred.min():.6f} | Max: {y_pred.max():.6f}\")\n",
    "        print(f\"Mean: {y_pred.mean():.6f} | Std: {y_pred.std():.6f}\")\n",
    "\n",
    "        # Safety check for wild predictions\n",
    "        if abs(y_pred).max() > 1:\n",
    "            print(\"‚ö†Ô∏è Warning: Some predictions exceed ¬±1 ‚Äî consider checking target scaling or model output activation.\")\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_shape, num_classes=5, epochs=10, batch_size=32, verbose=0):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(32, input_shape=(self.input_shape, 1)))  # fixed 32 units\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(self.num_classes, activation=\"softmax\"))\n",
    "        model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(f\"üß† [LSTMClassifierWrapper] Training on {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "        X = np.array(X).reshape((len(X), self.input_shape, 1))\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        # If multiclass and y is integer labels, one-hot encode\n",
    "        if self.num_classes > 2 and y.ndim == 1:\n",
    "            y = to_categorical(y, num_classes=self.num_classes)\n",
    "\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "        print(f\"‚úÖ [LSTMClassifierWrapper] Training complete.\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X).reshape((len(X), self.input_shape, 1))\n",
    "        probs = self.model.predict(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X).reshape((len(X), self.input_shape, 1))\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === üîß Model Optimizers (XGBoost, CatBoost, RF, LightGBM)  CLASSIFICATION ===\n",
    "def tune_xgboost(\n",
    "    X_train, y_train, splits, trials, num_classes,\n",
    "    study_name=\"xgboost_opt\", db_path=\"sqlite:///optuna_studies/xgb_opt.db\"\n",
    "):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'objective': 'multi:softmax',\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'num_class': num_classes,\n",
    "        }\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=splits)\n",
    "        scores = []\n",
    "\n",
    "        for train_idx, val_idx in tscv.split(X_train):\n",
    "            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            model = xgb.XGBClassifier(**params, n_jobs=-4)\n",
    "            model.fit(X_tr, y_tr)\n",
    "            preds = model.predict(X_val)\n",
    "            scores.append(f1_score(y_val, preds, average=\"macro\"))\n",
    "\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=study_name,\n",
    "        storage=db_path,\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "def tune_catboost(X, y, splits, trials, num_classes, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli'])\n",
    "\n",
    "        class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1000, step=100),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "            'bootstrap_type': bootstrap_type,\n",
    "            'loss_function': 'MultiClass',\n",
    "            'eval_metric': 'TotalF1',\n",
    "            'class_weights': class_weights.tolist(),\n",
    "            'verbose': 0\n",
    "        }\n",
    "\n",
    "        if bootstrap_type == 'Bayesian':\n",
    "            params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 1)\n",
    "\n",
    "        model = CatBoostClassifier(**params)\n",
    "        tscv = TimeSeriesSplit(n_splits=splits)\n",
    "        scores = cross_val_score(model, X, y, cv=tscv, scoring='f1_macro', n_jobs=-4)\n",
    "        return scores.mean()\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "def tune_rf(X, y, splits, trials, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
    "        }\n",
    "\n",
    "        model = RandomForestClassifier(**params, random_state=42)\n",
    "        tscv = TimeSeriesSplit(n_splits=splits)\n",
    "        scores = cross_val_score(model, X, y, cv=tscv, scoring=\"f1_macro\", n_jobs=-4)\n",
    "        return scores.mean()\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "def tune_lightgbm(X, y, splits, trials, num_classes, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': num_classes,\n",
    "            'metric': 'multi_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'verbosity': -1,\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10)\n",
    "        }\n",
    "\n",
    "        scores = []\n",
    "        tscv = TimeSeriesSplit(n_splits=splits)\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            model.fit(X.iloc[train_idx], y[train_idx])\n",
    "            preds = model.predict(X.iloc[val_idx])\n",
    "            scores.append(f1_score(y[val_idx], preds, average='macro'))\n",
    "\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "def tune_logistic_regression(X, y, splits, trials, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "        C = trial.suggest_float(\"C\", 1e-4, 10.0, log=True)\n",
    "        class_weight = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "        params = {\n",
    "            \"penalty\": penalty,\n",
    "            \"C\": C,\n",
    "            \"solver\": \"lbfgs\",\n",
    "            \"multi_class\": \"multinomial\",\n",
    "            \"max_iter\": 2000,\n",
    "            \"class_weight\": class_weight\n",
    "        }\n",
    "\n",
    "        model = make_pipeline(StandardScaler(), LogisticRegression(**params))\n",
    "        tscv = TimeSeriesSplit(n_splits=splits)\n",
    "        scores = cross_val_score(model, X, y, cv=tscv, scoring=\"f1_macro\", n_jobs=-4)\n",
    "        return scores.mean()\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "def tune_ridge_classifier(X, y, splits, trials, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        alpha = trial.suggest_float(\"alpha\", 0.01, 10.0, log=True)\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"])\n",
    "\n",
    "        model = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            RidgeClassifier(alpha=alpha, solver=solver, class_weight=\"balanced\")\n",
    "        )\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=splits)\n",
    "        scores = cross_val_score(model, X, y, cv=tscv, scoring=\"f1_macro\", n_jobs=-4)\n",
    "        return scores.mean()\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "def tune_svc(X, y, splits, trials, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        C = trial.suggest_float(\"C\", 0.1, 10.0, log=True)\n",
    "        kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\"])\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"])\n",
    "\n",
    "        model = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SVC(C=C, kernel=kernel, gamma=gamma, class_weight='balanced')\n",
    "        )\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=splits)\n",
    "        scores = cross_val_score(model, X, y, cv=tscv, scoring=\"f1_macro\", n_jobs=-4)\n",
    "        return scores.mean()\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === üîß Model Optimizers (XGBoost, CatBoost, RF, LightGBM)  Regression ===\n",
    "def tune_xgb_regressor(X, y, splits, trials, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        }\n",
    "        model = xgb.XGBRegressor(**params, n_jobs=-4)\n",
    "        scores = cross_val_score(model, X, y, cv=TimeSeriesSplit(n_splits=splits), scoring=\"neg_mean_squared_error\", n_jobs=-4)\n",
    "        return scores.mean()\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "# === LightGBM Regressor ===\n",
    "def tune_lgb_regressor(X, y, splits, trials, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 128),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 15),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10)\n",
    "        }\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=TimeSeriesSplit(n_splits=splits), scoring=\"neg_mean_squared_error\", n_jobs=-4)\n",
    "        return scores.mean()\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "# === CatBoost Regressor ===\n",
    "def tune_catboost_regressor(X, y, splits, trials, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 1000, step=100),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", 0.5, 5.0),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n",
    "        }\n",
    "        model = CatBoostRegressor(**params, verbose=0)\n",
    "        scores = cross_val_score(model, X, y, cv=TimeSeriesSplit(n_splits=splits), scoring=\"neg_mean_squared_error\", n_jobs=-4)\n",
    "        return scores.mean()\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "# === Random Forest Regressor ===\n",
    "def tune_rf_regressor(X, y, splits, trials, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 20),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "        }\n",
    "        model = RandomForestRegressor(**params, random_state=42)\n",
    "        scores = cross_val_score(model, X, y, cv=TimeSeriesSplit(n_splits=splits), scoring=\"neg_mean_squared_error\", n_jobs=-4)\n",
    "        return scores.mean()\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "# === Ridge Regressor ===\n",
    "def tune_ridge_regressor(X, y, splits, trials, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        alpha = trial.suggest_float(\"alpha\", 0.01, 10.0, log=True)\n",
    "        model = make_pipeline(StandardScaler(), Ridge(alpha=alpha))\n",
    "        scores = cross_val_score(model, X, y, cv=TimeSeriesSplit(n_splits=splits), scoring=\"neg_mean_squared_error\", n_jobs=-4)\n",
    "        return scores.mean()\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "# === ElasticNet Regressor ===\n",
    "def tune_elasticnet_regressor(X, y, splits, trials, study_name, db_path):\n",
    "    def objective(trial):\n",
    "        alpha = trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True)\n",
    "        l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0)\n",
    "        model = make_pipeline(StandardScaler(), ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000))\n",
    "        scores = cross_val_score(model, X, y, cv=TimeSeriesSplit(n_splits=splits), scoring=\"neg_mean_squared_error\", n_jobs=-4)\n",
    "        return scores.mean()\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=db_path, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=trials)\n",
    "    return study.best_trial.params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_classifier(name, model, X, y_true, feature_names=None):\n",
    "    print(f\"\\nüîç Evaluating {name}\")\n",
    "    preds = model.predict(X)\n",
    "    probs = model.predict_proba(X) if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds, average='macro')\n",
    "    prec = precision_score(y_true, preds, average='macro')\n",
    "\n",
    "    print(f\"‚úÖ Accuracy: {acc:.4f}\")\n",
    "    print(f\"üéØ Macro F1 Score: {f1:.4f}\")\n",
    "    print(f\"üìå Precision (macro): {prec:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, preds))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"{name} - Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Feature Importance\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        top_features = [feature_names[i] for i in indices[:20]] if feature_names else None\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=importances[indices[:20]], y=top_features)\n",
    "        plt.title(f\"{name} - Top 20 Feature Importances\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.show()\n",
    "\n",
    "    elif hasattr(model, \"coef_\"):\n",
    "        coef = model.coef_\n",
    "        if coef.ndim == 2:\n",
    "            coef = coef[0]\n",
    "        if feature_names:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x=np.abs(coef), y=feature_names)\n",
    "            plt.title(f\"{name} - Coefficient Magnitudes\")\n",
    "            plt.xlabel(\"Magnitude\")\n",
    "            plt.show()\n",
    "\n",
    "    return preds, probs\n",
    "\n",
    "# === üß† Main training pipeline ===\n",
    "def run_classification_pipeline(\n",
    "    lookahead: int,\n",
    "    parquet_path: str,\n",
    "    feature_groups: dict,\n",
    "    model_list: list,\n",
    "    train_end: str,\n",
    "    val_end: str,\n",
    "    splits: int = 3,\n",
    "    trials: int = 20,\n",
    "    num_classes: int = 3,\n",
    "    study_folder: str = \"optuna_studies\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple classifiers on labeled trading data.\n",
    "\n",
    "    Parameters:\n",
    "        lookahead: Lookahead window (used for study naming)\n",
    "        parquet_path: Path to labeled parquet file\n",
    "        feature_groups: Dict with keys ['tree', 'linear', 'sequential'] and feature name lists\n",
    "        model_list: Models to run ['xgboost', 'catboost', 'rf', 'lightgbm', 'logreg']\n",
    "        train_end: Timestamp (str) to end training data\n",
    "        val_end: Timestamp (str) to end validation data\n",
    "        splits: TimeSeriesSplit folds\n",
    "        trials: Optuna trials\n",
    "        num_classes: Number of label classes (usually 3 for triple-barrier)\n",
    "        study_folder: Directory to store Optuna .db files\n",
    "    \"\"\"\n",
    "    os.makedirs(study_folder, exist_ok=True)\n",
    "\n",
    "    # === Load & split data ===\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    # üîÅ Recover datetime if it's the index\n",
    "    if df.index.name == 'datetime' or pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "        df = df.reset_index()\n",
    "\n",
    "    # ‚úÖ Ensure datetime column exists\n",
    "    if 'datetime' not in df.columns:\n",
    "        raise KeyError(\"‚ùå 'datetime' column is missing. Make sure it exists or was saved as index.\")\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime')\n",
    "\n",
    "    train = df[df['datetime'] < train_end]\n",
    "    val = df[(df['datetime'] >= train_end) & (df['datetime'] < val_end)]\n",
    "    test = df[df['datetime'] >= val_end]  # Hold out for backtest\n",
    "\n",
    "    print(f\"Train rows: {len(train)} | Val rows: {len(val)} | Test rows: {len(test)}\")\n",
    "\n",
    "    # === Features & labels ===\n",
    "    X_train = train[feature_groups['tree']]\n",
    "    X_val = val[feature_groups['tree']]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    clf_col = [col for col in df.columns if col.startswith(\"clf_target\")][0]\n",
    "    print(f\"üìå Using classification label column: {clf_col}\")\n",
    "    y_train = le.fit_transform(train[clf_col])\n",
    "    y_val = le.transform(val[clf_col])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # === Loop through models ===\n",
    "    for model_name in model_list:\n",
    "        print(f\"\\n=== üîÑ Running {model_name.upper()} training ===\")\n",
    "\n",
    "        study_name = f\"{model_name}_class_opt_{lookahead}\"\n",
    "        db_path = f\"sqlite:///{study_folder}/{study_name}.db\"\n",
    "\n",
    "        if model_name == \"xgboost\":\n",
    "            import xgboost as xgb\n",
    "            from xgboost import XGBClassifier\n",
    "            def tuner(X, y): return tune_xgboost(X, y, splits, trials, num_classes, study_name, db_path)\n",
    "            ModelClass = XGBClassifier\n",
    "\n",
    "        elif model_name == \"catboost\":\n",
    "            from catboost import CatBoostClassifier\n",
    "            def tuner(X, y): return tune_catboost(X, y, splits, trials, num_classes, study_name, db_path)\n",
    "            ModelClass = CatBoostClassifier\n",
    "\n",
    "        elif model_name == \"rf\":\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            def tuner(X, y): return tune_rf(X, y, splits, trials, study_name, db_path)\n",
    "            ModelClass = RandomForestClassifier\n",
    "\n",
    "        elif model_name == \"lightgbm\":\n",
    "            import lightgbm as lgb\n",
    "            def tuner(X, y): return tune_lightgbm(X, y, splits, trials, num_classes, study_name, db_path)\n",
    "            ModelClass = lgb.LGBMClassifier\n",
    "\n",
    "        elif model_name == \"logreg\":\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            def tuner(X, y): return tune_logistic_regression(X, y, splits, trials, study_name, db_path)\n",
    "            def ModelClass(**params): return make_pipeline(StandardScaler(), LogisticRegression(**params))\n",
    "\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unknown model: {model_name}\")\n",
    "            continue\n",
    "\n",
    "        best_params = tuner(X_train, y_train)\n",
    "        model = ModelClass(**best_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        preds, probs = analyze_classifier(\n",
    "            name=model_name.upper(),\n",
    "            model=model,\n",
    "            X=X_val,\n",
    "            y_true=y_val,\n",
    "            feature_names=X_train.columns.tolist()\n",
    "        )\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"model\": model,\n",
    "            \"params\": best_params,\n",
    "            \"predictions\": preds,\n",
    "            \"probabilities\": probs\n",
    "        }\n",
    "\n",
    "    return results, le, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_regression(name, model, X, y_true, y_dates=None, entry_prices=None, exit_prices=None):\n",
    "    preds = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
    "    r2 = r2_score(y_true, preds)\n",
    "    mae = mean_absolute_error(y_true, preds)\n",
    "\n",
    "    print(f\"\\nüìà {name.upper()} REGRESSION RESULTS\")\n",
    "    print(f\"‚úÖ RMSE: {rmse:.6f}\")\n",
    "    print(f\"üéØ R¬≤ Score: {r2:.4f}\")\n",
    "    print(f\"üìå MAE: {mae:.6f}\")\n",
    "\n",
    "    # Residual plot\n",
    "    residuals = y_true - preds\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(residuals, bins=50, alpha=0.6, edgecolor='k')\n",
    "    plt.title(f\"{name.upper()} Residual Distribution\")\n",
    "    plt.xlabel(\"Residual (Actual - Predicted)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Directional accuracy\n",
    "    accuracy = (np.sign(y_true) == np.sign(preds)).mean()\n",
    "    print(f\"üìä Directional Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # Price-based PnL\n",
    "    if y_dates is not None and entry_prices is not None and exit_prices is not None:\n",
    "        pred_direction = np.sign(preds)\n",
    "        valid = entry_prices.notna() & exit_prices.notna()\n",
    "\n",
    "        entry_valid = entry_prices[valid].reset_index(drop=True)\n",
    "        exit_valid = exit_prices[valid].reset_index(drop=True)\n",
    "        dates_valid = y_dates[valid.values].reset_index(drop=True)\n",
    "        price_return = (exit_valid - entry_valid) / entry_valid\n",
    "\n",
    "        point_pnl = pred_direction[valid] * (exit_valid - entry_valid)\n",
    "        dollar_pnl = point_pnl * 20  # NQ multiplier is $20/point\n",
    "\n",
    "        pnl_series = pd.Series(dollar_pnl.values, index=dates_valid)\n",
    "        pnl_series = pnl_series.cumsum()\n",
    "\n",
    "        pnl_series.plot(title=f\"{name.upper()} Realistic Price-Based PnL (NQ Futures)\", figsize=(10, 4))\n",
    "        plt.grid(True)\n",
    "        plt.ylabel(\"Cumulative PnL ($)\")\n",
    "        plt.show()\n",
    "\n",
    "    # SHAP interaction plot for tree-based models\n",
    "    if name.lower() in [\"xgboost\", \"lightgbm\", \"catboost\", \"rf\"]:\n",
    "        print(f\"\\nüìä SHAP Interaction Analysis for {name.upper()}\")\n",
    "\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        interaction_values = explainer.shap_interaction_values(X)\n",
    "\n",
    "        # Handle multiclass case (not expected in regression)\n",
    "        if isinstance(interaction_values, list):\n",
    "            interaction_values = interaction_values[0]\n",
    "\n",
    "        feature_names = X.columns if hasattr(X, 'columns') else [f'feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "        # === Self-contribution (diagonal of mean interaction matrix) ===\n",
    "        diag_matrix = np.diagonal(interaction_values, axis1=1, axis2=2)\n",
    "        diag_mean = np.abs(diag_matrix).mean(axis=0)\n",
    "        feature_self_impact = pd.Series(diag_mean, index=feature_names).sort_values()\n",
    "\n",
    "        print(\"\\nüîª Features with Least Self-Impact (may not help alone):\")\n",
    "        print(feature_self_impact.head(15).to_string())\n",
    "\n",
    "        # === Total contribution (sum of all interactions per feature) ===\n",
    "        total_contrib = np.abs(interaction_values).sum(axis=(0, 1))\n",
    "        feature_total_impact = pd.Series(total_contrib, index=feature_names).sort_values()\n",
    "\n",
    "        print(\"\\n‚ùå Features with Lowest Total Contribution (including interactions):\")\n",
    "        print(feature_total_impact.head(15).to_string())\n",
    "\n",
    "        # === üßπ Suggest dropping features with total impact == 0\n",
    "        zero_impact_features = feature_total_impact[feature_total_impact == 0].index.tolist()\n",
    "        if zero_impact_features:\n",
    "            print(f\"\\nüö´ Found {len(zero_impact_features)} features with 0 total SHAP interaction contribution.\")\n",
    "            print(\"Suggested drop list:\")\n",
    "            for f in zero_impact_features:\n",
    "                print(f\" - {f}\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ All features have some contribution.\")\n",
    "\n",
    "        # Optional: plot top and bottom\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        feature_total_impact.tail(20).plot(kind=\"barh\", ax=axs[0], title=\"Top 20 Contributors\")\n",
    "        feature_total_impact.head(20).plot(kind=\"barh\", ax=axs[1], title=\"Bottom 20 Contributors\")\n",
    "        axs[0].grid(True)\n",
    "        axs[1].grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def run_regression_pipeline(\n",
    "    lookahead: int,\n",
    "    parquet_path: str,\n",
    "    feature_groups: dict,\n",
    "    model_list: list,\n",
    "    train_end: str,\n",
    "    val_end: str,\n",
    "    splits: int = 3,\n",
    "    trials: int = 20,\n",
    "    study_folder: str = \"optuna_studies\"\n",
    "):\n",
    "    os.makedirs(study_folder, exist_ok=True)\n",
    "\n",
    "    # === Load & split data ===\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    if df.index.name == 'datetime' or pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "        df = df.reset_index()\n",
    "    if 'datetime' not in df.columns:\n",
    "        raise KeyError(\"‚ùå 'datetime' column is missing.\")\n",
    "    \n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime')\n",
    "\n",
    "    train = df[df['datetime'] < train_end]\n",
    "    val = df[(df['datetime'] >= train_end) & (df['datetime'] < val_end)]\n",
    "    test = df[df['datetime'] >= val_end]  # optional\n",
    "\n",
    "    print(f\"Train rows: {len(train)} | Val rows: {len(val)} | Test rows: {len(test)}\")\n",
    "\n",
    "    # === Features & labels ===\n",
    "    X_train = train[feature_groups['tree']]\n",
    "    X_val = val[feature_groups['tree']]\n",
    "\n",
    "    reg_col = [col for col in df.columns if col.startswith(\"reg_target\")][0]\n",
    "    print(f\"üìå Using regression target column: {reg_col}\")\n",
    "    y_train = train[reg_col]\n",
    "    y_val = val[reg_col]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # === Loop through models ===\n",
    "    for model_name in model_list:\n",
    "        print(f\"\\n=== üîÑ Running {model_name.upper()} training ===\")\n",
    "        study_name = f\"{model_name}_reg_opt_{lookahead}\"\n",
    "        db_path = f\"sqlite:///{study_folder}/{study_name}.db\"\n",
    "\n",
    "        if model_name == \"xgboost\":\n",
    "            from xgboost import XGBRegressor\n",
    "            def tuner(X, y): return tune_xgb_regressor(X, y, splits, trials, study_name, db_path)\n",
    "            ModelClass = XGBRegressor\n",
    "\n",
    "        elif model_name == \"lightgbm\":\n",
    "            import lightgbm as lgb\n",
    "            def tuner(X, y): return tune_lgb_regressor(X, y, splits, trials, study_name, db_path)\n",
    "            ModelClass = lgb.LGBMRegressor\n",
    "\n",
    "        elif model_name == \"catboost\":\n",
    "            from catboost import CatBoostRegressor\n",
    "            def tuner(X, y): return tune_catboost_regressor(X, y, splits, trials, study_name, db_path)\n",
    "            ModelClass = CatBoostRegressor\n",
    "\n",
    "        elif model_name == \"rf\":\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            def tuner(X, y): return tune_rf_regressor(X, y, splits, trials, study_name, db_path)\n",
    "            ModelClass = RandomForestRegressor\n",
    "\n",
    "        elif model_name == \"ridge\":\n",
    "            from sklearn.linear_model import Ridge\n",
    "            def tuner(X, y): return tune_ridge_regressor(X, y, splits, trials, study_name, db_path)\n",
    "            def ModelClass(**params): return make_pipeline(StandardScaler(), Ridge(**params))\n",
    "\n",
    "        elif model_name == \"elasticnet\":\n",
    "            from sklearn.linear_model import ElasticNet\n",
    "            def tuner(X, y): return tune_elasticnet_regressor(X, y, splits, trials, study_name, db_path)\n",
    "            def ModelClass(**params): return make_pipeline(StandardScaler(), ElasticNet(**params))\n",
    "\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unknown model: {model_name}\")\n",
    "            continue\n",
    "\n",
    "        best_params = tuner(X_train, y_train)\n",
    "        model = ModelClass(**best_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        entry_prices = val['open'].shift(-1).reset_index(drop=True)\n",
    "        exit_prices = val['close'].shift(-(1 + lookahead)).reset_index(drop=True)\n",
    "\n",
    "        y_val_dates = val['datetime'].reset_index(drop=True)  # <- extract dates from validation set\n",
    "        preds = analyze_regression(\n",
    "            model_name, model, X_val, y_val,\n",
    "            y_dates=y_val_dates,\n",
    "            entry_prices=entry_prices,\n",
    "            exit_prices=exit_prices\n",
    "        )\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"model\": model,\n",
    "            \"params\": best_params,\n",
    "            \"predictions\": preds\n",
    "        }\n",
    "\n",
    "    return results, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOKAHEAD=12\n",
    "PARQUET_PATH = f\"parquet/labeled_data_L12_PT2SL1VB12-model-research.parquet\"\n",
    "# Split date ranges\n",
    "TRAIN_END = \"2024-01-01\"\n",
    "VAL_END = \"2025-01-01\"\n",
    "\n",
    "# Extract full list of columns from the DataFrame\n",
    "df_sample = pd.read_parquet(PARQUET_PATH)\n",
    "\n",
    "# Handle datetime index if needed\n",
    "if df_sample.index.name == 'datetime' or df_sample.index.dtype.kind == 'M':\n",
    "    df_sample = df_sample.reset_index()\n",
    "\n",
    "# Define exclusion logic\n",
    "non_feature_patterns = ['datetime', 'open', 'high', 'low', 'close', 'volume', 'target', 'label']\n",
    "non_feature_cols = [col for col in df_sample.columns if any(pat in col.lower() for pat in non_feature_patterns)]\n",
    "\n",
    "# Select only numeric, non-excluded features\n",
    "tree_based_features = [\n",
    "    col for col in df_sample.columns\n",
    "    if col not in non_feature_cols and pd.api.types.is_numeric_dtype(df_sample[col])\n",
    "]\n",
    "\n",
    "FEATURE_GROUPS = {\n",
    "    \"tree\": tree_based_features,\n",
    "    # \"linear\": tree_based_features,\n",
    "    # \"sequential\": tree_based_features\n",
    "}\n",
    "\n",
    "REGRESSION_MODEL_LIST = [\"xgboost\", \"catboost\", \"lightgbm\", \"elasticnet\"]\n",
    "\n",
    "# Run regression pipeline\n",
    "reg_results, df_all_reg = run_regression_pipeline(\n",
    "    lookahead=LOOKAHEAD,\n",
    "    parquet_path=PARQUET_PATH,\n",
    "    feature_groups=FEATURE_GROUPS,\n",
    "    model_list=REGRESSION_MODEL_LIST,\n",
    "    train_end=TRAIN_END,\n",
    "    val_end=VAL_END,\n",
    "    splits=3,\n",
    "    trials=10,\n",
    "    study_folder=\"dbs\"\n",
    ")\n",
    "\n",
    "MODEL_LIST = [\"xgboost\", \"catboost\", \"rf\", \"lightgbm\", \"logreg\", \"ridge\", \"svc\"]\n",
    "\n",
    "# Run pipeline\n",
    "results, label_encoder, df_all = run_classification_pipeline(\n",
    "    lookahead=LOOKAHEAD,\n",
    "    parquet_path=PARQUET_PATH,\n",
    "    feature_groups=FEATURE_GROUPS,\n",
    "    model_list=MODEL_LIST,\n",
    "    train_end=TRAIN_END,\n",
    "    val_end=VAL_END,\n",
    "    splits=3,\n",
    "    trials=10,\n",
    "    num_classes=3,\n",
    "    study_folder=\"dbs\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_lookahead_for_session_regression(LOOKAHEAD, cutoff, splits):\n",
    "#     labeled = pd.read_parquet(f\"labeled_data_{LOOKAHEAD}_session_less.parquet\")\n",
    "\n",
    "#     cutoff_date = pd.Timestamp(cutoff, tz=\"America/New_York\")\n",
    "#     train = labeled[labeled['datetime'] < cutoff_date]\n",
    "#     test = labeled[labeled['datetime'] >= cutoff_date]\n",
    "\n",
    "#     X_train_tree = train[tree_based_features]\n",
    "#     X_test_tree = test[tree_based_features]\n",
    "\n",
    "#     X_train_seq  = train[sequential_features]\n",
    "#     X_test_seq = test[sequential_features]\n",
    "\n",
    "#     y_train_tree = train['log_return']\n",
    "#     y_test_tree = test['log_return']\n",
    "\n",
    "#     y_train_seq = train['vol_adj_return']\n",
    "#     y_test_seq = test['vol_adj_return']\n",
    "\n",
    "#     y_train_transformed = np.sign(y_train_seq) * np.log1p(np.abs(y_train_seq))\n",
    "\n",
    "#     print(f\"Train range: {train['datetime'].min()} to {train['datetime'].max()} | Rows: {len(train)}\")\n",
    "#     print(f\"Test range: {test['datetime'].min()} to {test['datetime'].max()} | Rows: {len(test)}\")\n",
    "\n",
    "#     ###########################\n",
    "#     ########## Models #########\n",
    "#     ###########################\n",
    "\n",
    "#     def tune_xgboost(X_train, y_train):\n",
    "#         def objective(trial):\n",
    "#             params = {\n",
    "#                 'n_estimators': 2000,\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.3, log=True),  # tighten low end\n",
    "#                 'max_depth': trial.suggest_int('max_depth', 6, 14),  # more complex trees\n",
    "#                 'subsample': trial.suggest_float('subsample', 0.7, 1.0),  # prevent underfitting\n",
    "#                 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),  # prevent weak splits\n",
    "#                 'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),  # reduce L1 regularization\n",
    "#                 'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),  # reduce L2 regularization\n",
    "#                 'min_child_weight': trial.suggest_int('min_child_weight', 1, 6),  # avoid pruning all splits\n",
    "#                 'gamma': trial.suggest_float('gamma', 0.0, 1.0),  # allow moderate split pruning\n",
    "#                 'tree_method': 'hist',\n",
    "#             }\n",
    "\n",
    "#             tscv = TimeSeriesSplit(n_splits=splits)\n",
    "#             scores = []\n",
    "\n",
    "#             for train_idx, val_idx in tscv.split(X_train):\n",
    "#                 X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "#                 y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "#                 model = XGBRegressor(**params, random_state=42, eval_metric='rmse', early_stopping_rounds=20)\n",
    "#                 model.fit(\n",
    "#                     X_tr, y_tr,\n",
    "#                     eval_set=[(X_val, y_val)],\n",
    "#                     verbose=False\n",
    "#                 )\n",
    "#                 preds = model.predict(X_val)\n",
    "#                 rmse = root_mean_squared_error(y_val, preds)\n",
    "#                 scores.append(rmse)\n",
    "\n",
    "#             return np.mean(scores)\n",
    "\n",
    "#         study = optuna.create_study(\n",
    "#             direction='minimize',\n",
    "#             study_name=f'xgb_opt_reg_{LOOKAHEAD}',\n",
    "#             sampler=optuna.samplers.TPESampler(seed=42),\n",
    "#             pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=50, reduction_factor=4),\n",
    "#             storage=f'sqlite:///xgb_opt_study_session_less.db',\n",
    "#             load_if_exists=True\n",
    "#         )\n",
    "#         study.optimize(objective, n_trials=50)\n",
    "#         return study.best_params\n",
    "\n",
    "#     def tune_lightgbm(X_train, y_train):\n",
    "#         def objective(trial):\n",
    "#             params = {\n",
    "#                 \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1500, step=100),\n",
    "#                 \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.05, log=True),\n",
    "#                 \"max_depth\": trial.suggest_int(\"max_depth\", 3, 5),\n",
    "#                 \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 30),\n",
    "#                 \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 40),\n",
    "#                 \"subsample\": trial.suggest_float(\"subsample\", 0.6, 0.9),\n",
    "#                 \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
    "#                 \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0.05, 1.0),\n",
    "#                 \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1.0, 10.0),\n",
    "#                 \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 10.0),\n",
    "#                 \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 0.2),\n",
    "#                 \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 50, 200),\n",
    "#                 \"boosting_type\": \"gbdt\",\n",
    "#                 \"verbosity\": -1,\n",
    "#                 \"metric\": \"rmse\"\n",
    "#             }\n",
    "#             tscv = TimeSeriesSplit(n_splits=splits)\n",
    "#             scores = []\n",
    "#             for train_idx, val_idx in tscv.split(X_train):\n",
    "#                 X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "#                 y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "#                 model = LGBMRegressor(**params, random_state=42, n_jobs=-2)\n",
    "#                 model.fit(\n",
    "#                     X_tr, y_tr,\n",
    "#                     eval_set=[(X_val, y_val)],\n",
    "#                     eval_metric=\"rmse\",\n",
    "#                     callbacks=[early_stopping(stopping_rounds=50)]\n",
    "#                 )\n",
    "#                 preds = model.predict(X_val)\n",
    "#                 rmse = root_mean_squared_error(y_val, preds)\n",
    "#                 scores.append(rmse)\n",
    "#                 print(f\"Trial {trial.number} RMSE: {np.mean(scores):.6f} | Params: {params}\")\n",
    "#             return np.mean(scores)\n",
    "\n",
    "#         study = optuna.create_study(\n",
    "#             direction=\"minimize\",\n",
    "#             study_name=f\"lgbm_opt_reg_{LOOKAHEAD}\",\n",
    "#             sampler=optuna.samplers.TPESampler(seed=42),\n",
    "#             pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=50, reduction_factor=4),\n",
    "#             storage=f\"sqlite:///lgbm_opt_study_session_less.db\",\n",
    "#             load_if_exists=True\n",
    "#         )\n",
    "#         study.optimize(objective, n_trials=50)\n",
    "#         return study.best_params\n",
    "\n",
    "#     def tune_catboost(X_train, y_train):\n",
    "#         def objective(trial):\n",
    "#             params = {\n",
    "#                 'iterations': 2000,\n",
    "#                 'depth': trial.suggest_int('depth', 4, 8),\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
    "#                 'loss_function': 'RMSE',\n",
    "#                 'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 3.0, 10.0),\n",
    "#                 'random_strength': trial.suggest_float('random_strength', 1.0, 5.0),\n",
    "#                 'bootstrap_type': 'Bayesian',\n",
    "#                 'bagging_temperature': trial.suggest_float('bagging_temperature', 0.1, 1.0),\n",
    "#                 'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#             }\n",
    "\n",
    "#             tscv = TimeSeriesSplit(n_splits=splits)\n",
    "#             scores = []\n",
    "\n",
    "#             for train_idx, val_idx in tscv.split(X_train):\n",
    "#                 X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "#                 y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "#                 model = CatBoostRegressor(**params, random_state=42)\n",
    "#                 model.fit(\n",
    "#                     X_tr, y_tr,\n",
    "#                     eval_set=(X_val, y_val),\n",
    "#                     use_best_model=True,\n",
    "#                     verbose=False,\n",
    "#                     early_stopping_rounds=30\n",
    "#                 )\n",
    "#                 preds = model.predict(X_val)\n",
    "#                 rmse = root_mean_squared_error(y_val, preds)\n",
    "#                 scores.append(rmse)\n",
    "\n",
    "#             return np.mean(scores)\n",
    "\n",
    "#         study = optuna.create_study(\n",
    "#             direction='minimize',\n",
    "#             study_name=f'catboost_opt_reg_{LOOKAHEAD}',\n",
    "#             sampler=optuna.samplers.TPESampler(seed=42),\n",
    "#             pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=50, reduction_factor=4),\n",
    "#             storage=f'sqlite:///catboost_opt_study_session_less.db',\n",
    "#             load_if_exists=True\n",
    "#         )\n",
    "#         study.optimize(objective, n_trials=50)\n",
    "#         return study.best_params\n",
    "\n",
    "#     def tune_meta_xgb(X_meta, y_meta):\n",
    "#         def objective(trial):\n",
    "#             params = {\n",
    "#                 'n_estimators': 1000,\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "#                 'max_depth': trial.suggest_int('max_depth', 2, 8),  # allow deeper if needed\n",
    "#                 'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#                 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#                 'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.5),\n",
    "#                 'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.5),\n",
    "#                 'gamma': trial.suggest_float('gamma', 0.0, 1.0),  # extra regularization\n",
    "#             }\n",
    "\n",
    "#             tscv = TimeSeriesSplit(n_splits=splits)\n",
    "#             scores = []\n",
    "\n",
    "#             for train_idx, val_idx in tscv.split(X_meta):\n",
    "#                 X_tr, X_val = X_meta.iloc[train_idx], X_meta.iloc[val_idx]\n",
    "#                 y_tr, y_val = y_meta.iloc[train_idx], y_meta.iloc[val_idx]\n",
    "\n",
    "#                 model = XGBRegressor(**params, random_state=42, n_jobs=-2, eval_metric='rmse')\n",
    "#                 model.fit(\n",
    "#                     X_tr, y_tr,\n",
    "#                     eval_set=[(X_val, y_val)]\n",
    "#                 )\n",
    "#                 preds = model.predict(X_val)\n",
    "#                 rmse = root_mean_squared_error(y_val, preds)\n",
    "#                 scores.append(rmse)\n",
    "\n",
    "#             return np.mean(scores)\n",
    "\n",
    "#         study = optuna.create_study(\n",
    "#             direction='minimize',\n",
    "#             study_name=f'meta_xgb_reg_{LOOKAHEAD}',\n",
    "#             sampler=optuna.samplers.TPESampler(seed=42),\n",
    "#             pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "#             storage=f'sqlite:///meta_xgb_session_less.db',\n",
    "#             load_if_exists=True\n",
    "#         )\n",
    "#         study.optimize(objective, n_trials=50)\n",
    "#         return study.best_params\n",
    "\n",
    "#     def inverse_log_signed(x):\n",
    "#         return np.sign(x) * (np.expm1(np.abs(x)))\n",
    "#     ################################################\n",
    "#     ####### Ensure index consistency\n",
    "#     ####### Sequential #######\n",
    "#     y_train_seq = y_train_seq.loc[X_train_seq.index]\n",
    "#     y_test_seq = y_test_seq.loc[X_test_seq.index]\n",
    "\n",
    "#     ################################################\n",
    "#     ####### Tune models\n",
    "#     ####### Tree Based #######\n",
    "#     catboost_params     = tune_catboost(X_train_tree, y_train_transformed)\n",
    "#     xgboost_params      = tune_xgboost(X_train_tree, y_train_transformed)\n",
    "#    # lgbm_params         = tune_lightgbm(X_train_tree, y_train_transformed)\n",
    "#     ####### Sequential #######\n",
    "#     # N/A\n",
    "\n",
    "#     ################################################\n",
    "#     ####### Train models\n",
    "#     ####### Tree Based #######\n",
    "#     catboost    = CatBoostRegressor(**catboost_params, random_state=42, verbose=0)\n",
    "#     xgboost     = XGBRegressor(**xgboost_params, random_state=42)\n",
    "#    # lgbm        = LGBMRegressor(**lgbm_params, random_state=42)\n",
    "#     catboost.fit(X_train_tree, y_train_transformed)\n",
    "#     xgboost.fit(X_train_tree, y_train_transformed)\n",
    "#   #  lgbm.fit(X_train_tree, y_train_transformed)\n",
    "#     ####### Sequential #######\n",
    "#     X_lstm = X_train_seq.values\n",
    "#     y_lstm = y_train_transformed.values\n",
    "#     lstm_model = LSTMWrapper(input_shape=X_lstm.shape[1])\n",
    "#     lstm_model.fit(X_lstm, y_lstm)  # wrapper does the reshaping\n",
    "#     X_lstm_test = X_test_seq.values\n",
    "#     lstm_preds = lstm_model.predict(X_lstm_test)\n",
    "\n",
    "#     X_cnn = X_train_seq.values.reshape((len(X_train_seq), X_train_seq.shape[1], 1))\n",
    "#     y_cnn = y_train_seq.values\n",
    "#     cnn_model = CNN1DWrapper(input_shape=(X_cnn.shape[1], 1))\n",
    "#     cnn_model.fit(X_cnn, y_cnn)\n",
    "#     X_cnn_test = X_test_seq.values.reshape((len(X_test_seq), X_test_seq.shape[1], 1))\n",
    "#     cnn_preds = cnn_model.predict(X_cnn_test)\n",
    "\n",
    "#     ################################################\n",
    "#     ####### OOF Predicition\n",
    "#     ####### Tree Based #######\n",
    "#     oof_tree = generate_oof_predictions([catboost, xgboost], X_train_tree, y_train_transformed, splits)\n",
    "\n",
    "#     print(\"\\nüîç Checking variance in OOF base model predictions Base model:\")\n",
    "#     print(oof_tree.describe())\n",
    "#     print(\"Std per model:\\n\", oof_tree.std())\n",
    "#     ####### Sequential #######\n",
    "#     oof_preds_cnn = generate_oof_cnn(CNN1DWrapper, X_train_seq, y_train_seq, splits)\n",
    "\n",
    "#     print(\"\\nüîç Checking variance in OOF base model predictions for CNN:\")\n",
    "#     print(pd.Series(oof_preds_cnn).describe())\n",
    "#     print(\"Std:\", np.std(oof_preds_cnn))\n",
    "\n",
    "#     ################################################\n",
    "#     ####### Meta Params and Training\n",
    "#     ####### Tree Based #######\n",
    "#     X_seq_np = X_train_seq.values\n",
    "#     lstm_oof = generate_oof_lstm(LSTMWrapper, X_seq_np, y_train_transformed, splits)  # <- I can give you this\n",
    "\n",
    "#     X_meta_train = pd.DataFrame({\n",
    "#         'cat': oof_tree.iloc[:, 0],\n",
    "#         'xgb': oof_tree.iloc[:, 1],\n",
    "#         'lstm': lstm_oof\n",
    "#     })\n",
    "\n",
    "#     X_meta_train.describe()\n",
    "#     print(\"Meta input std:\\n\", X_meta_train.std())\n",
    "#     print(\"Meta input correlation:\\n\", X_meta_train.corr())\n",
    "\n",
    "#     X_test_meta = pd.DataFrame({\n",
    "#         'cat': catboost.predict(X_test_tree),\n",
    "#         'xgb': xgboost.predict(X_test_tree),\n",
    "#         'lstm': lstm_model.predict(X_test_seq.values)\n",
    "#     })\n",
    "\n",
    "#     meta_params = tune_meta_xgb(X_meta_train, y_train_transformed)\n",
    "#     meta_model = XGBRegressor(**meta_params, random_state=42, objective='reg:squarederror', eval_metric='rmse')\n",
    "#     meta_model.fit(X_meta_train, y_train_transformed)\n",
    "\n",
    "#     ################################################\n",
    "#     ####### Evaluate Model\n",
    "#     def evaluate_model(name, model, Xtr, Xte, ytr, yte, transformed=False):\n",
    "#         train_preds = model.predict(Xtr)\n",
    "#         test_preds = model.predict(Xte)\n",
    "\n",
    "#         if transformed:\n",
    "#         # Inverse-transform predictions\n",
    "#             train_preds = np.sign(train_preds) * (np.expm1(np.abs(train_preds)))\n",
    "#             test_preds = np.sign(test_preds) * (np.expm1(np.abs(test_preds)))\n",
    "#             ytr = np.sign(ytr) * (np.expm1(np.abs(ytr)))\n",
    "        \n",
    "#         train_mse = mean_squared_error(ytr, train_preds)\n",
    "#         test_mse = mean_squared_error(yte, test_preds)\n",
    "#         overfit_ratio = test_mse / train_mse if train_mse != 0 else float('inf')\n",
    "\n",
    "#         print(f\"\\nüìä {name} Performance:\")\n",
    "#         print(f\"Train MSE: {train_mse:.8f}\")\n",
    "#         print(f\"Test MSE: {test_mse:.8f}\")\n",
    "#         print(f\"Overfit ratio (Test / Train): {overfit_ratio:.2f}\")\n",
    "#         if overfit_ratio > 1.5:\n",
    "#             print(\"‚ö†Ô∏è Potential overfitting detected.\")\n",
    "#         elif overfit_ratio < 0.7:\n",
    "#             print(\"‚ö†Ô∏è Possibly underfitting.\")\n",
    "#         else:\n",
    "#             print(\"‚úÖ Generalization looks reasonable.\")\n",
    "#         return test_preds\n",
    "    \n",
    "#     ####### Tree Based #######\n",
    "#     print(\"\\nEvaluation XGBoost\")\n",
    "#     preds_xgboost   = evaluate_model(\"XGBoostRegressor\", xgboost, X_train_tree, X_test_tree, y_train_transformed, y_test_seq, transformed=True)\n",
    "#     print(\"\\nEvaluation CatBoost\")\n",
    "#     preds_catboost  = evaluate_model(\"CatBoostRegressor\", catboost, X_train_tree, X_test_tree, y_train_transformed, y_test_seq, transformed=True)\n",
    "#     print(\"\\nEvaluation Stack\")\n",
    "#     preds_stack     = evaluate_model(\"StackingRegressor\", meta_model, X_meta_train, X_test_meta, y_train_transformed.values, y_test_seq.values, transformed=True)\n",
    "#  #   print(\"\\nEvaluation LGBM\")\n",
    "# #    preds_lgbm      = evaluate_model(\"LightGBM\", lgbm, X_train_tree, X_test_tree, y_train_transformed, y_test_tree, transformed=True)\n",
    "#     ####### Sequential #######\n",
    "#     X_cnn_train = X_train_seq.values.reshape((len(X_train_seq), X_train_seq.shape[1], 1))\n",
    "#     X_cnn_test = X_test_seq.values.reshape((len(X_test_seq), X_test_seq.shape[1], 1))\n",
    "\n",
    "#     print(\"\\nEvaluation LSTM\")\n",
    "#     preds_lstm       = evaluate_model(\"LSTM\", lstm_model, X_train_seq.values, X_test_seq.values, y_train_transformed.values, y_test_seq.values, transformed=True)\n",
    "#     print(\"\\nEvaluation CNN\")\n",
    "#     preds_cnn      = evaluate_model(\"CNN\", cnn_model, X_cnn_train, X_cnn_test, y_train_seq.values, y_test_seq.values)\n",
    "\n",
    "#     ################################################\n",
    "#     ####### Target Distribution\n",
    "#     ####### Tree based #######\n",
    "#     print(\"\\nüîç Target distribution Tree:\")\n",
    "#     print(y_train_tree.describe())\n",
    "#     ####### Sequential #######\n",
    "#     print(\"\\nüîç Target distribution Seq:\")\n",
    "#     print(y_train_seq.describe())\n",
    "    \n",
    "#     ################################################\n",
    "#     ####### Choose final model\n",
    "#     ####### Tree Based #######\n",
    "#     # print(\"\\nüîç Checking prediction variance from LGBM model:\")\n",
    "#     # print(f\"Min: {preds_lgbm.min():.8f}\")\n",
    "#     # print(f\"Max: {preds_lgbm.max():.8f}\")\n",
    "#     # print(f\"Mean: {preds_lgbm.mean():.8f}\")\n",
    "#     # print(f\"Std Dev: {preds_lgbm.std():.8f}\")\n",
    "#     # print(f\"First 5 Predictions: {preds_lgbm[:5]}\")\n",
    "\n",
    "#     # mae_lgbm = mean_absolute_error(y_test_tree, preds_lgbm)\n",
    "#     # rmse_lgbm = np.sqrt(mean_squared_error(y_test_tree, preds_lgbm))\n",
    "#     # r2_lgbm = r2_score(y_test_tree, preds_lgbm)\n",
    "\n",
    "#     # print(f\"MAE: {mae_lgbm:.4f}\")\n",
    "#     # print(f\"RMSE: {rmse_lgbm:.4f}\")\n",
    "#     # print(f\"R¬≤: {r2_lgbm:.4f}\")\n",
    "#     ####### Stacked Model #######\n",
    "#     print(\"\\nüîç Checking prediction variance from Stack model:\")\n",
    "#     print(f\"Min: {preds_stack.min():.8f}\")\n",
    "#     print(f\"Max: {preds_stack.max():.8f}\")\n",
    "#     print(f\"Mean: {preds_stack.mean():.8f}\")\n",
    "#     print(f\"Std Dev: {preds_stack.std():.8f}\")\n",
    "#     print(f\"First 5 Predictions: {preds_stack[:5]}\")\n",
    "\n",
    "#     mae = mean_absolute_error(y_test_seq, preds_stack)\n",
    "#     rmse = np.sqrt(mean_squared_error(y_test_seq, preds_stack))\n",
    "#     r2 = r2_score(y_test_seq, preds_stack)\n",
    "\n",
    "#     print(f\"MAE: {mae:.4f}\")\n",
    "#     print(f\"RMSE: {rmse:.4f}\")\n",
    "#     print(f\"R¬≤: {r2:.4f}\")\n",
    "#     ####### Sequential Solo #######\n",
    "#     print(\"\\nüîç Checking prediction variance from CNN model:\")\n",
    "#     print(f\"Min: {preds_cnn.min():.8f}\")\n",
    "#     print(f\"Max: {preds_cnn.max():.8f}\")\n",
    "#     print(f\"Mean: {preds_cnn.mean():.8f}\")\n",
    "#     print(f\"Std Dev: {preds_cnn.std():.8f}\")\n",
    "#     print(f\"First 5 Predictions: {preds_cnn[:5]}\")\n",
    "\n",
    "#     mae_cnn = mean_absolute_error(y_test_seq, preds_cnn)\n",
    "#     rmse_cnn = np.sqrt(mean_squared_error(y_test_seq, preds_cnn))\n",
    "#     r2_cnn = r2_score(y_test_seq, preds_cnn)\n",
    "\n",
    "#     print(f\"MAE: {mae_cnn:.4f}\")\n",
    "#     print(f\"RMSE: {rmse_cnn:.4f}\")\n",
    "#     print(f\"R¬≤: {r2_cnn:.4f}\")\n",
    "\n",
    "#     metadata = {\n",
    "#         \"lookahead\": LOOKAHEAD,\n",
    "#         \"xgboost_params\": xgboost_params,\n",
    "#         \"catboost_params\": catboost_params,\n",
    "#         \"meta_params\": meta_params,\n",
    "#         # \"lgbm_params\": lgbm_params\n",
    "#     }\n",
    "#     with open(f\"regression_metadata_{LOOKAHEAD}.json\", \"w\") as f:\n",
    "#         json.dump(metadata, f, indent=2)\n",
    "        \n",
    "#     joblib.dump(meta_model, f\"stack_model_regression_LOOKAHEAD_{LOOKAHEAD}_session_less.pkl\")\n",
    "#     joblib.dump(cnn_model, f\"cnn_model_regression_LOOKAHEAD_{LOOKAHEAD}_session_less.pkl\")\n",
    "#    #joblib.dump(lgbm, f\"lgbm_model_regression_LOOKAHEAD_{LOOKAHEAD}_session_less.pkl\")\n",
    "\n",
    "#     return {\n",
    "#         'lookahead': LOOKAHEAD,\n",
    "#         'preds_stack': preds_stack,\n",
    "#         'preds_cnn': preds_cnn,\n",
    "#      #   'preds_lgbm': preds_lgbm,\n",
    "#         'X_test_seq': X_test_seq,\n",
    "#         'X_test_meta': X_test_meta,\n",
    "#         'true_values': y_test_seq.values\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_lookahead_for_session_classification(LOOKAHEAD, cutoff, splits):\n",
    "#     labeled = pd.read_parquet(f\"labeled_data_{LOOKAHEAD}_session_less.parquet\")\n",
    "\n",
    "#     cutoff_date = pd.Timestamp(cutoff, tz=\"America/New_York\")\n",
    "#     train = labeled[labeled['datetime'] < cutoff_date]\n",
    "#     test = labeled[labeled['datetime'] >= cutoff_date]\n",
    "\n",
    "#     X_train_tree = train[tree_based_features]\n",
    "#     X_test_tree = test[tree_based_features]\n",
    "#     X_train_seq  = train[sequential_features]\n",
    "#     X_test_seq = test[sequential_features]\n",
    "#     X_train_linear  = train[linear_features]\n",
    "#     X_test_linear = test[linear_features]\n",
    "\n",
    "#     le = LabelEncoder()\n",
    "#     y_train_class = pd.Series(le.fit_transform(train['triple_barrier_label']), index=train.index)\n",
    "#     y_test_class = pd.Series(le.transform(test['triple_barrier_label']), index=test.index)\n",
    "\n",
    "#     X_train_linear = X_train_linear.loc[y_train_class.index]\n",
    "#     X_test_linear = X_test_linear.loc[y_test_class.index]\n",
    "\n",
    "#     print(f\"Train range: {train['datetime'].min()} to {train['datetime'].max()} | Rows: {len(train)}\")\n",
    "#     print(f\"Test range: {test['datetime'].min()} to {test['datetime'].max()} | Rows: {len(test)}\")\n",
    "\n",
    "#     ###########################\n",
    "#     ########## Models #########\n",
    "#     ###########################\n",
    "\n",
    "#     def tune_xgboost(X_train, y_train):\n",
    "#         def objective(trial):\n",
    "\n",
    "#             params = {\n",
    "#                 'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "#                 'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "#                 'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#                 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#                 'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "#                 'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "#                 'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "#                 'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "#                 'eval_metric': 'logloss',\n",
    "#                 'objective': 'multi:softmax',  # or 'multi:softprob' if you need probabilities\n",
    "#                 'num_class': len(np.unique(y_train))\n",
    "#             }\n",
    "\n",
    "#             tscv = TimeSeriesSplit(n_splits=splits)\n",
    "#             scores = []\n",
    "\n",
    "#             for train_idx, val_idx in tscv.split(X_train):\n",
    "#                 X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "#                 y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "#                 sample_weights = compute_sample_weight(class_weight='balanced', y=y_tr)\n",
    "\n",
    "#                 model = XGBClassifier(**params, random_state=42, n_jobs=-1)\n",
    "#                 model.fit(X_tr, y_tr, sample_weight=sample_weights)\n",
    "\n",
    "#                 preds = model.predict(X_val)\n",
    "#                 score = f1_score(y_val, preds, average='macro')\n",
    "#                 scores.append(score)\n",
    "\n",
    "#             print(f\"Trial {trial.number} F1 Score: {np.mean(scores):.5f} | Params: {params}\")\n",
    "#             return np.mean(scores)\n",
    "\n",
    "#         study = optuna.create_study(\n",
    "#             direction='maximize',\n",
    "#             study_name=f'xgb_opt_class_{LOOKAHEAD}',\n",
    "#             sampler=optuna.samplers.TPESampler(seed=42),\n",
    "#             pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "#             storage=f'sqlite:///xgb_opt_study_session_less.db',\n",
    "#             load_if_exists=True\n",
    "#         )\n",
    "#         study.optimize(objective, n_trials=2)\n",
    "#         return study.best_params\n",
    "\n",
    "#     def tune_rf(X_train, y_train):\n",
    "#         def objective(trial):\n",
    "#             params = {\n",
    "#                 \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000, step=100),\n",
    "#                 \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "#                 \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "#                 \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "#                 \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "#                 \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "#                 \"class_weight\": trial.suggest_categorical(\"class_weight\", [None, \"balanced\", \"balanced_subsample\"]),\n",
    "#                 \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"]),\n",
    "#             }\n",
    "\n",
    "#             tscv = TimeSeriesSplit(n_splits=splits)\n",
    "#             model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
    "\n",
    "#             scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring=\"f1_macro\", n_jobs=-1)\n",
    "#             print(f\"Trial {trial.number} F1 Score: {scores.mean():.5f} | Params: {params}\")\n",
    "#             return scores.mean()\n",
    "\n",
    "#         study = optuna.create_study(\n",
    "#             direction=\"maximize\",\n",
    "#             study_name=f\"rf_opt_class_{LOOKAHEAD}\",\n",
    "#             sampler=optuna.samplers.TPESampler(seed=42),\n",
    "#             pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "#             storage=f\"sqlite:///rf_opt_study_session_less.db\",\n",
    "#             load_if_exists=True\n",
    "#         )\n",
    "#         study.optimize(objective, n_trials=2)\n",
    "#         return study.best_params\n",
    "\n",
    "#     def tune_catboost(X_train, y_train):\n",
    "#         def objective(trial):\n",
    "#             bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli'])\n",
    "\n",
    "#             class_weights = compute_class_weight(\n",
    "#                 class_weight='balanced',\n",
    "#                 classes=np.unique(y_train),\n",
    "#                 y=y_train\n",
    "#             )\n",
    "\n",
    "#             params = {\n",
    "#                 'iterations': trial.suggest_int('iterations', 300, 1500, step=100),\n",
    "#                 'depth': trial.suggest_int('depth', 4, 10),\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "#                 'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "#                 'random_strength': trial.suggest_float('random_strength', 0.5, 5.0),\n",
    "#                 'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#                 'bootstrap_type': bootstrap_type,\n",
    "#                 'loss_function': 'MultiClass',\n",
    "#                 'eval_metric': 'TotalF1',\n",
    "#                 'class_weights': class_weights.tolist(),\n",
    "#                 'verbose': 0\n",
    "#             }\n",
    "\n",
    "#             if bootstrap_type == 'Bayesian':\n",
    "#                 params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0.0, 1.0)\n",
    "\n",
    "#             model = CatBoostClassifier(**params, random_state=42)\n",
    "\n",
    "#             tscv = TimeSeriesSplit(n_splits=splits)\n",
    "#             scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='f1_macro', n_jobs=-1)\n",
    "#             print(f\"Trial {trial.number} F1 Score: {scores.mean():.5f} | Params: {params}\")\n",
    "#             return scores.mean()\n",
    "\n",
    "#         study = optuna.create_study(\n",
    "#             direction='maximize',\n",
    "#             study_name=f'catboost_opt_class_{LOOKAHEAD}',\n",
    "#             sampler=optuna.samplers.TPESampler(seed=42),\n",
    "#             pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "#             storage=f'sqlite:///catboost_opt_study_session_less.db',\n",
    "#             load_if_exists=True\n",
    "#         )\n",
    "#         study.optimize(objective, n_trials=2)\n",
    "#         return study.best_params\n",
    "\n",
    "#     def tune_meta_logreg(X_meta, y_meta):\n",
    "#         def objective(trial):\n",
    "#             penalty = trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "#             if penalty is not None:\n",
    "#                 C = trial.suggest_float(\"C\", 1e-4, 10.0, log=True)\n",
    "#             else:\n",
    "#                 C = 1.0  # default, unused\n",
    "\n",
    "#             class_weight = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "#             params = {\n",
    "#                 \"penalty\": penalty,\n",
    "#                 \"C\": C,\n",
    "#                 \"solver\": \"lbfgs\",\n",
    "#                 \"max_iter\": 2000,\n",
    "#                 \"class_weight\": class_weight,\n",
    "#             }\n",
    "\n",
    "#             model = make_pipeline(StandardScaler(), LogisticRegression(**params, random_state=42))\n",
    "#             tscv = TimeSeriesSplit(n_splits=splits)\n",
    "\n",
    "#             scores = cross_val_score(model, X_meta, y_meta, cv=tscv, scoring=\"f1_macro\", n_jobs=-1)\n",
    "#             print(f\"Trial {trial.number} F1 Score: {scores.mean():.5f} | Params: {params}\")\n",
    "#             return scores.mean()\n",
    "\n",
    "#         study = optuna.create_study(\n",
    "#             direction=\"maximize\",\n",
    "#             study_name=f\"meta_logreg_class_{LOOKAHEAD}\",\n",
    "#             sampler=optuna.samplers.TPESampler(seed=42),\n",
    "#             pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "#             storage=f\"sqlite:///meta_logreg_stack_session_less.db\",\n",
    "#             load_if_exists=True\n",
    "#         )\n",
    "#         study.optimize(objective, n_trials=2)  # adjust trial count as needed\n",
    "#         return study.best_params\n",
    "\n",
    "#     def tune_lstm_classifier_with_optuna(X, y, splits, lookahead, num_classes=2):\n",
    "#         def objective(trial):\n",
    "#             units = trial.suggest_int(\"units\", 16, 128, step=16)\n",
    "#             lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "#             batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "#             epochs = trial.suggest_int(\"epochs\", 5, 30)\n",
    "\n",
    "#             scores = []\n",
    "#             tscv = TimeSeriesSplit(n_splits=splits)\n",
    "\n",
    "#             for train_idx, val_idx in tscv.split(X):\n",
    "#                 X_tr, X_val = X[train_idx], X[val_idx]\n",
    "#                 y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "#                 model = LSTMClassifierWrapper(\n",
    "#                     input_shape=X.shape[1],\n",
    "#                     units=units,\n",
    "#                     lr=lr,\n",
    "#                     epochs=epochs,\n",
    "#                     batch_size=batch_size,\n",
    "#                     verbose=0,\n",
    "#                     num_classes=num_classes\n",
    "#                 )\n",
    "#                 model.fit(X_tr, y_tr)\n",
    "#                 preds = model.predict(X_val)\n",
    "#                 acc = accuracy_score(y_val, preds)\n",
    "#                 scores.append(acc)\n",
    "\n",
    "#             mean_acc = np.mean(scores)\n",
    "#             print(f\"Trial {trial.number} Accuracy: {mean_acc:.5f} | Params: units={units}, lr={lr}, batch={batch_size}, epochs={epochs}\")\n",
    "#             return mean_acc\n",
    "\n",
    "#         study = optuna.create_study(\n",
    "#             direction=\"maximize\",\n",
    "#             study_name=\"lstm_class_opt\",\n",
    "#             storage=f\"sqlite:///lstm_class_opt_study{lookahead}_session_less.db\",\n",
    "#             load_if_exists=True\n",
    "#         )\n",
    "#         study.optimize(objective, n_trials=2)\n",
    "#         print(\"Best trial:\", study.best_trial.params)\n",
    "#         return study.best_trial.params\n",
    "#     ################################################\n",
    "#     ####### Ensure index consistency\n",
    "#     ####### Sequential #######\n",
    "#     y_train_seq = y_train_class.loc[X_train_seq.index]\n",
    "#     y_test_seq = y_test_class.loc[X_test_seq.index]\n",
    "\n",
    "#     ################################################\n",
    "#     ####### Tune models\n",
    "#     ####### Tree Based #######\n",
    "#     catboost_params     = tune_catboost(X_train_tree, y_train_class)\n",
    "#     xgboost_params      = tune_xgboost(X_train_tree, y_train_class)\n",
    "#     rf_params         = tune_rf(X_train_tree, y_train_class)\n",
    "#     ####### Sequential #######\n",
    "#     X_lstm = X_train_seq.values\n",
    "#     y_lstm = y_train_class.values\n",
    "\n",
    "#     ################################################\n",
    "#     ####### Train models\n",
    "#     ####### Tree Based #######\n",
    "#     xgb_weights = compute_sample_weight('balanced', y_train_class)\n",
    "#     cb_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_class), y=y_train_class)\n",
    "#     catboost    = CatBoostClassifier(**catboost_params, random_state=42, class_weights=cb_weights.tolist(), verbose=0)\n",
    "#     xgboost     = XGBClassifier(**xgboost_params, random_state=42)\n",
    "#     rf          = RandomForestClassifier(**rf_params, random_state=42)\n",
    "#     catboost.fit(X_train_tree, y_train_class)\n",
    "#     xgboost.fit(X_train_tree, y_train_class, sample_weight=xgb_weights)\n",
    "#     rf.fit(X_train_tree, y_train_class)\n",
    "#     ####### Sequential #######\n",
    "#     lstm_model = LSTMClassifierWrapper(input_shape=X_lstm.shape[1])\n",
    "#     lstm_model.fit(X_lstm, y_lstm)  # wrapper does the reshaping\n",
    "#     X_lstm_test = X_test_seq.values\n",
    "#     lstm_preds = lstm_model.predict(X_lstm_test)\n",
    "#     ################################################\n",
    "#     ####### OOF Predicition\n",
    "#     ####### Tree Based #######\n",
    "#     oof_tree = generate_oof_predictions_class([catboost, rf], X_train_tree, y_train_class, splits)\n",
    "#     oof_lstm = generate_oof_lstm_classifier(LSTMClassifierWrapper, X_lstm, y_lstm, splits)  # <- Uses sequential input\n",
    "\n",
    "#     ################################################\n",
    "#     ####### Train Meta Model\n",
    "#     ####### Tree Based #######\n",
    "#     X_meta_train = pd.DataFrame({\n",
    "#         'cat': oof_tree.iloc[:, 0].values,\n",
    "#         'rf': oof_tree.iloc[:, 1].values,\n",
    "#         'lstm': oof_lstm.values\n",
    "#     }, index=y_train_class.index)\n",
    "\n",
    "#     X_meta_test = pd.DataFrame({\n",
    "#         \"cat\": catboost.predict(X_test_tree).flatten(),\n",
    "#         \"rf\": rf.predict(X_test_tree).flatten(),\n",
    "#         \"lstm\": lstm_model.predict(X_test_seq.values).flatten()\n",
    "#     })\n",
    "\n",
    "#     X_meta_train_combined = pd.concat([\n",
    "#         X_meta_train.reset_index(drop=True),\n",
    "#         X_train_linear.reset_index(drop=True)\n",
    "#     ], axis=1)\n",
    "\n",
    "#     X_meta_test_combined = pd.concat([\n",
    "#         X_meta_test.reset_index(drop=True),\n",
    "#         X_test_linear.reset_index(drop=True)\n",
    "#     ], axis=1)\n",
    "\n",
    "#     meta_params = tune_meta_logreg(X_meta_train_combined, y_train_class)\n",
    "#     meta_model = make_pipeline(StandardScaler(),LogisticRegression(**meta_params, random_state=42))\n",
    "#     meta_model.fit(X_meta_train_combined, y_train_class)\n",
    "\n",
    "#     ################################################\n",
    "#     ####### Evaluate Model\n",
    "#     def evaluate_model(name, model, Xtr, Xte, ytr, yte):\n",
    "#         train_preds = model.predict(Xtr)\n",
    "#         test_preds = model.predict(Xte)\n",
    "\n",
    "#         train_acc = accuracy_score(ytr, train_preds)\n",
    "#         test_acc = accuracy_score(yte, test_preds)\n",
    "\n",
    "#         print(f\"\\nüìä {name} Classification Accuracy:\")\n",
    "#         print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "#         print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#         return test_preds\n",
    "    \n",
    "#     ####### Tree Based #######\n",
    "#     print(\"\\nXGBoost\")\n",
    "#     preds_xgboost   = evaluate_model(\"XGBoostRegressor\", xgboost, X_train_tree, X_test_tree, y_train_seq, y_test_seq)\n",
    "#     print(\"\\nCatboost\")\n",
    "#     preds_catboost  = evaluate_model(\"CatBoostRegressor\", catboost, X_train_tree, X_test_tree, y_train_seq, y_test_seq)\n",
    "#     print(\"\\nRF\")\n",
    "#     preds_rf        = evaluate_model(\"RandomForest\", rf, X_train_tree, X_test_tree, y_train_seq, y_test_seq)\n",
    "#     print(\"\\nLSTM\")\n",
    "#     preds_lstm       = evaluate_model(\"LSTM\", lstm_model, X_train_seq.values, X_test_seq.values, y_train_seq.values, y_test_seq.values)\n",
    "#     print(\"\\nMeta Model\")\n",
    "#     preds_stack     = evaluate_model(\"StackingRegressor\", meta_model, X_meta_train_combined, X_meta_test_combined, y_train_class.values, y_test_class.values)\n",
    "\n",
    "#     ################################################\n",
    "#     ####### Target Distribution\n",
    "#     print(\"\\nüîç Target distribution:\")\n",
    "#     print(y_train_class.describe())\n",
    "    \n",
    "#     ################################################\n",
    "#     ####### Choose final model\n",
    "#     ####### Tree Based #######\n",
    "#     preds_xgboost = xgboost.predict(X_test_tree)\n",
    "#     print(\"\\nüîç Checking XGBoost prediction distribution (classification):\")\n",
    "#     print(f\"Classes predicted: {np.unique(preds_xgboost)}\")\n",
    "#     print(f\"Prediction counts:\\n{pd.Series(preds_xgboost).value_counts()}\")\n",
    "\n",
    "#     # Classification metrics\n",
    "#     acc = accuracy_score(y_test_class, preds_xgboost)\n",
    "#     f1 = f1_score(y_test_class, preds_xgboost, average='macro')\n",
    "#     print(f\"Accuracy: {acc:.4f}\")\n",
    "#     print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "#     print(\"\\nClassification report:\")\n",
    "#     print(classification_report(y_test_class, preds_xgboost))\n",
    "\n",
    "#     ####### Stacked Model #######\n",
    "#     preds_meta_model = meta_model.predict(X_meta_test_combined)\n",
    "#     print(\"\\nüîç Checking Meta Model prediction distribution (classification):\")\n",
    "#     print(f\"Classes predicted: {np.unique(preds_meta_model)}\")\n",
    "#     print(f\"Prediction counts:\\n{pd.Series(preds_meta_model).value_counts()}\")\n",
    "\n",
    "#     # Classification metrics\n",
    "#     acc = accuracy_score(y_test_class, preds_meta_model)\n",
    "#     f1 = f1_score(y_test_class, preds_meta_model, average='macro')\n",
    "#     print(f\"Accuracy: {acc:.4f}\")\n",
    "#     print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "#     print(\"\\nClassification report:\")\n",
    "#     print(classification_report(y_test_class, preds_meta_model))\n",
    "\n",
    "#     metadata = {\n",
    "#         \"lookahead\": LOOKAHEAD,\n",
    "#         \"xgboost_params\": xgboost_params,\n",
    "#         \"catboost_params\": catboost_params,\n",
    "#         \"rf_params\": rf_params,\n",
    "#         \"meta_params\": meta_params,\n",
    "#     }\n",
    "#     with open(f\"classifier_metadata_{LOOKAHEAD}.json\", \"w\") as f:\n",
    "#         json.dump(metadata, f, indent=2)\n",
    "        \n",
    "#     joblib.dump(meta_model, f\"stack_model_classifier_LOOKAHEAD_{LOOKAHEAD}_session_less.pkl\")\n",
    "#     joblib.dump(xgboost, f\"xgboost_model_classifier_LOOKAHEAD_{LOOKAHEAD}_session_less.pkl\")\n",
    "\n",
    "#     return {\n",
    "#         'lookahead': LOOKAHEAD,\n",
    "#         'preds_stack': meta_model.predict_proba(X_meta_test_combined),\n",
    "#         'preds_xgboost': xgboost.predict_proba(X_test_tree),\n",
    "#         'X_test_tree': X_test_tree,\n",
    "#         'X_test_linear': X_test_linear,\n",
    "#         'X_meta_test_combined': X_meta_test_combined,\n",
    "#         'true_values': y_test_class.values,\n",
    "#         'label_encoder': le\n",
    "#     }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Training\n",
    "lookahead_values = [5, 10, 20]\n",
    "reg_results = []\n",
    "\n",
    "for val in lookahead_values:\n",
    "    cutoff = \"2025-04-01\"\n",
    "    splits=4\n",
    "    regression_models = run_lookahead_for_session_regression(val, cutoff, splits)\n",
    "    reg_results.append(regression_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Training\n",
    "lookahead_values = [10]\n",
    "class_results = []\n",
    "\n",
    "for val in lookahead_values:\n",
    "    cutoff = \"2025-04-01\"\n",
    "    splits=4\n",
    "    classification_models = run_lookahead_for_session_classification(val, cutoff, splits)\n",
    "    class_results.append(classification_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression StandAlone Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "thresholds = [0.000002]\n",
    "\n",
    "for result in reg_results:\n",
    "    lookahead = result['lookahead']\n",
    "    preds_stack = result['preds_stack']  # or 'preds_cnn'\n",
    "    preds_cnn = result['preds_cnn']\n",
    "    X_test_combined = result['X_test_meta']  # or 'X_test_seq'\n",
    "    y_test = result['true_values']\n",
    "    labeled = pd.read_parquet(f\"labeled_data_{lookahead}_session_less.parquet\")\n",
    "    df_backtest = labeled.copy()\n",
    "\n",
    "    print(f\"\\nüîé Predicted return range for LOOKAHEAD={lookahead}: STACK: min={preds_stack.min():.8f}, max={preds_stack.max():.8f} | CNN: min={preds_cnn.min():.8f}, max={preds_cnn.max():.8f}\")\n",
    "    for params in combinations:\n",
    "        for thresh in thresholds:\n",
    "            results = evaluate_regression(\n",
    "                X_test=X_test_combined,\n",
    "                preds_stack=preds_stack,\n",
    "                preds_cnn=preds_cnn,\n",
    "                labeled=labeled,\n",
    "                df=df_backtest,\n",
    "                avoid_funcs=avoid_funcs,\n",
    "                SL_ATR_MULT=params['SL_ATR_MULT'],\n",
    "                TP_ATR_MULT=params['TP_ATR_MULT'],\n",
    "                TRAIL_START_MULT=params['TRAIL_START_MULT'],\n",
    "                TRAIL_STOP_MULT=params['TRAIL_STOP_MULT'],\n",
    "                TICK_VALUE=params['TICK_VALUE'],\n",
    "                is_same_session=is_same_session,\n",
    "                long_thresh=thresh,\n",
    "                short_thresh=-thresh,\n",
    "                base_contracts=1,\n",
    "                max_contracts=5,\n",
    "                skip_weak_conf=True,\n",
    "                weak_conf_zscore=0.2\n",
    "            )\n",
    "\n",
    "            results['params'] = params\n",
    "            results['threshold'] = thresh\n",
    "            all_results.append(results)\n",
    "\n",
    "            print(f\"\\n\\nüîç Evaluating with params: {params}\")\n",
    "\n",
    "            print(\n",
    "                f\"\\n‚úÖ LOOKAHEAD={lookahead} | Threshold={thresh}\"\n",
    "                f\"\\nPnL: ${results['pnl']:.2f}\"\n",
    "                f\"\\nTrades: {results['trades']}\"\n",
    "                f\"\\nWin Rate: {results['win_rate']:.2%}\"\n",
    "                f\"\\nExpectancy: {results['expectancy']:.2f}\"\n",
    "                f\"\\nProfit Factor: {results['profit_factor']:.2f}\"\n",
    "                f\"\\nSharpe Ratio: {results['sharpe']:.2f}\"\n",
    "                f\"\\nLong Trades: {results['long_trades']} | Short Trades: {results['short_trades']}\"\n",
    "            )\n",
    "\n",
    "            print(\"Avoid Hits:\")\n",
    "            for name, count in results['avoid_hits'].items():\n",
    "                print(f\" - {name}: {count}\")\n",
    "\n",
    "            if not results['results'].empty and 'pnl' in results['results'].columns:\n",
    "                print(\"\\nüî¢ Top 5 PnL trades:\")\n",
    "                print(results['results'].sort_values(by='pnl', ascending=False).head(5))\n",
    "\n",
    "                print(\"\\nüîª Bottom 5 PnL trades:\")\n",
    "                print(results['results'].sort_values(by='pnl', ascending=True).head(5))\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è No trades executed, skipping PnL trade breakdown.\")\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame([{\n",
    "    'pnl': r['pnl'],\n",
    "    'sharpe': r['sharpe'],\n",
    "    'expectancy': r['expectancy'],\n",
    "    'profit_factor': r['profit_factor'],\n",
    "    'win_rate': r['win_rate'],\n",
    "    'trades': r['trades'],\n",
    "    **r['params']\n",
    "} for r in all_results])\n",
    "top = summary_df.sort_values(by='sharpe', ascending=False).head(10)\n",
    "print(\"\\nüèÅ Top 10 Configurations Across All Lookaheads:\")\n",
    "print(top)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification StandAlone Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classification_results = []\n",
    "\n",
    "for result in class_results:\n",
    "    lookahead = result['lookahead']\n",
    "    preds_stack = result['preds_stack']  # or 'preds_cnn'\n",
    "    preds_xgboost = result['preds_xgboost']\n",
    "    X_test_combined = result['X_meta_test_combined']  # or 'X_test_seq'\n",
    "    y_test = result['true_values']\n",
    "    labeled = pd.read_parquet(f\"labeled_data_{lookahead}_session_less.parquet\")\n",
    "    df_backtest = labeled.copy()\n",
    "    le = result['label_encoder']\n",
    "\n",
    "    for params in combinations:\n",
    "        # X_test, preds_stack, preds_cnn, preds_lgbm, \n",
    "        results = evaluate_classification(\n",
    "            X_test=X_test_combined,\n",
    "            preds_stack=preds_stack,\n",
    "            preds_xgboost=preds_xgboost,\n",
    "            labeled=labeled,\n",
    "            df=df_backtest,\n",
    "            avoid_funcs=avoid_funcs,\n",
    "            le=le,\n",
    "            SL_ATR_MULT=params['SL_ATR_MULT'],\n",
    "            TP_ATR_MULT=params['TP_ATR_MULT'],\n",
    "            TRAIL_START_MULT=params['TRAIL_START_MULT'],\n",
    "            TRAIL_STOP_MULT=params['TRAIL_STOP_MULT'],\n",
    "            TICK_VALUE=params['TICK_VALUE'],\n",
    "            is_same_session=is_same_session,\n",
    "            base_contracts=1,\n",
    "            max_contracts=5,\n",
    "            skip_weak_conf=True,\n",
    "            weak_conf_zscore=0.2\n",
    "        )\n",
    "\n",
    "        results['params'] = params\n",
    "        all_classification_results.append(results)\n",
    "\n",
    "        print(f\"\\n\\nüîç Evaluating with params: {params}\")\n",
    "\n",
    "        print(\n",
    "            f\"\\n‚úÖ LOOKAHEAD={lookahead} | Threshold={thresh}\"\n",
    "            f\"\\nPnL: ${results['pnl']:.2f}\"\n",
    "            f\"\\nTrades: {results['trades']}\"\n",
    "            f\"\\nWin Rate: {results['win_rate']:.2%}\"\n",
    "            f\"\\nExpectancy: {results['expectancy']:.2f}\"\n",
    "            f\"\\nProfit Factor: {results['profit_factor']:.2f}\"\n",
    "            f\"\\nSharpe Ratio: {results['sharpe']:.2f}\"\n",
    "            f\"\\nLong Trades: {results['long_trades']} | Short Trades: {results['short_trades']}\"\n",
    "        )\n",
    "\n",
    "        print(\"Avoid Hits:\")\n",
    "        for name, count in results['avoid_hits'].items():\n",
    "            print(f\" - {name}: {count}\")\n",
    "\n",
    "        if not results['results'].empty and 'pnl' in results['results'].columns:\n",
    "            print(\"\\nüî¢ Top 5 PnL trades:\")\n",
    "            print(results['results'].sort_values(by='pnl', ascending=False).head(5))\n",
    "\n",
    "            print(\"\\nüîª Bottom 5 PnL trades:\")\n",
    "            print(results['results'].sort_values(by='pnl', ascending=True).head(5))\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è No trades executed, skipping PnL trade breakdown.\")\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame([{\n",
    "    'pnl': r['pnl'],\n",
    "    'sharpe': r['sharpe'],\n",
    "    'expectancy': r['expectancy'],\n",
    "    'profit_factor': r['profit_factor'],\n",
    "    'win_rate': r['win_rate'],\n",
    "    'trades': r['trades'],\n",
    "    **r['params']\n",
    "} for r in all_results])\n",
    "top = summary_df.sort_values(by='sharpe', ascending=False).head(10)\n",
    "print(\"\\nüèÅ Top 10 Configurations Across All Lookaheads:\")\n",
    "print(top)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combo Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combo_results = []\n",
    "thresholds = [0.0000035]\n",
    "\n",
    "for reg_result, class_result in zip(reg_results, class_results):\n",
    "    lookahead = reg_result['lookahead']\n",
    "    preds_stack_reg = reg_result['preds_stack']\n",
    "    preds_cnn_reg = reg_result['preds_cnn']\n",
    "    X_test_combo = reg_result['X_test_meta']\n",
    "    y_test = reg_result['true_values']\n",
    "\n",
    "    preds_stack_class = class_result['preds_stack']  # shape (n_samples, n_classes)\n",
    "    preds_xgboost_class = class_result['preds_xgboost']\n",
    "    le = class_result['label_encoder']\n",
    "    labeled = pd.read_parquet(f\"labeled_data_{lookahead}_session_less.parquet\")\n",
    "    df_backtest = labeled.copy()\n",
    "\n",
    "    print(f\"\\nüîé LOOKAHEAD={lookahead} Regression Prediction Range:\")\n",
    "    print(f\"Stack Min: {np.min(preds_stack_reg):.6f} | Max: {np.max(preds_stack_reg):.6f}\")\n",
    "    print(f\"CNN   Min: {np.min(preds_cnn_reg):.6f} | Max: {np.max(preds_cnn_reg):.6f}\")\n",
    "    print(f\"Classification Unique Labels: {np.unique(le.inverse_transform(np.argmax(preds_stack_class, axis=1)))}\")\n",
    "\n",
    "    # ‚úÖ Align classifier and regression predictions to common length\n",
    "    min_len = min(len(X_test_combo), len(preds_stack_class), len(preds_xgboost_class), len(preds_stack_reg), len(preds_cnn_reg))\n",
    "    X_test_trimmed = X_test_combo.iloc[:min_len]\n",
    "    probs_stack_aligned = np.array(preds_stack_class)[:min_len]\n",
    "    probs_xgb_aligned = np.array(preds_xgboost_class)[:min_len]\n",
    "    preds_stack_reg = np.array(preds_stack_reg)[:min_len]\n",
    "    preds_cnn_reg = np.array(preds_cnn_reg)[:min_len]\n",
    "\n",
    "    for params in combinations:\n",
    "        for thresh in thresholds:\n",
    "            results = evaluate_combo(\n",
    "                X_test=X_test_trimmed,\n",
    "                preds_reg_stack=preds_stack_reg,\n",
    "                preds_reg_cnn=preds_cnn_reg,\n",
    "                le=le,\n",
    "                probs_class_stack=probs_stack_aligned,\n",
    "                probs_class_xgboost=probs_xgb_aligned,\n",
    "                labeled=labeled,\n",
    "                df=df_backtest,\n",
    "                avoid_funcs=avoid_funcs,\n",
    "                SL_ATR_MULT=params['SL_ATR_MULT'],\n",
    "                TP_ATR_MULT=params['TP_ATR_MULT'],\n",
    "                TRAIL_START_MULT=params['TRAIL_START_MULT'],\n",
    "                TRAIL_STOP_MULT=params['TRAIL_STOP_MULT'],\n",
    "                TICK_VALUE=params['TICK_VALUE'],\n",
    "                is_same_session=is_same_session,\n",
    "                long_threshold=thresh,\n",
    "                short_threshold=-thresh,\n",
    "                base_contracts=1,\n",
    "                max_contracts=5,\n",
    "                skip_weak_conf=True,\n",
    "                weak_conf_zscore=0.2\n",
    "            )\n",
    "\n",
    "            results['params'] = params\n",
    "            results['threshold'] = thresh\n",
    "            results['lookahead'] = lookahead\n",
    "            all_combo_results.append(results)\n",
    "\n",
    "            print(f\"\\n‚úÖ LOOKAHEAD={lookahead} | Threshold={thresh}\")\n",
    "            print(f\"PnL: ${results['pnl']:.2f}\")\n",
    "            print(f\"Trades: {results['trades']}\")\n",
    "            print(f\"Win Rate: {results['win_rate']:.2%}\")\n",
    "            print(f\"Expectancy: {results['expectancy']:.2f}\")\n",
    "            print(f\"Profit Factor: {results['profit_factor']:.2f}\")\n",
    "            print(f\"Sharpe Ratio: {results['sharpe']:.2f}\")\n",
    "            print(f\"Long Trades: {results['long_trades']} | Short Trades: {results['short_trades']}\")\n",
    "\n",
    "            print(\"Avoid Hits:\")\n",
    "            for name, count in results['avoid_hits'].items():\n",
    "                print(f\" - {name}: {count}\")\n",
    "\n",
    "            if not results['results'].empty and 'pnl' in results['results'].columns:\n",
    "                print(\"\\nüî¢ Top 5 PnL trades:\")\n",
    "                print(results['results'].sort_values(by='pnl', ascending=False).head(5))\n",
    "\n",
    "                print(\"\\nüîª Bottom 5 PnL trades:\")\n",
    "                print(results['results'].sort_values(by='pnl', ascending=True).head(5))\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è No trades executed, skipping PnL trade breakdown.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
