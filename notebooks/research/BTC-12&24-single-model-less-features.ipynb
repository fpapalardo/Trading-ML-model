{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T23:55:16.424411Z",
     "start_time": "2025-06-03T23:55:16.416845Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import platform\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta, datetime\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "from scipy.stats.mstats import winsorize\n",
    "import numba\n",
    "import pandas_ta as pta\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add one directory up to sys.path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Internal Libraries\n",
    "from data_loader import load_and_resample_data, apply_feature_engineering\n",
    "from backtest import evaluate_regression, evaluate_static_tp_two_contracts\n",
    "from labeling_utils import label_and_save\n",
    "from helpers import check_overfit, generate_oof_predictions, is_same_session, generate_oof_lstm_classifier\n",
    "#\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import Huber, MeanSquaredError\n",
    "import tensorflow as tf\n",
    "#\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.base import clone, BaseEstimator, RegressorMixin, ClassifierMixin\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, cross_val_predict\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, StackingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, ElasticNet, Ridge, ElasticNetCV, RidgeCV\n",
    "from sklearn.metrics import classification_report, root_mean_squared_error, mean_squared_error, mean_absolute_error, r2_score, \\\n",
    "    confusion_matrix, precision_recall_curve, roc_curve, auc, accuracy_score, classification_report, f1_score, precision_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import label_binarize, StandardScaler, LabelEncoder\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n",
    "\n",
    "# Models and Training\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.svm import SVC\n",
    "#\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*There are no meaningful features.*\", category=UserWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "market = \"BTC\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize features or indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T23:55:16.521479Z",
     "start_time": "2025-06-03T23:55:16.518032Z"
    }
   },
   "outputs": [],
   "source": [
    "avoid_funcs = {\n",
    "    #'avoid_hour_18_19': avoid_hour_18_19\n",
    "    #'news_window': avoid_news,\n",
    "}\n",
    "\n",
    "param_grid_strategy = {\n",
    "    'SL_ATR_MULT': [1.0, 1.5, 2.0],  # Wider stops\n",
    "    'TP_ATR_MULT': [3.0, 4.0, 5.0, 8.0],   # More conservative targets\n",
    "    'TRAIL_START_MULT': [1.0, 1.5],    # Let winners run\n",
    "    'TRAIL_STOP_MULT': [0.8, 1.0],     # Tighter trailing stops\n",
    "    'TICK_VALUE': [20],\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid_strategy.items())\n",
    "combinations = [dict(zip(keys, v)) for v in product(*values)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T23:55:25.118760Z",
     "start_time": "2025-06-03T23:55:16.558279Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1min, resampled = load_and_resample_data(market, timeframes=['5min', '15min','1h'])\n",
    "\n",
    "df_5min = resampled['5min']\n",
    "df_15min = resampled['15min']\n",
    "df_1hr = resampled['1h']\n",
    "\n",
    "print(\"5min shape:\", df_5min.shape)\n",
    "print(\"15min shape:\", df_15min.shape)\n",
    "print(\"1h shape:\", df_1hr.shape)\n",
    "\n",
    "\n",
    "df_merged = apply_feature_engineering(\n",
    "    resampled=resampled,\n",
    "    add_all_features=add_all_features,\n",
    "    add_time_session_features=add_time_session_features,\n",
    "    #add_trend_features=add_trend_features,  # Pass your trend features function here\n",
    "    timeframes=['5min', '15min', '1h'],\n",
    "    base_tf='5min'\n",
    ")\n",
    "\n",
    "# Add this code right after the feature engineering but before model fitting\n",
    "\n",
    "print(f\"Original merged dataset shape: {df_merged.shape}\")\n",
    "\n",
    "# Count rows with at least one NaN\n",
    "nan_rows_count = df_merged.isna().any(axis=1).sum()\n",
    "total_rows = len(df_merged)\n",
    "print(f\"Rows with at least one NaN: {nan_rows_count} out of {total_rows} ({(nan_rows_count/total_rows)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# Verify Data\n",
    "# def check_feature_alignment(df_merged, tf_suffix, feature_keyword, sample_times):\n",
    "#     \"\"\"\n",
    "#     Check if features from the correct timeframe are available and what values they have at key timestamps.\n",
    "#     \"\"\"\n",
    "#     suffix_str = f\"_{tf_suffix}\"\n",
    "#     matching_cols = [col for col in df_merged.columns if feature_keyword in col and col.endswith(suffix_str)]\n",
    "    \n",
    "#     print(f\"\\nüîç Checking features containing '{feature_keyword}' with suffix '{suffix_str}'\")\n",
    "\n",
    "#     if not matching_cols:\n",
    "#         print(f\"‚ö†Ô∏è No matching columns found. Available columns ending with {suffix_str}:\")\n",
    "#         sample_suffix_cols = [col for col in df_merged.columns if col.endswith(suffix_str)]\n",
    "#         print(sample_suffix_cols[:10])  # Show only first 10 to keep it clean\n",
    "#         return\n",
    "\n",
    "#     for time in sample_times:\n",
    "#         if time not in df_merged.index:\n",
    "#             print(f\"‚ùå Time {time} not found in df_merged index.\")\n",
    "#         else:\n",
    "#             print(f\"\\n‚è∞ At time {time} ‚Äî values:\")\n",
    "#             print(df_merged.loc[time, matching_cols])\n",
    "\n",
    "# sample_times = [\n",
    "#     pd.Timestamp(\"2022-01-05 12:05:00-05:00\"),\n",
    "#     pd.Timestamp(\"2022-01-05 12:15:00-05:00\"),\n",
    "#     pd.Timestamp(\"2022-01-05 12:59:00-05:00\"),\n",
    "#     pd.Timestamp(\"2022-01-05 13:00:00-05:00\")\n",
    "# ]\n",
    "\n",
    "# check_feature_alignment(df_merged, tf_suffix=\"1h\", feature_keyword=\"RSI\", sample_times=sample_times)\n",
    "# check_feature_alignment(df_merged, tf_suffix=\"15min\", feature_keyword=\"MACD\", sample_times=sample_times)\n",
    "# check_feature_alignment(df_merged, tf_suffix=\"5min\", feature_keyword=\"EMA\", sample_times=sample_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T23:55:28.681923Z",
     "start_time": "2025-06-03T23:55:25.163561Z"
    }
   },
   "outputs": [],
   "source": [
    "lookahead_options = [12, 6, 3]\n",
    "for lookahead in lookahead_options:\n",
    "    labeled = label_and_save(\n",
    "        df_input_features=df_merged,\n",
    "        lookahead_period=lookahead,\n",
    "        vol_col_name='ATR_14_5min',\n",
    "        pt_multiplier=2.0,\n",
    "        sl_multiplier=1.0,\n",
    "        min_return_percentage=0.0005,\n",
    "        output_file_suffix=f'{lookahead}{market}',\n",
    "        feature_columns_for_dropna=[]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T23:55:28.724938Z",
     "start_time": "2025-06-03T23:55:28.718184Z"
    }
   },
   "outputs": [],
   "source": [
    "# selected_indicators_class = [\n",
    "#     \"Session_5min\", \"Time_of_Day_5min\",\n",
    "#         \"Open_of_Day_5min\", \"High_of_Day_5min\", \"Low_of_Day_5min\",\n",
    "#         \"Prev_High_5min\", \"Prev_Low_5min\",\n",
    "#         \"Price_vs_Open_5min\", \"Price_vs_Session_High_Low_5min\",\n",
    "#         \"Volume_Spike_5min\", \"CVD_3_5min\",\n",
    "#         \"Stop_Hunt_5min\", \"Bull_Engulfing_5min\", \"Bear_Engulfing_5min\",\n",
    "#         \"FVG_Exists_5min\"\n",
    "# ]\n",
    "\n",
    "selected_indicators_class = [ # PRETTY GOOD RESULTS\n",
    "    'POC_Dist_Current_Points_1h', 'POC_Dist_Current_Points_5min', 'Day_of_Week', \n",
    "    'POC_Dist_Current_Points_15min', 'Day_Sin', 'RSI_7_5min', 'Minus_DI_14_1h', \n",
    "    'Trend_Score_15min', 'Trend_Strength_5min', 'Prev_Swing_Dist_15min', 'Time_Sin', \n",
    "    'Volume_Trend_15min', 'Is_Trending_5min', 'Is_Trending_15min'\n",
    "    ]\n",
    "\n",
    "selected_indicators_reg = ['open', 'high', 'low', 'close', 'volume', 'Volume_SMA_20_5min', 'BBB_20_2.0_5min', 'BBP_20_2.0_5min', 'close_vs_BB_Upper_5min', 'close_vs_BB_Lower_5min', 'ATR_14_5min', 'EMA_21_Slope_21_3_5min', 'MACDh_12_26_9_5min', 'MACD_12_26_9_Cross_Signal_5min', 'ADX_14_5min', 'Plus_DI_14_5min', 'Minus_DI_14_5min', 'RSI_14_5min', 'RSI_7_Is_Overbought_70_5min', 'RSI_7_Is_Oversold_30_5min', 'RSI_14_Is_Overbought_70_5min', 'RSI_14_Is_Oversold_30_5min', 'CHOP_14_1_100_5min', 'CHOP_7_1_100_5min', 'Is_Choppy_14_5min', 'Is_Choppy_7_5min', 'STOCHk_14_3_3_Is_Overbought_80_5min', 'STOCHk_14_3_3_Is_Oversold_20_5min', 'PPO_12_26_9_5min', 'ROC_10_5min', 'Candle_Range_5min', 'Candle_Body_5min', 'Upper_Wick_5min', 'Lower_Wick_5min', 'Body_vs_Range_5min', 'CDL_DOJI_10_0.1_5min', 'CDL_HAMMER_5min', 'CDL_ENGULFING_5min', 'Rolling_Skew_30_5min', 'Rolling_Kurtosis_30_5min', 'Candle_Body_Lag_1_5min', 'Candle_Body_Lag_2_5min', 'Candle_Body_Lag_3_5min', 'Is_Trending_5min', 'Is_Choppy_5min', 'Is_High_Vol_5min', 'cum_tpv_5min', 'VWAP_Session_Dist_5min', 'Vol_Delta_1_5min', 'Vol_Delta_2_5min', 'Vol_Delta_3_5min', 'Vol_zscore_20_5min', 'High_Vol_Event_20_5min', 'Vol_zscore_10_5min', 'High_Vol_Event_10_5min', 'Vol_zscore_5_5min', 'POC_Dist_Current_Points_5min', 'POC_Dist_Previous_Points_5min', 'Close_Pos_%_5min', 'Rel_Vol_20_5min', 'CVD_3_5min', 'Upper_Wick_%_5min', 'Lower_Wick_%_5min', 'Trend_Direction_5min', 'Trend_Strength_5min', 'Vol_Regime_5min', 'Volume_Trend_5min', 'Trend_Alignment_5min', 'Mean_Reversion_5min', 'Trend_Score_5min', 'Hour_of_Day', 'Minute_of_Hour', 'Day_of_Week', 'Time_Sin', 'Time_Cos', 'Day_Sin', 'Day_Cos', 'Is_Asian_Session', 'Is_London_Session', 'Is_NY_Session', 'Is_Overlap', 'Is_US_Open_Hour', 'Is_US_Close_Hour', 'Volume_SMA_20_15min', 'BBB_20_2.0_15min', 'BBP_20_2.0_15min', 'close_vs_BB_Upper_15min', 'close_vs_BB_Lower_15min', 'ATR_14_15min', 'MACDh_12_26_9_15min', 'MACD_12_26_9_Cross_Signal_15min', 'ADX_14_15min', 'Plus_DI_14_15min', 'Minus_DI_14_15min', 'RSI_14_15min', 'RSI_7_Is_Overbought_70_15min', 'RSI_7_Is_Oversold_30_15min', 'RSI_14_Is_Overbought_70_15min', 'RSI_14_Is_Oversold_30_15min', 'CHOP_14_1_100_15min', 'CHOP_7_1_100_15min', 'Is_Choppy_14_15min', 'Is_Choppy_7_15min', 'STOCHk_14_3_3_Is_Overbought_80_15min', 'STOCHk_14_3_3_Is_Oversold_20_15min', 'PPO_12_26_9_15min', 'ROC_10_15min', 'Candle_Range_15min', 'Candle_Body_15min', 'Upper_Wick_15min', 'Lower_Wick_15min', 'Body_vs_Range_15min', 'CDL_DOJI_10_0.1_15min', 'CDL_HAMMER_15min', 'CDL_ENGULFING_15min', 'Rolling_Skew_30_15min', 'Rolling_Kurtosis_30_15min', 'Candle_Body_Lag_1_15min', 'Candle_Body_Lag_2_15min', 'Candle_Body_Lag_3_15min', 'Is_Trending_15min', 'Is_Choppy_15min', 'Is_High_Vol_15min', 'TPV_15min', 'VWAP_Session_Dist_15min', 'Vol_Delta_1_15min', 'Vol_Delta_2_15min', 'Vol_Delta_3_15min', 'Vol_zscore_20_15min', 'High_Vol_Event_20_15min', 'Vol_zscore_10_15min', 'High_Vol_Event_10_15min', 'Vol_zscore_5_15min', 'POC_Dist_Current_Points_15min', 'POC_Dist_Previous_Points_15min', 'Close_Pos_%_15min', 'Rel_Vol_20_15min', 'CVD_3_15min', 'Upper_Wick_%_15min', 'Lower_Wick_%_15min', 'Trend_Direction_15min', 'Trend_Strength_15min', 'Vol_Regime_15min', 'Volume_Trend_15min', 'Trend_Alignment_15min', 'Trend_Score_15min', 'Volume_SMA_20_1h', 'BBB_20_2.0_1h', 'BBP_20_2.0_1h', 'close_vs_BB_Upper_1h', 'close_vs_BB_Lower_1h', 'ATR_14_1h', 'MACDh_12_26_9_1h', 'MACD_12_26_9_Cross_Signal_1h', 'ADX_14_1h', 'Plus_DI_14_1h', 'Minus_DI_14_1h', 'RSI_14_1h', 'RSI_7_Is_Overbought_70_1h', 'RSI_7_Is_Oversold_30_1h', 'RSI_14_Is_Overbought_70_1h', 'RSI_14_Is_Oversold_30_1h', 'CHOP_14_1_100_1h', 'CHOP_7_1_100_1h', 'Is_Choppy_14_1h', 'Is_Choppy_7_1h', 'STOCHk_14_3_3_Is_Overbought_80_1h', 'STOCHk_14_3_3_Is_Oversold_20_1h', 'PPO_12_26_9_1h', 'PPOh_12_26_9_1h', 'ROC_10_1h', 'Candle_Range_1h', 'Candle_Body_1h', 'Upper_Wick_1h', 'Lower_Wick_1h', 'Body_vs_Range_1h', 'CDL_DOJI_10_0.1_1h', 'CDL_HAMMER_1h', 'CDL_ENGULFING_1h', 'Rolling_Std_Dev_14_1h', 'Rolling_Skew_30_1h', 'Rolling_Kurtosis_30_1h', 'Candle_Body_Lag_1_1h', 'Candle_Body_Lag_2_1h', 'Candle_Body_Lag_3_1h', 'Is_Trending_1h', 'Is_Choppy_1h', 'Is_High_Vol_1h', 'cum_tpv_1h', 'VWAP_Session_Dist_1h', 'Vol_Delta_1_1h', 'Vol_Delta_2_1h', 'Vol_Delta_3_1h', 'Vol_zscore_20_1h', 'High_Vol_Event_20_1h', 'Vol_zscore_10_1h', 'High_Vol_Event_10_1h', 'Vol_zscore_5_1h', 'POC_Dist_Current_Points_1h', 'POC_Dist_Previous_Points_1h', 'Close_Pos_%_1h', 'CVD_3_1h', 'Upper_Wick_%_1h', 'Lower_Wick_%_1h', 'Trend_Direction_1h', 'Trend_Strength_1h', 'Vol_Regime_1h', 'Volume_Trend_1h', 'Trend_Alignment_1h', 'Trend_Score_1h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1DWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_shape, filters=64, kernel_size=3, dropout=0.2, learning_rate=0.001):\n",
    "        self.input_shape = input_shape\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential([\n",
    "            Conv1D(self.filters, kernel_size=self.kernel_size, activation='relu', input_shape=(self.input_shape, 1)),\n",
    "            Dropout(self.dropout),\n",
    "            Conv1D(self.filters * 2, kernel_size=self.kernel_size, activation='relu'),\n",
    "            Dropout(self.dropout),\n",
    "            GlobalAveragePooling1D(),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        self.model = self.build_model()\n",
    "        self.model.fit(\n",
    "            X_reshaped, y,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "            ],\n",
    "            verbose=0\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        return self.model.predict(X_reshaped).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T00:54:25.841256Z",
     "start_time": "2025-06-04T00:54:25.810221Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_lookahead_for_session_regression():\n",
    "    # === Load data ===\n",
    "    path = f\"parquet/labeled_data_6NQ.parquet\"\n",
    "    labeled = pd.read_parquet(path)\n",
    "\n",
    "    # === Ensure datetime column exists and is parsed ===\n",
    "    if labeled.index.name == 'datetime' or pd.api.types.is_datetime64_any_dtype(labeled.index):\n",
    "        labeled = labeled.reset_index()\n",
    "    if 'datetime' not in labeled.columns:\n",
    "        raise KeyError(\"‚ùå 'datetime' column is missing.\")\n",
    "\n",
    "    labeled['datetime'] = pd.to_datetime(labeled['datetime'])\n",
    "    labeled = labeled.sort_values('datetime')\n",
    "\n",
    "    # === Train/test split ===\n",
    "    cutoff_date = pd.Timestamp(\"2025-05-01\", tz=\"America/New_York\")\n",
    "    train = labeled[labeled['datetime'] < cutoff_date]\n",
    "    test = labeled[labeled['datetime'] >= cutoff_date]\n",
    "\n",
    "    train = train.set_index('datetime')\n",
    "    test = test.set_index('datetime')\n",
    "\n",
    "    # === Feature selection ===\n",
    "    X_train = train[selected_indicators_reg]\n",
    "    X_test = test[selected_indicators_reg]\n",
    "\n",
    "    # === Find regression target column ===\n",
    "    reg_cols = [col for col in labeled.columns if col.startswith(\"reg_\")]\n",
    "    if not reg_cols:\n",
    "        raise ValueError(\"‚ùå No regression target column found starting with 'reg_'.\")\n",
    "    reg_col = reg_cols[0]\n",
    "    print(f\"üìå Using regression target column: {reg_col}\")\n",
    "\n",
    "    y_train_seq = train[reg_col]\n",
    "    y_test_seq = test[reg_col]\n",
    "\n",
    "    print(f\"Train range: {train.index.min()} to {train.index.max()} | Rows: {len(train)}\")\n",
    "    print(f\"Test range: {test.index.min()} to {test.index.max()} | Rows: {len(test)}\")\n",
    "\n",
    "    ###########################\n",
    "    ########## Models #########\n",
    "    ###########################    \n",
    "    def tune_catboost_regressor(X, y, df, study_name=\"catboost-default\", db_path=\"dbs/catboost.db\", n_trials=10, threshold_points=10):\n",
    "        def objective(trial):\n",
    "            # 1) Suggest a set of CatBoost hyperparameters:\n",
    "            params = {\n",
    "                \"iterations\"       : trial.suggest_int(\"iterations\", 100, 5000, step=100),\n",
    "                \"depth\"            : trial.suggest_int(\"depth\", 4, 10),\n",
    "                \"learning_rate\"    : trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"l2_leaf_reg\"      : trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "                \"random_strength\"  : trial.suggest_float(\"random_strength\", 0.5, 5.0),\n",
    "                \"min_data_in_leaf\" : trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n",
    "                \"loss_function\"    : \"RMSE\",   # We will still optimize Sharpe, not RMSE directly.\n",
    "                \"verbose\"          : 0,\n",
    "                \"random_state\"     : 42\n",
    "            }\n",
    "\n",
    "            fold_scores = []\n",
    "            tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "            for train_idx, val_idx in tscv.split(X):\n",
    "                X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                prices_val   = df.iloc[val_idx][\"open\"].to_numpy()  # entry prices\n",
    "\n",
    "                # build sample weights that up‚Äêweight negative examples if you like:\n",
    "                w_tr = np.ones(len(train_idx))\n",
    "                # (optional) w_tr[y_tr<0] *= (1 + 10*abs(y_tr[y_tr<0]))\n",
    "\n",
    "                # train\n",
    "                model = CatBoostRegressor(**params)\n",
    "                model.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "\n",
    "                # predict\n",
    "                preds = model.predict(X_val)\n",
    "                actual = y_val.to_numpy()\n",
    "\n",
    "                # compute direction‚Äêaware 10-point threshold in pct\n",
    "                thr_pct = threshold_points / prices_val\n",
    "\n",
    "                # classify\n",
    "                long_pred =  preds >=  thr_pct\n",
    "                long_true =  actual >=  thr_pct\n",
    "                short_pred = preds <= -thr_pct\n",
    "                short_true = actual <= -thr_pct\n",
    "\n",
    "                correct = np.sum(long_pred & long_true) + np.sum(short_pred & short_true)\n",
    "                wrong   = np.sum(long_pred & ~long_true) + np.sum(short_pred & ~short_true)\n",
    "\n",
    "                fold_scores.append(correct - wrong)\n",
    "\n",
    "            # average net‚Äêcorrectness across folds\n",
    "            return float(np.mean(fold_scores))\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=study_name,\n",
    "            storage=f\"sqlite:///{db_path}\",\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=n_trials, n_jobs=15)\n",
    "\n",
    "        # pull out only the CatBoost init args\n",
    "        all_params = study.best_trial.params\n",
    "        valid_keys = {\"iterations\",\"depth\",\"learning_rate\",\"l2_leaf_reg\",\"random_strength\",\"min_data_in_leaf\",\"loss_function\"}\n",
    "        best_model_params = {k: all_params[k] for k in all_params if k in valid_keys}\n",
    "        return best_model_params\n",
    "    \n",
    "    def tune_xgbm_regressor(X, y, df,study_name=\"xgbm-default\", db_path=\"dbs/xgbm.db\", n_trials=10, threshold_points=10):\n",
    "        def objective(trial):\n",
    "            # 1) Suggest a set of CatBoost hyperparameters:\n",
    "            params = {\n",
    "                \"objective\":        \"reg:squarederror\",    # we‚Äôll still optimize Sharpe, not RMSE directly\n",
    "                \"n_estimators\":     trial.suggest_int(\"n_estimators\",     100,  5000, step=50),\n",
    "                \"learning_rate\":    trial.suggest_float(\"learning_rate\",  0.01, 0.3,  log=True),\n",
    "                \"max_depth\":        trial.suggest_int(\"max_depth\",        3,    12),\n",
    "                \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1,    10),\n",
    "                \"subsample\":        trial.suggest_float(\"subsample\",       0.6,  1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.6,  1.0),\n",
    "                \"gamma\":            trial.suggest_float(\"gamma\",           0.0,  0.1),\n",
    "                \"reg_alpha\":        trial.suggest_float(\"reg_alpha\",       0.0,  1.0),\n",
    "                \"reg_lambda\":       trial.suggest_float(\"reg_lambda\",      0.0,  1.0),\n",
    "                \"random_state\":     42,\n",
    "                \"verbosity\":        0          # silent\n",
    "            }\n",
    "\n",
    "            fold_scores = []\n",
    "            tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "            for train_idx, val_idx in tscv.split(X):\n",
    "                X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                prices_val   = df.iloc[val_idx][\"open\"].to_numpy()  # entry prices\n",
    "\n",
    "                # build sample weights that up‚Äêweight negative examples if you like:\n",
    "                w_tr = np.ones(len(train_idx))\n",
    "                # (optional) w_tr[y_tr<0] *= (1 + 10*abs(y_tr[y_tr<0]))\n",
    "\n",
    "                # train\n",
    "                model = xgb.XGBRegressor(**params)\n",
    "                model.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "\n",
    "                # predict\n",
    "                preds = model.predict(X_val)\n",
    "                actual = y_val.to_numpy()\n",
    "\n",
    "                # compute direction‚Äêaware 10-point threshold in pct\n",
    "                thr_pct = threshold_points / prices_val\n",
    "\n",
    "                # classify\n",
    "                long_pred =  preds >=  thr_pct\n",
    "                long_true =  actual >=  thr_pct\n",
    "                short_pred = preds <= -thr_pct\n",
    "                short_true = actual <= -thr_pct\n",
    "\n",
    "                correct = np.sum(long_pred & long_true) + np.sum(short_pred & short_true)\n",
    "                wrong   = np.sum(long_pred & ~long_true) + np.sum(short_pred & ~short_true)\n",
    "\n",
    "                fold_scores.append(correct - wrong)\n",
    "\n",
    "            # average net‚Äêcorrectness across folds\n",
    "            return float(np.mean(fold_scores))\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=study_name,\n",
    "            storage=f\"sqlite:///{db_path}\",\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=n_trials, n_jobs=15)\n",
    "\n",
    "        # pull out only the CatBoost init args\n",
    "        all_params = study.best_trial.params\n",
    "        valid_keys = {\"iterations\",\"depth\",\"learning_rate\",\"l2_leaf_reg\",\"random_strength\",\"min_data_in_leaf\",\"loss_function\"}\n",
    "        best_model_params = {k: all_params[k] for k in all_params if k in valid_keys}\n",
    "        return best_model_params\n",
    "\n",
    "    def tune_lgbm_regressor(X, y, df,study_name=\"lgbm-default\", db_path=\"dbs/lgbm.db\", n_trials=10, threshold_points=10):\n",
    "        def objective(trial):\n",
    "            # 1) Suggest a set of CatBoost hyperparameters:\n",
    "            params = {\n",
    "                \"n_estimators\":       trial.suggest_int(\"n_estimators\",       100, 2000, step=50),\n",
    "                \"learning_rate\":      trial.suggest_float(\"learning_rate\",   0.01, 0.3, log=True),\n",
    "                \"max_depth\":          trial.suggest_int(\"max_depth\",           3,   12),\n",
    "                \"num_leaves\":         trial.suggest_int(\"num_leaves\",         20,  200),\n",
    "                \"min_child_samples\":  trial.suggest_int(\"min_child_samples\",   5,   50),\n",
    "                \"subsample\":          trial.suggest_float(\"subsample\",         0.6,  1.0),\n",
    "                \"colsample_bytree\":   trial.suggest_float(\"colsample_bytree\",  0.6,  1.0),\n",
    "                \"reg_alpha\":          trial.suggest_float(\"reg_alpha\",         0.0,  1.0),\n",
    "                \"reg_lambda\":         trial.suggest_float(\"reg_lambda\",         0.0,  1.0),\n",
    "                \"random_state\":     42,\n",
    "                \"verbosity\":        0          # silent\n",
    "            }\n",
    "\n",
    "            fold_scores = []\n",
    "            tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "            for train_idx, val_idx in tscv.split(X):\n",
    "                X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                prices_val   = df.iloc[val_idx][\"open\"].to_numpy()  # entry prices\n",
    "\n",
    "                # build sample weights that up‚Äêweight negative examples if you like:\n",
    "                w_tr = np.ones(len(train_idx))\n",
    "                # (optional) w_tr[y_tr<0] *= (1 + 10*abs(y_tr[y_tr<0]))\n",
    "\n",
    "                # train\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "\n",
    "                # predict\n",
    "                preds = model.predict(X_val)\n",
    "                actual = y_val.to_numpy()\n",
    "\n",
    "                # compute direction‚Äêaware 10-point threshold in pct\n",
    "                thr_pct = threshold_points / prices_val\n",
    "\n",
    "                # classify\n",
    "                long_pred =  preds >=  thr_pct\n",
    "                long_true =  actual >=  thr_pct\n",
    "                short_pred = preds <= -thr_pct\n",
    "                short_true = actual <= -thr_pct\n",
    "\n",
    "                correct = np.sum(long_pred & long_true) + np.sum(short_pred & short_true)\n",
    "                wrong   = np.sum(long_pred & ~long_true) + np.sum(short_pred & ~short_true)\n",
    "\n",
    "                fold_scores.append(correct - wrong)\n",
    "\n",
    "            # average net‚Äêcorrectness across folds\n",
    "            return float(np.mean(fold_scores))\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=study_name,\n",
    "            storage=f\"sqlite:///{db_path}\",\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=n_trials, n_jobs=15)\n",
    "\n",
    "        # pull out only the CatBoost init args\n",
    "        all_params = study.best_trial.params\n",
    "        valid_keys = {\"iterations\",\"depth\",\"learning_rate\",\"l2_leaf_reg\",\"random_strength\",\"min_data_in_leaf\",\"loss_function\"}\n",
    "        best_model_params = {k: all_params[k] for k in all_params if k in valid_keys}\n",
    "        return best_model_params\n",
    "    \n",
    "    ################################################\n",
    "    ####### Tune models\n",
    "    xgbm_params          = tune_xgbm_regressor(X_train, y_train_seq, train, study_name=f\"xgbm-{market}\")\n",
    "    catboost_params      = tune_catboost_regressor(X_train, y_train_seq, train, study_name=f\"catboost-{market}\")\n",
    "    lgbm_params          = tune_lgbm_regressor(X_train, y_train_seq, train, study_name=f\"lgbm-{market}\")\n",
    "\n",
    "    ################################################\n",
    "    ####### Train models\n",
    "    catboost    = CatBoostRegressor(**catboost_params, random_state=42, verbose=0)\n",
    "    xgbm        = xgb.XGBRegressor(**xgbm_params, random_state=42)\n",
    "    lgbm        = lgb.LGBMRegressor(**xgbm_params, random_state=42)\n",
    "\n",
    "    ################################################\n",
    "    ####### Base Stacks\n",
    "    catboost_models = [catboost]\n",
    "    catboost_oof = generate_oof_predictions(catboost_models, X_train, y_train_seq, splits=4)\n",
    "    # The returned DataFrame usually has column name [\"model_0\"] by default.\n",
    "    # Rename it to \"catboost_oof\" so that the same name can be used at test time:\n",
    "    catboost_oof = catboost_oof.rename(columns={\"model_0\": \"catboost_oof\"})\n",
    "\n",
    "    scaler_cb = StandardScaler()\n",
    "    catboost_oof_scaled = scaler_cb.fit_transform(catboost_oof)  \n",
    "    # Now the scaler knows there‚Äôs exactly one feature: \"catboost_oof\".\n",
    "\n",
    "    lgbm_models = [lgbm]\n",
    "    lgbm_oof = generate_oof_predictions(lgbm_models, X_train, y_train_seq, splits=4)\n",
    "    # The returned DataFrame usually has column name [\"model_0\"] by default.\n",
    "    # Rename it to \"catboost_oof\" so that the same name can be used at test time:\n",
    "    lgbm_oof = lgbm_oof.rename(columns={\"model_0\": \"lgbm_oof\"})\n",
    "\n",
    "    scaler_lgbm = StandardScaler()\n",
    "    lgbm_oof_scaled = scaler_lgbm.fit_transform(lgbm_oof)  \n",
    "\n",
    "    # (b) LightGBM OOF\n",
    "    xgbm_models = [xgbm]\n",
    "    xgbm_oof = generate_oof_predictions(xgbm_models, X_train, y_train_seq, splits=4)\n",
    "    xgbm_oof = xgbm_oof.rename(columns={\"model_0\": \"xgbm_oof\"})\n",
    "\n",
    "    scaler_xgb = StandardScaler()\n",
    "    xgbm_oof_scaled = scaler_xgb.fit_transform(xgbm_oof)\n",
    "    # The scaler now knows there‚Äôs exactly one feature: \"lgbm_oof\".\n",
    "\n",
    "\n",
    "    # 5) === Build your meta‚Äêtraining matrix ===\n",
    "    X_meta_train = pd.DataFrame({\n",
    "        \"catboost_oof\": catboost_oof_scaled.ravel(),   # flatten into 1D\n",
    "        \"xgbm_oof\":     xgbm_oof_scaled.ravel(),\n",
    "        \"lgbm_oof\":     lgbm_oof_scaled.ravel()\n",
    "    }, index=X_train.index)\n",
    "\n",
    "    alphas     = np.logspace(-4, 2, 40)\n",
    "    meta_model = RidgeCV(alphas=alphas)\n",
    "    meta_model.fit(X_meta_train, y_train_seq)\n",
    "\n",
    "\n",
    "    # 6) === Retrain base models on the full training set ===\n",
    "    for model in catboost_models + xgbm_models + lgbm_models:\n",
    "        model.fit(X_train, y_train_seq)\n",
    "\n",
    "\n",
    "    # 7) === At test time, get each base model‚Äôs raw prediction on X_test ===\n",
    "    #      Make sure to use the exact same column names as above!\n",
    "\n",
    "    # (a) CatBoost raw test‚Äêpred:\n",
    "    cat_preds = pd.DataFrame({\n",
    "        \"catboost_oof\": catboost_models[0].predict(X_test)\n",
    "    }, index=X_test.index)\n",
    "\n",
    "    # (b) LightGBM raw test‚Äêpred:\n",
    "    xgb_preds = pd.DataFrame({\n",
    "        \"xgbm_oof\": xgbm_models[0].predict(X_test)\n",
    "    }, index=X_test.index)\n",
    "\n",
    "    lgb_preds = pd.DataFrame({\n",
    "        \"lgbm_oof\": lgbm_models[0].predict(X_test)\n",
    "    }, index=X_test.index)\n",
    "\n",
    "\n",
    "    # 8) === Scale those test‚Äêtime columns with the *same* scalers ===\n",
    "    cat_preds_scaled = scaler_cb.transform(cat_preds)    # cat_preds has column \"catboost_oof\"\n",
    "    xgb_preds_scaled = scaler_xgb.transform(xgb_preds)  # lgb_preds has column \"lgbm_oof\"\n",
    "    lgb_preds_scaled = scaler_lgbm.transform(lgb_preds)\n",
    "\n",
    "    # 9) === Build the two‚Äêcolumn ‚Äúmeta‚Äù feature matrix for test ===\n",
    "    X_meta_test = pd.DataFrame({\n",
    "        \"catboost_oof\": cat_preds_scaled.ravel(),\n",
    "        \"xgbm_oof\":     xgb_preds_scaled.ravel(),\n",
    "        \"lgbm_oof\":     lgb_preds_scaled.ravel()\n",
    "    }, index=X_test.index)\n",
    "\n",
    "\n",
    "    # 10) === Final stacked prediction ===\n",
    "    preds_stack = meta_model.predict(X_meta_test)\n",
    "\n",
    "    corr = np.corrcoef(y_test_seq, preds_stack)[0, 1]\n",
    "    print(f\"Correlation with target: {corr:.4f}\")\n",
    "\n",
    "    ################################################\n",
    "    ####### Evaluate Model\n",
    "    def evaluate_model(name, model, Xtr, Xte, ytr, yte, transformed=False):\n",
    "        train_preds = model.predict(Xtr)\n",
    "        test_preds = model.predict(Xte)\n",
    "\n",
    "        if transformed:\n",
    "        # Inverse-transform predictions\n",
    "            train_preds = np.sign(train_preds) * (np.expm1(np.abs(train_preds)))\n",
    "            test_preds = np.sign(test_preds) * (np.expm1(np.abs(test_preds)))\n",
    "            ytr = np.sign(ytr) * (np.expm1(np.abs(ytr)))\n",
    "\n",
    "        train_mse = mean_squared_error(ytr, train_preds)\n",
    "        test_mse = mean_squared_error(yte, test_preds)\n",
    "        overfit_ratio = test_mse / train_mse if train_mse != 0 else float('inf')\n",
    "\n",
    "        print(f\"\\nüìä {name} Performance:\")\n",
    "        print(f\"Train MSE: {train_mse:.8f}\")\n",
    "        print(f\"Test MSE: {test_mse:.8f}\")\n",
    "        print(f\"Overfit ratio (Test / Train): {overfit_ratio:.2f}\")\n",
    "        if overfit_ratio > 1.5:\n",
    "            print(\"‚ö†Ô∏è Potential overfitting detected.\")\n",
    "        elif overfit_ratio < 0.7:\n",
    "            print(\"‚ö†Ô∏è Possibly underfitting.\")\n",
    "        else:\n",
    "            print(\"‚úÖ Generalization looks reasonable.\")\n",
    "        return test_preds\n",
    "\n",
    "    ####### Tree Based #######\n",
    "    print(\"\\nEvaluation CatBoost\")\n",
    "    preds_catboost  = evaluate_model(\"CatBoostRegressorUp\", catboost, X_train, X_test, y_train_seq, y_test_seq, transformed=False)\n",
    "\n",
    "    print(\"\\nEvaluation XGBM\")\n",
    "    preds_xgbm  = evaluate_model(\"XGBM\", xgbm, X_train, X_test, y_train_seq, y_test_seq, transformed=False)\n",
    "\n",
    "    print(\"\\nEvaluation LGBM\")\n",
    "    preds_lgbm  = evaluate_model(\"LGBM\", lgbm, X_train, X_test, y_train_seq, y_test_seq, transformed=False)\n",
    "\n",
    "    ################################################\n",
    "    ####### Target Distribution\n",
    "    print(\"\\nüîç Target distribution Seq:\")\n",
    "    print(y_train_seq.describe())\n",
    "\n",
    "    ################################################\n",
    "    ####### Choose final model\n",
    "    for pred in [preds_stack, preds_xgbm, preds_catboost, preds_lgbm]:\n",
    "        print(\"\\nüîç Checking prediction variance:\")\n",
    "        print(f\"Min: {pred.min():.8f}\")\n",
    "        print(f\"Max: {pred.max():.8f}\")\n",
    "        print(f\"Mean: {pred.mean():.8f}\")\n",
    "        print(f\"Std Dev: {pred.std():.8f}\")\n",
    "        print(f\"First 5 Predictions: {pred[:5]}\")\n",
    "\n",
    "        mae = mean_absolute_error(y_test_seq, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_seq, pred))\n",
    "        r2 = r2_score(y_test_seq, pred)\n",
    "\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"R¬≤: {r2:.4f}\")\n",
    "\n",
    "    # === Ensure datetime is preserved ===\n",
    "    X_test = X_test.copy()\n",
    "    X_test[\"datetime\"] = X_test.index\n",
    "\n",
    "    metadata = {\n",
    "        \"catboost_params\": catboost_params,\n",
    "        \"xgbm_params\": xgbm_params,\n",
    "        \"lgbm_params\": lgbm_params,\n",
    "    }\n",
    "    with open(f\"regression_metadata_{market}.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    joblib.dump(list(X_train.columns), f\"pkl/model_features_{market}-12combo.pkl\")\n",
    "    joblib.dump(meta_model, f\"pkl/stack_model_regression_{market}-12combo.pkl\")\n",
    "\n",
    "    return {\n",
    "        'preds_stack': preds_stack,\n",
    "        'X_test': X_test,\n",
    "        'true_values': y_test_seq\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lookahead_for_session_classification(LOOKAHEAD):\n",
    "    # === Load data ===\n",
    "    path = f\"parquet/labeled_data_{LOOKAHEAD}{market}.parquet\"\n",
    "    labeled = pd.read_parquet(path)\n",
    "\n",
    "    # === Ensure datetime column exists and is parsed ===\n",
    "    if labeled.index.name == 'datetime' or pd.api.types.is_datetime64_any_dtype(labeled.index):\n",
    "        labeled = labeled.reset_index()\n",
    "    if 'datetime' not in labeled.columns:\n",
    "        raise KeyError(\"‚ùå 'datetime' column is missing.\")\n",
    "\n",
    "    labeled['datetime'] = pd.to_datetime(labeled['datetime'])\n",
    "    labeled = labeled.sort_values('datetime')\n",
    "\n",
    "    # === Train/test split ===\n",
    "    cutoff_date = pd.Timestamp(\"2025-01-01\", tz=\"America/New_York\")\n",
    "    train = labeled[labeled['datetime'] < cutoff_date]\n",
    "    test = labeled[labeled['datetime'] >= cutoff_date]\n",
    "\n",
    "    train = train.set_index('datetime')\n",
    "    test = test.set_index('datetime')\n",
    "\n",
    "    # === Feature selection ===\n",
    "    X_train = train[selected_indicators_class]\n",
    "    # X_train = X_train.apply(lambda col: col.astype('category').cat.codes if col.dtypes == 'object' else col)\n",
    "    X_test = test[selected_indicators_class]\n",
    "    # X_test = X_test.apply(lambda col: col.astype('category').cat.codes if col.dtypes == 'object' else col)\n",
    "\n",
    "    # === Find regression target column ===\n",
    "    reg_cols = [col for col in labeled.columns if col.startswith(\"clf_target\")]\n",
    "    if not reg_cols:\n",
    "        raise ValueError(\"‚ùå No regression target column found starting with 'clf_target'.\")\n",
    "    reg_col = reg_cols[0]\n",
    "    print(f\"üìå Using regression target column: {reg_col}\")\n",
    "\n",
    "    y_train = train[reg_col]\n",
    "    y_test = test[reg_col]\n",
    "\n",
    "    print(f\"Train range: {train.index.min()} to {train.index.max()} | Rows: {len(train)}\")\n",
    "    print(f\"Test range: {test.index.min()} to {test.index.max()} | Rows: {len(test)}\")\n",
    "\n",
    "    ###########################\n",
    "    ########## Models #########\n",
    "    ###########################\n",
    "\n",
    "    def tune_xgboost(X_train, y_train):\n",
    "        def objective(trial):\n",
    "\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "                'eval_metric': 'logloss',\n",
    "                'objective': 'multi:softmax',  # or 'multi:softprob' if you need probabilities\n",
    "                'num_class': len(np.unique(y_train))\n",
    "            }\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=splits)\n",
    "            scores = []\n",
    "\n",
    "            for train_idx, val_idx in tscv.split(X_train):\n",
    "                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                sample_weights = compute_sample_weight(class_weight='balanced', y=y_tr)\n",
    "\n",
    "                model = XGBClassifier(**params, random_state=42, n_jobs=-1)\n",
    "                model.fit(X_tr, y_tr, sample_weight=sample_weights)\n",
    "\n",
    "                preds = model.predict(X_val)\n",
    "                score = f1_score(y_val, preds, average='macro')\n",
    "                scores.append(score)\n",
    "\n",
    "            print(f\"Trial {trial.number} F1 Score: {np.mean(scores):.5f} | Params: {params}\")\n",
    "            return np.mean(scores)\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name=f'xgb_opt_class_{LOOKAHEAD}',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "            storage=f'sqlite:///xgb_opt_study_session_less.db',\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=2)\n",
    "        return study.best_params\n",
    "    \n",
    "    def tune_lgbm(X_train, y_train,\n",
    "        splits=4,\n",
    "        n_trials=10,\n",
    "        study_name=f\"lgbm_opt_class_{market}\",\n",
    "        db_path=\"sqlite:///lgbm.db\"):\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                \"n_estimators\":        trial.suggest_int(\"n_estimators\",        100, 3000, step=100),\n",
    "                \"learning_rate\":       trial.suggest_float(\"learning_rate\",    0.01, 0.3, log=True),\n",
    "                \"max_depth\":           trial.suggest_int(\"max_depth\",            3,  12),\n",
    "                \"num_leaves\":          trial.suggest_int(\"num_leaves\",         20, 256),\n",
    "                \"min_child_samples\":   trial.suggest_int(\"min_child_samples\",    5, 100),\n",
    "                \"subsample\":           trial.suggest_float(\"subsample\",          0.5, 1.0),\n",
    "                \"colsample_bytree\":    trial.suggest_float(\"colsample_bytree\",   0.5, 1.0),\n",
    "                \"reg_alpha\":           trial.suggest_float(\"reg_alpha\",          0.0, 5.0),\n",
    "                \"reg_lambda\":          trial.suggest_float(\"reg_lambda\",          0.0, 5.0),\n",
    "                \"min_split_gain\":      trial.suggest_float(\"min_split_gain\",     0.0, 1.0),\n",
    "                \"objective\":           \"multiclass\",\n",
    "                \"num_class\":           len(np.unique(y_train)),\n",
    "                \"random_state\":        42,\n",
    "                \"n_jobs\":              -1,\n",
    "                \"verbosity\":           -1\n",
    "            }\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=splits)\n",
    "            scores = []\n",
    "\n",
    "            for train_idx, val_idx in tscv.split(X_train):\n",
    "                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                sample_weights = compute_sample_weight(class_weight='balanced', y=y_tr)\n",
    "\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_tr, y_tr, sample_weight=sample_weights)\n",
    "\n",
    "                preds = model.predict(X_val)\n",
    "                score = f1_score(y_val, preds, average='macro')\n",
    "                scores.append(score)\n",
    "\n",
    "            print(f\"Trial {trial.number} F1 Score: {np.mean(scores):.5f} | Params: {params}\")\n",
    "            return np.mean(scores)\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=study_name,\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "            storage=db_path,\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "        return study.best_params\n",
    "\n",
    "    def tune_rf(X_train, y_train):\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000, step=100),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 5, 15),\n",
    "                \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 100, 300, step=50),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 10, 100, step=10),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 5, 50, step=5),\n",
    "                \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", 0.2, 0.5, 0.8]),\n",
    "                \"bootstrap\": True,\n",
    "                \"oob_score\": True,\n",
    "                \"class_weight\": trial.suggest_categorical(\"class_weight\", [None, \"balanced\"]),\n",
    "                \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            }\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=6)\n",
    "            model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
    "\n",
    "            cv_scores = cross_val_score(\n",
    "                model, X_train, y_train,\n",
    "                cv=tscv, scoring=\"f1_macro\", n_jobs=-1\n",
    "            )\n",
    "            # Fit to get OOB\n",
    "            model.fit(X_train, y_train)\n",
    "            oob = model.oob_score_\n",
    "\n",
    "            # Weighted combination still anchored to F1\n",
    "            score = 0.8 * cv_scores.mean() + 0.2 * oob\n",
    "            trial.set_user_attr(\"cv_mean\", cv_scores.mean())\n",
    "            trial.set_user_attr(\"oob_score\", oob)\n",
    "            return score\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=f\"rf_opt_class_{LOOKAHEAD}{market}\",\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "            storage=f\"sqlite:///dbs/rf_opt_study_session_less{market}.db\",\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=5)\n",
    "        return study.best_params\n",
    "\n",
    "    def tune_catboost(X_train, y_train):\n",
    "        def objective(trial):\n",
    "            bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli'])\n",
    "\n",
    "            class_weights = compute_class_weight(\n",
    "                class_weight='balanced',\n",
    "                classes=np.unique(y_train),\n",
    "                y=y_train\n",
    "            )\n",
    "\n",
    "            params = {\n",
    "                'iterations': trial.suggest_int('iterations', 300, 1500, step=100),\n",
    "                'depth': trial.suggest_int('depth', 4, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "                'random_strength': trial.suggest_float('random_strength', 0.5, 5.0),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "                'bootstrap_type': bootstrap_type,\n",
    "                'loss_function': 'MultiClass',\n",
    "                'eval_metric': 'TotalF1',\n",
    "                'class_weights': class_weights.tolist(),\n",
    "                'verbose': 0\n",
    "            }\n",
    "\n",
    "            if bootstrap_type == 'Bayesian':\n",
    "                params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0.0, 1.0)\n",
    "\n",
    "            model = CatBoostClassifier(**params, random_state=42)\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=splits)\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='f1_macro', n_jobs=-1)\n",
    "            print(f\"Trial {trial.number} F1 Score: {scores.mean():.5f} | Params: {params}\")\n",
    "            return scores.mean()\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name=f'catboost_opt_class_{LOOKAHEAD}',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "            storage=f'sqlite:///catboost_opt_study_session_less.db',\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=2)\n",
    "        return study.best_params\n",
    "\n",
    "    def tune_meta_logreg(X_meta, y_meta):\n",
    "        def objective(trial):\n",
    "            penalty = trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "            if penalty is not None:\n",
    "                C = trial.suggest_float(\"C\", 1e-4, 10.0, log=True)\n",
    "            else:\n",
    "                C = 1.0  # default, unused\n",
    "\n",
    "            class_weight = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "            params = {\n",
    "                \"penalty\": penalty,\n",
    "                \"C\": C,\n",
    "                \"solver\": \"lbfgs\",\n",
    "                \"max_iter\": 2000,\n",
    "                \"class_weight\": class_weight,\n",
    "            }\n",
    "\n",
    "            model = make_pipeline(StandardScaler(), LogisticRegression(**params, random_state=42))\n",
    "            tscv = TimeSeriesSplit(n_splits=splits)\n",
    "\n",
    "            scores = cross_val_score(model, X_meta, y_meta, cv=tscv, scoring=\"f1_macro\", n_jobs=-1)\n",
    "            print(f\"Trial {trial.number} F1 Score: {scores.mean():.5f} | Params: {params}\")\n",
    "            return scores.mean()\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=f\"meta_logreg_class_{LOOKAHEAD}\",\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "            storage=f\"sqlite:///meta_logreg_stack_session_less.db\",\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=2)  # adjust trial count as needed\n",
    "        return study.best_params\n",
    "\n",
    "    def tune_lstm_classifier_with_optuna(X, y, splits, lookahead, num_classes=2):\n",
    "        def objective(trial):\n",
    "            units = trial.suggest_int(\"units\", 16, 128, step=16)\n",
    "            lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "            batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "            epochs = trial.suggest_int(\"epochs\", 5, 30)\n",
    "\n",
    "            scores = []\n",
    "            tscv = TimeSeriesSplit(n_splits=splits)\n",
    "\n",
    "            for train_idx, val_idx in tscv.split(X):\n",
    "                X_tr, X_val = X[train_idx], X[val_idx]\n",
    "                y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "                model = LSTMClassifierWrapper(\n",
    "                    input_shape=X.shape[1],\n",
    "                    units=units,\n",
    "                    lr=lr,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=0,\n",
    "                    num_classes=num_classes\n",
    "                )\n",
    "                model.fit(X_tr, y_tr)\n",
    "                preds = model.predict(X_val)\n",
    "                acc = accuracy_score(y_val, preds)\n",
    "                scores.append(acc)\n",
    "\n",
    "            mean_acc = np.mean(scores)\n",
    "            print(f\"Trial {trial.number} Accuracy: {mean_acc:.5f} | Params: units={units}, lr={lr}, batch={batch_size}, epochs={epochs}\")\n",
    "            return mean_acc\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            study_name=\"lstm_class_opt\",\n",
    "            storage=f\"sqlite:///lstm_class_opt_study{lookahead}_session_less.db\",\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        study.optimize(objective, n_trials=2)\n",
    "        print(\"Best trial:\", study.best_trial.params)\n",
    "        return study.best_trial.params\n",
    "    ################################################\n",
    "    ####### Ensure index consistency\n",
    "    ####### Sequential #######\n",
    "    # y_train_seq = y_train_class.loc[X_train_seq.index]\n",
    "    # y_test_seq = y_test_class.loc[X_test_seq.index]\n",
    "\n",
    "    ################################################\n",
    "    ####### Tune models\n",
    "    ####### Tree Based #######\n",
    "    rf_params         = tune_rf(X_train, y_train)\n",
    "    # X_lstm = X_train_seq.values\n",
    "    # y_lstm = y_train_class.values\n",
    "\n",
    "    ################################################\n",
    "    ####### Train models\n",
    "    ####### Tree Based #######\n",
    "    rf          = RandomForestClassifier(**rf_params, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    # ####### Sequential #######\n",
    "    # lstm_model = LSTMClassifierWrapper(input_shape=X_lstm.shape[1])\n",
    "    # lstm_model.fit(X_lstm, y_lstm)  # wrapper does the reshaping\n",
    "    # X_lstm_test = X_test_seq.values\n",
    "    # lstm_preds = lstm_model.predict(X_lstm_test)\n",
    "    ################################################\n",
    "    ####### OOF Predicition\n",
    "    ####### Tree Based #######\n",
    "    # oof_tree = generate_oof_lstm_classifier([lgbm], X_train, y_train, 4)\n",
    "    # oof_lstm = generate_oof_lstm_classifier(LSTMClassifierWrapper, X_lstm, y_lstm, splits)  # <- Uses sequential input\n",
    "\n",
    "    ################################################\n",
    "    ####### Train Meta Model\n",
    "    ####### Tree Based #######\n",
    "    # X_meta_train = pd.DataFrame({\n",
    "    #     'cat': oof_tree.iloc[:, 0].values,\n",
    "    #     'rf': oof_tree.iloc[:, 1].values,\n",
    "    #     'lstm': oof_lstm.values\n",
    "    # }, index=y_train_class.index)\n",
    "\n",
    "    # X_meta_test = pd.DataFrame({\n",
    "    #     \"cat\": catboost.predict(X_test_tree).flatten(),\n",
    "    #     \"rf\": rf.predict(X_test_tree).flatten(),\n",
    "    #     \"lstm\": lstm_model.predict(X_test_seq.values).flatten()\n",
    "    # })\n",
    "\n",
    "    # X_meta_train_combined = pd.concat([\n",
    "    #     X_meta_train.reset_index(drop=True),\n",
    "    #     X_train_linear.reset_index(drop=True)\n",
    "    # ], axis=1)\n",
    "\n",
    "    # X_meta_test_combined = pd.concat([\n",
    "    #     X_meta_test.reset_index(drop=True),\n",
    "    #     X_test_linear.reset_index(drop=True)\n",
    "    # ], axis=1)\n",
    "\n",
    "    # meta_params = tune_meta_logreg(X_meta_train_combined, y_train_class)\n",
    "    # meta_model = make_pipeline(StandardScaler(),LogisticRegression(**meta_params, random_state=42))\n",
    "    # meta_model.fit(X_meta_train_combined, y_train_class)\n",
    "\n",
    "    ################################################\n",
    "    ####### Evaluate Model\n",
    "    def evaluate_model(name, model, Xtr, Xte, ytr, yte):\n",
    "        train_preds = model.predict(Xtr)\n",
    "        test_preds = model.predict(Xte)\n",
    "\n",
    "        train_acc = accuracy_score(ytr, train_preds)\n",
    "        test_acc = accuracy_score(yte, test_preds)\n",
    "\n",
    "        print(f\"\\nüìä {name} Classification Accuracy:\")\n",
    "        print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        return test_preds\n",
    "    \n",
    "    ####### Tree Based #######\n",
    "    print(\"\\nRF\")\n",
    "    preds_rf        = evaluate_model(\"RandomForest\", rf, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    ################################################\n",
    "    ####### Target Distribution\n",
    "    print(\"\\nüîç Target distribution:\")\n",
    "    print(y_train.describe())\n",
    "    \n",
    "    ################################################\n",
    "    ####### Choose final model\n",
    "    preds_rf = rf.predict(X_test)\n",
    "    print(\"\\nüîç Checking LGBM prediction distribution (classification):\")\n",
    "    print(f\"Classes predicted: {np.unique(preds_rf)}\")\n",
    "    print(f\"Prediction counts:\\n{pd.Series(preds_rf).value_counts()}\")\n",
    "\n",
    "    # Classification metrics\n",
    "    acc = accuracy_score(y_test, preds_rf)\n",
    "    f1 = f1_score(y_test, preds_rf, average='macro')\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, preds_rf))\n",
    "\n",
    "    metadata = {\n",
    "        \"lookahead\": LOOKAHEAD,\n",
    "        # \"xgboost_params\": xgboost_params,\n",
    "        # \"catboost_params\": catboost_params,\n",
    "        \"rf_params\": rf_params,\n",
    "        # \"meta_params\": meta_params,\n",
    "        # \"lgbm_params\": lgbm_params\n",
    "    }\n",
    "    with open(f\"classifier_metadata_{LOOKAHEAD}{market}.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "        \n",
    "    # joblib.dump(meta_model, f\"stack_model_classifier_LOOKAHEAD_{LOOKAHEAD}_session_less.pkl\")\n",
    "    joblib.dump(rf, f\"pkl/rf_model_classifier_LOOKAHEAD_{LOOKAHEAD}{market}_session_less.pkl\")\n",
    "\n",
    "    return {\n",
    "        'lookahead': LOOKAHEAD,\n",
    "        'preds_stack': rf.predict_proba(X_test),\n",
    "        # 'preds_xgboost': xgboost.predict_proba(X_test_tree),\n",
    "        'X_test_tree': X_test,\n",
    "        'X_test_linear': X_test,\n",
    "        # 'X_meta_test_combined': X_meta_test_combined,\n",
    "        'true_values': y_test.values,\n",
    "        # 'label_encoder': le\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1) Load & prepare\n",
    "df = pd.read_parquet(f\"parquet/labeled_data_6{market}.parquet\")\n",
    "clf_col = [c for c in df.columns if c.startswith(\"clf_target\")][0]\n",
    "df_clean = df.dropna(subset=[clf_col] + selected_indicators_class)\n",
    "\n",
    "X = df_clean[selected_indicators_class].copy()\n",
    "y_full = df_clean[clf_col]\n",
    "\n",
    "# --- Feature Filtering Step 1: Zero/Near-Zero Variance ---\n",
    "vt = VarianceThreshold(threshold=1e-4)\n",
    "vt.fit(X)\n",
    "X = X.loc[:, vt.get_support()]\n",
    "\n",
    "# --- Feature Filtering Step 2: Multicollinearity Removal ---\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "X = X.drop(columns=to_drop_corr)\n",
    "\n",
    "# 2) Stage-1 target: hit vs no-hit\n",
    "y_hit = (y_full != 0).astype(int)\n",
    "\n",
    "# 3) Stage-2 data: only hits\n",
    "mask_hits = y_hit == 1\n",
    "X2_all = X[mask_hits]\n",
    "y_dir = (y_full[mask_hits] == 2).astype(int)  # 0=LONG, 1=SHORT\n",
    "\n",
    "# --- Feature Filtering Step 3: Permutation Importance on Stage-2 ---\n",
    "imp_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "imp_model.fit(X2_all, y_dir)\n",
    "perm = permutation_importance(\n",
    "    imp_model, X2_all, y_dir,\n",
    "    n_repeats=5, random_state=42, n_jobs=4\n",
    ")\n",
    "imp_series = pd.Series(perm.importances_mean, index=X2_all.columns)\n",
    "to_drop_perm = imp_series[imp_series <= 0].index.tolist()\n",
    "X = X.drop(columns=to_drop_perm)\n",
    "\n",
    "# Rebuild Stage-2 inputs after dropping\n",
    "mask_hits = (y_full != 0)\n",
    "X2_all = X[mask_hits]\n",
    "\n",
    "# 4) Define models\n",
    "stage1_models = {\n",
    "    \"XGBoost\": xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42, n_jobs=4\n",
    "    ),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42, n_jobs=4\n",
    "    ),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        loss_function=\"Logloss\",\n",
    "        class_weights=[1,1],\n",
    "        random_state=42, verbose=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "stage2_models = {\n",
    "    \"XGBoost\": xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42, n_jobs=4\n",
    "    ),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42, n_jobs=4\n",
    "    ),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        loss_function=\"Logloss\",\n",
    "        class_weights=[1,1],\n",
    "        random_state=42, verbose=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "# 5) Hyperparameter grids for Stage 2\n",
    "param_grids = {\n",
    "    \"XGBoost\": {\n",
    "        \"max_depth\": [4,6,8],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"n_estimators\": [100, 200]\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"num_leaves\": [31,63,127],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"n_estimators\": [100, 200]\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"depth\": [4,6,8],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"iterations\": [100, 200]\n",
    "    },\n",
    "}\n",
    "\n",
    "# 6) Two-stage cascade with TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "for name in stage1_models:\n",
    "    m1 = stage1_models[name]\n",
    "    m2 = stage2_models[name]\n",
    "    print(f\"\\n=== Two-stage Cascade with {name} ===\")\n",
    "    \n",
    "    for fold, (tr, va) in enumerate(tscv.split(X)):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        # Stage 1: Hit vs No-Hit\n",
    "        X1_tr, X1_va = X.iloc[tr], X.iloc[va]\n",
    "        y1_tr, y1_va = y_hit.iloc[tr], y_hit.iloc[va]\n",
    "        m1.fit(X1_tr, y1_tr)\n",
    "        hit_pred = m1.predict(X1_va)\n",
    "        print(\"Stage-1 report:\")\n",
    "        print(classification_report(\n",
    "            y1_va, hit_pred,\n",
    "            target_names=[\"NO-HIT\",\"HIT\"]\n",
    "        ))\n",
    "        \n",
    "        # Stage 2: LONG vs SHORT on true hits\n",
    "        tr_hits = tr[y1_tr.values == 1]\n",
    "        X2_tr = X.iloc[tr_hits]\n",
    "        y2_tr = (y_full.iloc[tr_hits] == 2).astype(int)\n",
    "        \n",
    "        # Oversample LONG\n",
    "        ros = RandomOverSampler(sampling_strategy='not majority', random_state=42)\n",
    "        X2_tr_bal, y2_tr_bal = ros.fit_resample(X2_tr, y2_tr)\n",
    "        \n",
    "        # Hyperparam search\n",
    "        grid = GridSearchCV(\n",
    "            m2, param_grids[name],\n",
    "            cv=TimeSeriesSplit(n_splits=3),\n",
    "            scoring='f1', n_jobs=4\n",
    "        )\n",
    "        grid.fit(X2_tr_bal, y2_tr_bal)\n",
    "        best_m2 = grid.best_estimator_\n",
    "        \n",
    "        # Predict on predicted hits\n",
    "        va_hits = va[hit_pred == 1]\n",
    "        dir_pred = (best_m2.predict(X.iloc[va_hits])\n",
    "                    if len(va_hits) else np.array([], dtype=int))\n",
    "        \n",
    "        # Assemble final predictions\n",
    "        final_pred = np.zeros_like(hit_pred)\n",
    "        final_pred[hit_pred == 1] = dir_pred + 1  # 1‚ÜíLONG, 2‚ÜíSHORT\n",
    "        \n",
    "        print(\"Final 3-class report:\")\n",
    "        print(classification_report(\n",
    "            y_full.iloc[va], final_pred,\n",
    "            target_names=[\"NO-HIT\",\"LONG\",\"SHORT\"]\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "df = pd.read_parquet(f\"parquet/labeled_data_6{market}.parquet\")\n",
    "clf_col = [c for c in df.columns if c.startswith(\"clf_target\")][0]\n",
    "df_clean = df.dropna(subset=[clf_col] + selected_indicators_class)\n",
    "\n",
    "X = df_clean[selected_indicators_class].copy()\n",
    "y_full = df_clean[clf_col]\n",
    "\n",
    "def benchmark(clf, X, y, label):\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(\n",
    "        clf, X, y,\n",
    "        cv=3,               # use fewer folds for a quick test\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1 if hasattr(clf, 'n_jobs') else 1\n",
    "    )\n",
    "    duration = time.time() - start\n",
    "    print(f\"{label}: mean acc={scores.mean():.4f}, took {duration:.1f}s\")\n",
    "\n",
    "# Prepare your data\n",
    "X_arr = X.values  # or X if you want a DataFrame\n",
    "y_arr = y_full\n",
    "\n",
    "# CPU-hist (multicore)\n",
    "clf_cpu = XGBClassifier(\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "benchmark(clf_cpu, X_arr, y_arr, \"CPU-hist\")\n",
    "\n",
    "# GPU-hist (single process)\n",
    "clf_gpu = XGBClassifier(\n",
    "    tree_method='gpu_hist',\n",
    "    gpu_id=0,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "benchmark(clf_gpu, X_arr, y_arr, \"GPU-hist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Load & prep ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df = pd.read_parquet(f\"parquet/labeled_data_6{market}.parquet\")\n",
    "\n",
    "# your regression target\n",
    "reg_col = \"reg_target_lookahead6\"\n",
    "\n",
    "# drop rows missing the reg target or your features set\n",
    "df_clean = df.dropna(subset=[reg_col] + selected_indicators_class)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Build feature list ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "exclude = set([\"clf_target_numba_6\", reg_col])\n",
    "features = [c for c in df_clean.columns if c not in exclude]\n",
    "\n",
    "X = df_clean[features].copy()\n",
    "y = df_clean[reg_col].copy()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Keep only numeric & drop any row with NaN/Inf ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "X_num = X.select_dtypes(include=[np.number]).replace([np.inf, -np.inf], np.nan)\n",
    "mask  = X_num.notnull().all(axis=1)\n",
    "\n",
    "X_clean = X_num.loc[mask]\n",
    "y_clean = y.loc[mask]\n",
    "\n",
    "print(f\"Dropped {len(X_num)-len(X_clean)} rows; now {X_clean.shape[0]} samples √ó {X_clean.shape[1]} features\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) RFECV setup for regression ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "reg = XGBRegressor(\n",
    "    tree_method=\"gpu_hist\",     # or \"gpu_hist\" if you have a GPU\n",
    "    gpu_id=0,\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=reg,\n",
    "    step=1,\n",
    "    min_features_to_select=30,\n",
    "    cv=tscv,\n",
    "    scoring=\"r2\",            # use R^2; you could also use 'neg_mean_squared_error'\n",
    "    n_jobs=4,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 5) Fit and select ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "rfecv.fit(X_clean.values, y_clean.values)\n",
    "\n",
    "n_best = rfecv.n_features_\n",
    "best_feats = list(X_clean.columns[rfecv.support_])\n",
    "print(f\"Optimal # features = {n_best}\")\n",
    "print(\"Selected features:\")\n",
    "for f in best_feats:\n",
    "    print(\"  ‚Ä¢\", f)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 6) (Optional) RFE to extract exactly n_best features ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "rfe = RFE(\n",
    "    estimator=rfecv.estimator_,\n",
    "    n_features_to_select=n_best,\n",
    "    step=1\n",
    ")\n",
    "rfe.fit(X_clean.values, y_clean.values)\n",
    "\n",
    "final_feats = list(X_clean.columns[rfe.support_])\n",
    "print(\"\\nConfirmed features at peak R¬≤:\")\n",
    "for f in final_feats:\n",
    "    print(\"  ‚Ä¢\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer\n",
    "\n",
    "# Filter out class 0 (no-trade) for evaluation\n",
    "def hybrid_score(y_true, y_pred, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Combine directional accuracy and RMSE for regression scoring.\n",
    "    Higher = better.\n",
    "    \n",
    "    alpha: weight for directional accuracy (0.0 to 1.0)\n",
    "           0.7 means 70% direction, 30% error penalty\n",
    "    \"\"\"\n",
    "    mask = ~np.isnan(y_true)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    # Directional accuracy\n",
    "    direction_acc = (np.sign(y_true) == np.sign(y_pred)).mean()\n",
    "\n",
    "    # RMSE (negated so that lower RMSE = higher score)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred, squared=False)\n",
    "    \n",
    "    return alpha * direction_acc - (1 - alpha) * rmse\n",
    "\n",
    "# Sklearn-compatible scorer\n",
    "hybrid_scorer = make_scorer(hybrid_score, greater_is_better=True)\n",
    "\n",
    "df = pd.read_parquet(f\"parquet/labeled_data_6{market}.parquet\")\n",
    "# 1) Identify your label columns\n",
    "#    Replace 'clf_target' and 'reg_target' with whatever your actual target column names are\n",
    "label_cols = ['clf_target_numba_6', 'reg_target_lookahead6']\n",
    "\n",
    "# 2) Build the exclusion set (OHLC + any targets)\n",
    "exclude_cols = set(['open', 'high', 'low', 'close', 'volume'] + label_cols)\n",
    "\n",
    "# 3) Collect feature names with a simple for-loop\n",
    "features = [\n",
    "    col for col in df.columns\n",
    "    if col not in exclude_cols\n",
    "]\n",
    "\n",
    "max_k = 80\n",
    "\n",
    "# 4) (Optional) show what you found\n",
    "print(f\"Found {len(features)} feature columns\")\n",
    "\n",
    "X = df[features]\n",
    "y = df['reg_target_lookahead6']\n",
    "\n",
    "# 2) Extract only numeric features (drop datetime/object columns)\n",
    "X_num = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# 3) Drop any column with inf or NaN\n",
    "mask = np.isfinite(X_num).all(axis=1)\n",
    "X_num_clean = X_num.loc[mask]\n",
    "y = y.loc[mask]\n",
    "\n",
    "print(f\"Using {X_num_clean.shape[1]} numeric features after dropna/inf\")\n",
    "\n",
    "# 1) Suppose these are the column names you must include:\n",
    "\n",
    "\n",
    "# 2) Convert them to integer indices into X_clean:\n",
    "# fixed_idxs = list(range(len(must_include)))\n",
    "\n",
    "# min_total = max(7, len(fixed_idxs))\n",
    "\n",
    "# 2) Time-series splitter\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# 3) Your estimator\n",
    "reg = lgb.LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=5\n",
    ")\n",
    "\n",
    "sfs = SFS(\n",
    "    estimator=reg,\n",
    "    k_features=(10, max_k),        \n",
    "    forward=True,\n",
    "    floating=True,           # no backtracking\n",
    "    scoring=hybrid_score,\n",
    "    cv=tscv,\n",
    "    n_jobs=-2,                 # 1 if using GPU; use -1 for CPU\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# 5) Fit SFS on your cleaned data\n",
    "sfs = sfs.fit(X_num_clean.values, y)\n",
    "\n",
    "# 6) Inspect final selected feature names\n",
    "selected = list(sfs.k_feature_names_)\n",
    "print(f\"Selected {len(selected)} features via SFS:\")\n",
    "for feat in selected:\n",
    "    print(\" ‚Ä¢\", X_num_clean.columns[int(feat)])\n",
    "\n",
    "# 7) (Optional) See CV score at each subset size\n",
    "print(\"\\nPerformance by subset size:\")\n",
    "for k, info in sfs.subsets_.items():\n",
    "    feats = [X_num_clean.columns[i] for i in info['feature_idx']]\n",
    "    print(f\"{k:2d} features ‚Üí score = {info['avg_score']:.4f} ‚Üí {feats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SampleWeightWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator, sample_weight):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.sample_weight = sample_weight\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self.base_estimator.fit(X, y, sample_weight=self.sample_weight.loc[y.index])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.base_estimator.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.base_estimator.predict_proba(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.base_estimator.score(X, y)\n",
    "    \n",
    "\n",
    "# Filter out class 0 (no-trade) for evaluation\n",
    "def long_short_f1(y_true, y_pred):\n",
    "    # Focus only on long (1) and short (2)\n",
    "    mask = (y_true == 1) | (y_true == 2)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0  # Avoid division by zero if no trades\n",
    "    return f1_score(y_true[mask], y_pred[mask], average='macro')\n",
    "\n",
    "f1_scorer = make_scorer(long_short_f1, greater_is_better=True)\n",
    "\n",
    "df = pd.read_parquet(f\"parquet/labeled_data_6{market}.parquet\")\n",
    "# 1) Identify your label columns\n",
    "#    Replace 'clf_target' and 'reg_target' with whatever your actual target column names are\n",
    "label_cols = ['clf_target_numba_6', 'reg_target_lookahead6']\n",
    "\n",
    "# 2) Build the exclusion set (OHLC + any targets)\n",
    "exclude_cols = set(['open', 'high', 'low', 'close', 'volume'] + label_cols)\n",
    "\n",
    "static_features = []\n",
    "\n",
    "# 3) Collect feature names with a simple for-loop\n",
    "features = [\n",
    "    col for col in df.columns\n",
    "    if col not in exclude_cols\n",
    "]\n",
    "\n",
    "# 4) (Optional) show what you found\n",
    "print(f\"Found {len(features)} feature columns\")\n",
    "\n",
    "X = df[features]\n",
    "y = df['clf_target_numba_6']\n",
    "\n",
    "# 2) Extract only numeric features (drop datetime/object columns)\n",
    "X_num = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# 3) Drop any column with inf or NaN\n",
    "mask = np.isfinite(X_num).all(axis=1)\n",
    "X_num_clean = X_num.loc[mask]\n",
    "y = y.loc[mask]\n",
    "\n",
    "print(f\"Using {X_num_clean.shape[1]} numeric features after dropna/inf\")\n",
    "\n",
    "# 2) Time-series splitter\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "classes = np.array([0, 1, 2])\n",
    "weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "class_weight_map = dict(zip(classes, weights))\n",
    "sample_weight = y.map(class_weight_map)\n",
    "\n",
    "# 3) Your estimator\n",
    "clf = XGBClassifier(\n",
    "    tree_method='gpu_hist',  # or 'hist' for CPU or gpu_hist\n",
    "    gpu_id=0,\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=5,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "wrapped_model = SampleWeightWrapper(clf, sample_weight)\n",
    "# 4) Configure SFS\n",
    "#    Let‚Äôs say you want to pick the same \"lean_n\" features you found earlier:\n",
    "max_k = 2  # or whatever count you determined was best\n",
    "\n",
    "sfs = SFS(\n",
    "    estimator=wrapped_model,\n",
    "    k_features=(1, max_k),        \n",
    "    forward=True,\n",
    "    floating=True,           # no backtracking\n",
    "    scoring=f1_scorer,\n",
    "    cv=tscv,\n",
    "    n_jobs=1,                 # 1 if using GPU; use -1 for CPU\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# 5) Fit SFS on your cleaned data\n",
    "sfs = sfs.fit(X_num_clean.values, y)\n",
    "\n",
    "# 6) Inspect final selected feature names\n",
    "selected = list(sfs.k_feature_names_)\n",
    "print(f\"Selected {len(selected)} features via SFS:\")\n",
    "for feat in selected:\n",
    "    print(\" ‚Ä¢\", X_num_clean.columns[int(feat)])\n",
    "\n",
    "# 7) (Optional) See CV score at each subset size\n",
    "print(\"\\nPerformance by subset size:\")\n",
    "for k, info in sfs.subsets_.items():\n",
    "    feats = [X_num_clean.columns[i] for i in info['feature_idx']]\n",
    "    print(f\"{k:2d} features ‚Üí score = {info['avg_score']:.4f} ‚Üí {feats}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T01:17:27.107482Z",
     "start_time": "2025-06-04T00:54:31.122684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Regression Training\n",
    "reg_results = []\n",
    "class_results = []\n",
    "\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "classification_models = run_lookahead_for_session_classification(6)\n",
    "class_results.append(classification_models)\n",
    "\n",
    "# regression_models = run_lookahead_for_session_regression()\n",
    "# reg_results.append(regression_models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T01:29:01.886642Z",
     "start_time": "2025-06-04T01:28:46.711155Z"
    }
   },
   "outputs": [],
   "source": [
    "from backtest import evaluate_classification\n",
    "all_results = []\n",
    "\n",
    "for result in class_results:\n",
    "    preds_stack = result['preds_stack']\n",
    "    X_test = result['X_test_tree']\n",
    "    y_test = result['true_values']\n",
    "    labeled = pd.read_parquet(f\"parquet/labeled_data_6{market}.parquet\")\n",
    "    df_backtest = labeled.copy()\n",
    "\n",
    "    print(f\"\\nüîé Predicted return range for STACK: min={preds_stack.min():.15f}, max={preds_stack.max():.15f}\")\n",
    "    # results = evaluate_regression(\n",
    "    #     X_test=X_test_combined,\n",
    "    #     preds_stack=preds_stack,\n",
    "    #     labeled=labeled,\n",
    "    #     df=df_backtest,\n",
    "    #     avoid_funcs=avoid_funcs,\n",
    "    #     TICK_VALUE=20,\n",
    "    #     is_same_session=is_same_session,\n",
    "    #     long_thresh=0.0009,\n",
    "    #     TRAIL_START_MULT=1.0,\n",
    "    #     TRAIL_STOP_MULT=1.0,\n",
    "    #     short_thresh=-0.00009,\n",
    "    #     base_contracts=1,\n",
    "    #     max_contracts=1,\n",
    "    # )\n",
    "    # results = evaluate_static_tp_two_contracts(\n",
    "    #     X_test=X_test_combined,\n",
    "    #     preds_stack=preds_stack,\n",
    "    #     labeled=labeled,\n",
    "    #     df=df_backtest,\n",
    "    #     avoid_funcs=avoid_funcs,\n",
    "    #     TICK_VALUE=20,\n",
    "    #     is_same_session=is_same_session,\n",
    "    #     long_thresh=0.00001,\n",
    "    #     short_thresh=-0.00001,\n",
    "    #     TP_POINTS = 10.0,\n",
    "    # )\n",
    "    results = evaluate_classification(\n",
    "        X_test, preds_stack, labeled, df_backtest,\n",
    "        avoid_funcs,\n",
    "        TRAIL_START_MULT=0, TRAIL_STOP_MULT=0, TICK_VALUE=1\n",
    "    )\n",
    "\n",
    "    all_results.append(results)\n",
    "    print(results)\n",
    "    print(\n",
    "        f\"\\nPnL: ${results['pnl']:.2f}\"\n",
    "        f\"\\nTrades: {results['trades']}\"\n",
    "        f\"\\nWin Rate: {results['win_rate']:.2%}\"\n",
    "        f\"\\nExpectancy: {results['expectancy']:.2f}\"\n",
    "        f\"\\nProfit Factor: {results['profit_factor']:.2f}\"\\\n",
    "        f\"\\nSharpe Ratio: {results['sharpe']:.2f}\"\n",
    "        f\"\\nLong Trades: {results['long_trades']} | Short Trades: {results['short_trades']}\"\n",
    "        f\"\\n\"\n",
    "    )\n",
    "\n",
    "    print(\"Avoid Hits:\")\n",
    "    for name, count in results['avoid_hits'].items():\n",
    "        print(f\" - {name}: {count}\")\n",
    "\n",
    "    if not results['results'].empty and 'pnl' in results['results'].columns:\n",
    "        print(\"\\nüî¢ Top 5 PnL trades:\")\n",
    "        print(results['results'].sort_values(by='pnl', ascending=False).head(5))\n",
    "\n",
    "        print(\"\\nüîª Bottom 5 PnL trades:\")\n",
    "        print(results['results'].sort_values(by='pnl', ascending=True).head(5))\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No trades executed, skipping PnL trade breakdown.\")\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame([{\n",
    "    'pnl': r['pnl'],\n",
    "    'sharpe': r['sharpe'],\n",
    "    'expectancy': r['expectancy'],\n",
    "    'profit_factor': r['profit_factor'],\n",
    "    'win_rate': r['win_rate'],\n",
    "    'trades': r['trades'],\n",
    "    'results': r['results'],\n",
    "} for r in all_results])\n",
    "top = summary_df.sort_values(by='sharpe', ascending=False).head(10)\n",
    "print(\"\\nüèÅ Top 10 Configurations Across All Lookaheads:\")\n",
    "print(top[['pnl', 'sharpe', 'expectancy', 'profit_factor', 'win_rate', 'trades']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = None\n",
    "\n",
    "def compute_max_consecutive_loss(df):\n",
    "    \"\"\"\n",
    "    Calculates the worst cumulative loss (drawdown) from any starting point.\n",
    "    \"\"\"\n",
    "    pnl_series = df['pnl'].values\n",
    "    max_loss = 0.0\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "\n",
    "    for i in range(len(pnl_series)):\n",
    "        cumulative = 0.0\n",
    "        for j in range(i, len(pnl_series)):\n",
    "            cumulative += pnl_series[j]\n",
    "            if cumulative < max_loss:\n",
    "                max_loss = cumulative\n",
    "                start_idx = i\n",
    "                end_idx = j\n",
    "\n",
    "    return max_loss, df['entry_time'].iloc[start_idx], df['entry_time'].iloc[end_idx]\n",
    "\n",
    "\n",
    "for r in all_results:\n",
    "    df = r['results'].copy()\n",
    "    df = df.sort_values(by='entry_time')\n",
    "    df['cumulative_pnl'] = df['pnl'].cumsum()\n",
    "\n",
    "    # Count how many trades exited for each reason\n",
    "    exit_counts = df['exit_reason'].value_counts(dropna=False)\n",
    "    print(exit_counts)\n",
    "\n",
    "    if (\n",
    "        df['cumulative_pnl'].iloc[-1] > 0 and\n",
    "        r['sharpe'] > 0.01 and\n",
    "        r['trades'] > 1 and\n",
    "        r['win_rate'] > 0.001 and\n",
    "        r['profit_factor'] > 0.01 and\n",
    "        r['expectancy'] > 0.01 and\n",
    "        r['pnl'] > 100\n",
    "    ):\n",
    "        if best_result is None or r['sharpe'] > best_result['sharpe']:\n",
    "            best_result = r.copy()\n",
    "            best_result['cumulative_pnl'] = df['cumulative_pnl']\n",
    "            best_result['entry_time'] = df['entry_time']\n",
    "\n",
    "            # === Calculate max drawdown (largest PnL loss from peak)\n",
    "            cumulative = df['cumulative_pnl']\n",
    "            rolling_max = cumulative.cummax()\n",
    "            drawdowns = cumulative - rolling_max\n",
    "            max_drawdown = drawdowns.min()  # Most negative drop\n",
    "            max_drawdown_start = rolling_max[drawdowns.idxmin()]\n",
    "            best_result['max_drawdown'] = max_drawdown\n",
    "\n",
    "# === Plot the best one ===\n",
    "# === After determining best_result\n",
    "if best_result:\n",
    "    df = best_result['results'].copy()\n",
    "    df = df.sort_values(by='entry_time')\n",
    "    df['cumulative_pnl'] = df['pnl'].cumsum()\n",
    "\n",
    "    max_loss, loss_start, loss_end = compute_max_consecutive_loss(df)\n",
    "\n",
    "    # === Plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(df['entry_time'], df['cumulative_pnl'], label='Cumulative PnL', color='green')\n",
    "    plt.axvspan(loss_start, loss_end, color='red', alpha=0.2, label='Max Loss Window')\n",
    "    plt.title(f\"Top Sharpe Strategy | Max Consecutive Loss: {max_loss:.2f} | Cumulative PnL: {best_result['pnl']:.2f}\")\n",
    "    plt.xlabel(\"Datetime\")\n",
    "    plt.ylabel(\"Cumulative PnL\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"üí£ Max Consecutive PnL Loss: {max_loss:.2f}\")\n",
    "    print(f\"üìÜ Period: {loss_start} ‚Üí {loss_end}\")\n",
    "    best_result['results'].to_csv(\"best_strategy_results.csv\", index=False)\n",
    "    print(\"‚úÖ Saved best_strategy_results.csv\")\n",
    "else:\n",
    "    print(\"‚ùå No strategy met the conditions.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
